{
  "columns": [
    "Unnamed: 0",
    "0"
  ],
  "data": [
    [
      0,
      "deep neural networks have shown great success in various applications such as objection recognition and speech recognition . furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing . these include, but are not limited to, language modeling, paraphrase detection and word embedding extraction. in the field of statistical machine translation , deep neural networks have begun to show promising results. summarizes a successful usage of feedforward neural networks in the framework of phrase-based smt system. along this line of research on using neural networks for smt, this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase-based smt system. the proposed neural network architecture, which we will refer to as an rnn encoder-decoder, consists of two recurrent neural networks that act as an encoder and a decoder pair. the encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. the two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training.the proposed rnn encoder-decoder with a novel hidden unit is empirically evaluated on the task of translating from english to french. we train the model to learn the translation probability of an english phrase to a corresponding french phrase. the model is then used as apart of a standard phrase-based smt system by scoring each phrase pair in the phrase table. the empirical evaluation reveals that this approach of scoring phrase pairs with an rnn encoder-decoder improves the translation performance.we qualitatively analyze the trained rnn encoder-decoder by comparing its phrase scores with those given by the existing translation model. the qualitative analysis shows that the rnn encoder-decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. the further analysis of the model reveals that the rnn encoder-decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase. a recurrent neural network is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = . at each time step t, the hidden state ht of the rnn is updated bywhere f is a non-linear activation function. f maybe as simple as an elementwise logistic sigmoid function and as complex as along short-term memory unit).an rnn can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. in that case, the output at each timestep t is the conditional distribution p. for example, a multinomial distribution can be output using a softmax activation functionfor all possible symbols j = 1, . . . , k, where w j are the rows of a weight matrix w. by combining these probabilities, we can compute the probability of the sequence x usingfrom this learned distribution, it is straightforward to sample anew sequence by iteratively sampling a symbol at each time step."
    ],
    [
      1,
      "in neural language modelling, a neural network estimates a distribution over sequences of words or characters that belong to a given language. in neural machine translation, the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language. the network can bethought of as composed of two parts: a source network that encodes the source sequence into a representation and a target network that uses the representation of the source encoder to generate the target sequence.recurrent neural networks are powerful sequence models and are widely used in language modelling), yet they have a potential drawback. rnns have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation. forward and backward signals in a rnn also need to traverse the full distance of the serial path to reach from one token in the sequence to another. the larger the distance, the harder it is to learn the dependencies between the tokens.a number of neural architectures have been proposed for modelling translation, such as encoder-decoder networks, networks with attentional pooling and twodimensional networks. despite the generally good performance, the proposed models arxiv:1610.10099v2 15 mar 2017 eos eos eos |s| |t| |t|. dynamic unfolding in the bytenet architecture. at each step the decoder is conditioned on the source representation produced by the encoder for that step, or simply on no representation for steps beyond the extended length |t|. the decoding ends when the target network produces an end-of-sequence symbol. either have running time that is super-linear in the length of the source and target sequences, or they process the source sequence into a constant size representation, burdening the model with a memorization step. both of these drawbacks grow more severe as the length of the sequences increases.we present a family of encoder-decoder neural networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above. the first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal resolution of the sequences; this is in contrast with architectures that encode the source into a fixed-size representation. the second mechanism is the dynamic unfolding mechanism that allows the network to process in a simple and efficient way source and target sequences of different lengths .the bytenet is the instance within this family of models that uses one-dimensional convolutional neural networks of fixed depth for both the encoder and the decoder). the two cnns use increasing factors of dilation to rapidly grow the receptive fields; a similar technique is also used in. the convolutions in the decoder cnn are masked to prevent the network from seeing future tokens in the target sequence .the network has beneficial computational and learning properties. from a computational perspective, the network has a running time that is linear in the length of the source and target sequences . the computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences . from a learning perspective, the representation of the source sequence in the bytenet is resolution preserving; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder. in addition, the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis-tance between the tokens. dependencies overlarge distances are connected by short paths and can be learnt more easily.we apply the bytenet model to strings of characters for character-level language modelling and character-tocharacter machine translation. we evaluate the decoder network on the hutter prize wikipedia task where it achieves the state-of-the-art performance of 1.31 bits/character. we further evaluate the encoderdecoder network on character-to-character machine translation on the english-to-german wmt benchmark where it achieves a state-of-the-art bleu score of 22.85 and 25.53 on the 2014 and 2015 test sets, respectively. on the character-level machine translation task, bytenet betters a comparable version of gnmt that is a state-of-the-art system. these results show that deep cnns are simple, scalable and effective architectures for challenging linguistic processing tasks.the paper is organized as follows. section 2 lays out the background and some desiderata for neural architectures underlying translation models. section 3 defines the proposed family of architectures and the specific convolutional instance used in the experiments. section 4 analyses bytenet as well as existing neural translation models based on the desiderata set out in section 2. section 5 reports the experiments on language modelling and section 6 reports the experiments on character-to-character machine translation."
    ],
    [
      2,
      "recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.recurrent models typically factor computation along the symbol positions of the input and output sequences. aligning the positions to steps in computation time, they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t. this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. the fundamental constraint of sequential computation, however, remains.attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. in all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.in this work we propose the transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. the transformer allows for significantly more parallelization and can reach anew state of the art in translation quality after being trained for as little as twelve hours on eight p100 gpus."
    ],
    [
      3,
      "neural machine translation has attracted a lot of interest in solving the machine translation problem in recent years. unlike conventional statistical machine translation systems which consist of multiple separately tuned components, nmt models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. moreover, nmt models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation.in general, there are two types of nmt topologies: the encoder-decoder network and the attention network. the encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword. the attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. recent results show that the systems based on these models can achieve similar performance to conventional smt systems.however, a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the wmt'14 english-to-french task. the best bleu score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0.we focus on improving the single model perfor-mance by increasing the model depth. deep topology has been proven to outperform the shallow architecture in computer vision. in the past two years the top positions of the imagenet contest have always been occupied by systems with tensor even hundreds of layers. but in nmt, the biggest depth used successfully is only six. we attribute this problem to the properties of the long short-term memory which is widely used in nmt. in the lstm, there are more non-linear activations than in convolution layers. these activations significantly decrease the magnitude of the gradient in the deep topology, especially when the gradient propagates in recurrent form. there are also many efforts to increase the depth of the lstm such as the work by, where the shortcuts do not avoid the nonlinear and recurrent computation.in this work, we introduce anew type of linear connections for multi-layer recurrent networks. these connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. in addition, we introduce an interleaved bi-directional architecture to stack lstm layers in the encoder. this topology can be used for both the encoder-decoder network and the attention network. on the wmt'14 englishto-french task, this is the deepest nmt topology that has ever been investigated. with our deep attention model, the bleu score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 bleu points. this is also the first time on this task that a single nmt model achieves state-of-the-art performance and outperforms the best conventional smt system with an improvement of 0.7. even without using the attention mechanism, we can still achieve 36.3 with a single model. after model ensembling and unknown word processing, the bleu score can be further improved to 40.4. when evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. as a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the smt model can achieve the bleu score of about 45. our models are also validated on the more difficult wmt'14 english-to-german task."
    ],
    [
      4,
      "neural machine translation, directly applying a single neural network to transform the source sentence into the target sentence, has now reached impressive performance. the nmt typically consists of two sub neural networks. the encoder network reads and encodes the source sentence into a 1 feng wang is the corresponding author of this paper context vector, and the decoder network generates the target sentence iteratively based on the context vector. nmt can be studied in supervised and unsupervised learning settings. in the supervised setting, bilingual corpora is available for training the nmt model. in the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages. due to lack of alignment information, the unsupervised nmt is considered more challenging. however, this task is very promising, since the monolingual corpora is usually easy to be collected.motivated by recent success in unsupervised cross-lingual embeddings, the models proposed for unsupervised nmt often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space. following this assumption, use a single encoder and a single decoder for both the source and target languages. the encoder and decoder, acting as a standard auto-encoder , are trained to reconstruct the inputs. and utilize a shared encoder but two independent decoders. with some good performance, they share a glaring defect, i.e., only one encoder is shared by the source and target languages. although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure. since each language has its own characteristics, the source and target languages should be encoded and learned independently. therefore, we conjecture that the shared encoder maybe a factor limit-ing the potential translation performance.in order to address this issue, we extend the encoder-shared model, i.e., the model with one shared encoder, by leveraging two independent encoders with each for one language. similarly, two independent decoders are utilized. for each language, the encoder and its corresponding decoder perform an ae, where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations. to map the latent representations from different languages to a shared-latent space, we propose the weightsharing constraint to the two aes. specifically, we share the weights of the last few layers of two encoders that are responsible for extracting highlevel representations of input sentences. similarly, we share the weights of the first few layers of two decoders. to enforce the shared-latent space, the word embeddings are used as a reinforced encoding component in our encoders. for cross-language translation, we utilize the backtranslation following. additionally, two different generative adversarial networks , namely the local and global gan, are proposed to further improve the cross-language translation. we utilize the local gan to constrain the source and target latent representations to have the same distribution, whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation. we apply the global gan to finetune the corresponding generator, i.e., the composition of the encoder and decoder of the other language, where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 . in summary, we mainly make the following contributions: we propose the weight-sharing constraint to unsupervised nmt, enabling the model to utilize an independent encoder for each language. to enforce the shared-latent space, we also propose the embedding-reinforced encoders and two different gans for our model. we conduct extensive experiments on the code that we utilized to train and evaluate our models can be found at link english-german, english-french and chinese-to-english translation tasks. experimental results show that the proposed approach consistently achieves great success. last but not least, we introduce the directional self-attention to model temporal order information for the proposed model. experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self-attention layers of nmt."
    ],
    [
      5,
      "neural machine translation is a rapidly changing research area. since 2016 when nmt systems first showed to achieve significantly better results than statistical machine translation systems, the dominant neural network architectures for nmt have changed on a yearly basis. the state-of-the-art in 2016 were shallow attention-based recurrent neural networks with gated recurrent units in recurrent layers. in 2017, multiplicative long short-term memory units and deep gru models were introduced in nmt. the same year, selfattentional models were introduced. consequently, in 2018, most of the top scoring systems in the shared task on news translation of the third conference on machine translation were trained using transformer models 1 . however, it is already evident that the state-of-the-art architectures will 1 all 14 of the best automatically scored systems according to the information provided by participants in the official submission portal link were indicated as being based on transformer models. be pushed even further in 2018. for instance, have recently proposed rnmt+ models that combine deep lstm-based models with multi-head attention and showed that the models outperform transformer models.in wmt 2017, tilde participated with mlstm-based nmt systems. in this paper, we compare the mlstmbased models with transformer models for english-estonian and estonian-english and we show that the state-of-the-art of wmt 2017 is well behind the new models. therefore, for wmt 2018, tilde submitted nmt systems that were trained using transformer models.the paper is further structured as follows: section 2 provides an overview of systems submitted for the wmt 2018 shared task on news translation, section 3 describes the data used to train the nmt systems and the data pre-processing workflows, section 4 describes all nmt systems trained and experiments on handling of named entities and combination of systems, section 5 provides automatic evaluation results, and section 6 concludes the paper."
    ],
    [
      6,
      "word embeddings, which are distributed and continuous vector representations for word tokens, have been one of the basic building blocks for many neural network-based models used in natural language processing tasks, such as language modeling, text classification and machine translation. different from classic one-hot representation, the learned word embeddings contain semantic information which can measure the semantic similarity between words, and can also be transferred into other learning tasks.in deep learning approaches for nlp tasks, word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters. as the inputs of the neural network, word embeddings carryall the information of words that will be further processed by the network, and the quality of embeddings is critical and highly impacts the final performance of the learning task. unfortunately, we find the word embeddings learned by many deep learning approaches are far from perfect. as shown in and 1, in the embedding space learned by word2vec model, the nearest neighbors of word \"peking\" includes \"quickest\", \"multicellular\", and \"epigenetic\", which are not semantically similar, while semantically related words such as \"beijing\" and \"china\" are far from it. similar phenomena are observed from the word embeddings learned from translation tasks.with a careful study, we find a more general problem which is rooted in low-frequency words in the text corpus. without any confusion, we also call high-frequency words as popular words and call low-frequency words as rare words. as is well known, the frequency distribution of words roughly follows a simple mathematical form known as zipf's law. when the size of a text corpus grows, the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words. interestingly, the learned embeddings of rare words and popular words behave differently. in the embedding space, a popular word usually has semantically related neighbors, while a rare word usually does not. moreover, the nearest neighbors of more than 85% rare words are rare words. word embeddings encode frequency information. as shown in and 1, the embeddings of rare words and popular words actually lie in different subregions of the space. such a phenomenon is also observed in.we argue that the different behaviors of the embeddings of popular words and rare words are problematic. first, such embeddings will affect the semantic understanding of words. we observe more than half of the rare words are nouns or variants of popular words. those rare words should have similar meanings or share the same topics with popular words. second, the neighbors of a large number of rare words are semantically unrelated rare words. to some extent, those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding. it will consequently limit the performance of down-stream tasks using the embeddings. for example, in text classification, it cannot be well guaranteed that the label of a sentence does not change when you replace one popular/rare word in the sentence by its rare/popular alternatives.to address this problem, in this paper, we propose an adversarial training method to learn frequency-agnostic word embedding . for a given nlp task, in addition to minimize the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word. the discriminator optimizes its parameters to maximize its classification accuracy, while word embeddings are optimized towards a low task-dependent loss as well as fooling the discriminator to mis-classify the popular and rare words. when the whole training process converges and the system achieves an equilibrium, the discriminator cannot well differentiate popular words from rare words. consequently, rare words lie in the same region as and are mixed with popular words in the embedding space. then frage will catch better semantic information and help the task-specific model to perform better.we conduct experiments on four types of nlp tasks, including three word similarity tasks, two language modeling tasks, three sentiment classification tasks and two machine translation tasks to test our method. in all tasks, frage outperforms the baselines. specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on ptb, wt2 and wmt14 english-german datasets."
    ],
    [
      7,
      "due to the explosive growth of data, subset selection methods are increasingly popular fora wide range of machine learning and computer vision applications. this kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset. by analyzing a few, we can roughly know all. such case is very important to summarize and visualize huge datasets of texts, images and videos etc.. besides, by only using the selected exemplars for succeeding tasks, the cost of memories and computational time will be greatly reduced. additionally, as outliers are generally less representative, the side effect of outliers will be reduced, thus boosting the performance of subsequent applications.there have been several subset selection methods. the most intuitional method is to randomly select a fixed number of samples. although highly efficient, there is no guarantee for an effective selection. for the other methods, depending on the mechanism of representative exemplars, there are mainly three categories of selection methods. one category data size selection time"
    ],
    [
      8,
      "named entity recognition is a challenging learning problem. one the one hand, inmost languages and domains, there is only a very small amount of supervised training data available. on the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. as a result, carefully constructed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. unfortunately, languagespecific resources and features are costly to develop in new languages and new domains, making ner a challenge to adapt. unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision. however, even systems that have relied extensively on unsupervised features have used these to augment, rather than replace, hand-engineered features and specialized knowledge resources .in this paper, we present neural architectures for ner that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. our models are designed to capture two intuitions. first, since names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important. we compare two models here, a bidirectional lstm with a sequential conditional random layer above it , and anew model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack lstms . second, token-level evidence for \"being a name\" includes both orthographic evidence and distributional evidence . to capture orthographic sensitivity, we use character-based word representation model to capture distributional sensitivity, we combine these representations with distributional representations. our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence .experiments in english, dutch, german, and spanish show that we are able to obtain state-of-the-art ner performance with the lstm-crf model in dutch, german, and spanish, and very near the state-of-the-art in english without any hand-engineered features or gazetteers . the transition-based algorithm likewise surpasses the best previously published results in several languages, although it performs less well than the lstm-crf model."
    ],
    [
      9,
      "in order to democratize large-scale nlp and information extraction while minimizing our environmental footprint, we require fast, resource-efficient methods for sequence tagging tasks such as part-of-speech tagging and named entity recognition . speed is not sufficient of course: they must also be expressive enough to tolerate the tremendous lexical variation in input data.the massively parallel computation facilitated by gpu hardware has led to a surge of successful neural network architectures for sequence labeling. while these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a gpu, and thus their speed is limited. specifically, they employ either recurrent neural networks for feature extraction, or viterbi inference in a structured output model, both of which require sequential computation across the length of the input.instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing gpu resource usage and minimizing the amount of time it takes to train and evaluate models. convolutional neural networks provide exactly this property. rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once. their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware. this provides, for example, audio generation models that can be trained in parallel + 1. the number of layers required to incorporate the entire input context grows linearly with the length of the sequence. to avoid this scaling, one could pool representations across the sequence, but this is not appropriate for sequence labeling, since it reduces the output resolution of the representation.in response, this paper presents an application of dilated convolutions for sequence labeling). for dilated convolutions, the effective input width can grow exponentially with the depth, with no loss in resolution at each layer and with a modest number of parameters to estimate. like typical cnn layers, dilated convolutions operate on a sliding window of context over the sequence, but unlike conventional convolutions, the context need not be consecutive; the dilated window skips over every dilation width d inputs. by stacking layers of dilated convolutions of exponentially increasing dilation width, we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers: the size of the effective input width fora token at layer l is now given by 2 l+1 ?1. more concretely, just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens -longer than the average sentence length in the penn treebank.our overall iterated dilated cnn architecture repeatedly applies the same block of dilated convolutions to token-wise representations. this parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network. similar to models that use logits produced by an rnn, the id-cnn provides two methods for performing prediction: we can predict each token's label independently, or by running viterbi inference in a chain structured graphical model.in experiments on conll 2003 and ontonotes 1 what we call effective input width here is known as the receptive field in the vision literature, drawing an analogy to the visual receptive field of a neuron in the retina.: a dilated cnn block with maximum dilation width 4 and filter width 3. neurons contributing to a single highlighted neuron in the last layer are also highlighted. 5.0 english ner, we demonstrate significant speed gains of our id-cnns over various recurrent models, while maintaining similar f1 performance. when performing prediction using independent classification, the id-cnn consistently outperforms a bidirectional lstm , and performs on par with inference in a crf with logits from a bi-lstm . as an extractor of per-token logits fora crf, our model out-performs the bi-lstm-crf. we also apply id-cnns to entire documents, where independent token classification is as accurate as the bi-lstm-crf while decoding almost 8 faster. the clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive nlp tasks which have until now been limited by the computational complexity of existing context-rich models. 2 2 background"
    ],
    [
      10,
      "due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in nlp systems. many prior studies have shown that they capture useful semantic and syntactic information and including them in nlp systems has been shown to be enormously helpful fora variety of downstream tasks.however, in many nlp tasks it is essential to represent not just the meaning of a word, but also the word in context. for example, in the two phrases \"a central bank spokesman\" and \"the central african republic\", the word 'central' is used as part of both an organization and location. accordingly, current state of the art sequence tagging models typically include a bidirectional re-current neural network that encodes token sequences into a context sensitive representation before making token specific predictions.although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional rnn are typically learned only on labeled data. previous work has explored methods for jointly learning the bidirectional rnn with supplemental labeled data from other tasks , pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence and use it in the supervised sequence tagging model. since the lm embeddings are used to compute the probability of future words in a neural lm, they are likely to encode both the semantic and syntactic roles of words in context.our main contribution is to show that the context sensitive representation captured in the lm embeddings is useful in the supervised sequence tagging setting. when we include the lm embeddings in our system overall performance increases from 90.87% to 91.93% f 1 for the conll 2003 ner task, a more then 1% absolute f1 increase, and a substantial improvement over the previous state of the art. we also establish anew state of the art result for the conll 2000 chunking task.as a secondary contribution, we show that using both forward and backward lm embeddings boosts performance over a forward only lm. we also demonstrate that domain specific pre-training is not necessary by applying a lm trained in the news domain to scientific papers. the main components in our language-modelaugmented sequence tagger are illustrated in. after pre-training word embeddings and a neural lm on large, unlabeled corpora , we extract the word and lm embeddings for every token in a given input sequencestep 2) and use them in the supervised sequence tagging model ."
    ],
    [
      11,
      "pre-trained word representations area key component in many neural language understanding models. however, learning high quality representations can be challenging. they should ideally model both complex characteristics of word use , and how these uses vary across linguistic contexts . in this paper, we introduce anew type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems.our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. we use vectors derived from a bidirectional lstm that is trained with a coupled lan-guage model objective on a large text corpus. for this reason, we call them elmo representations. unlike previous approaches for learning contextualized word vectors, elmo representations are deep, in the sense that they area function of all of the internal layers of the bilm. more specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top lstm layer.combining the internal states in this manner allows for very rich word representations. using intrinsic evaluations, we show that the higher-level lstm states capture context-dependent aspects of word meaning while lowerlevel states model aspects of syntax . simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.extensive experiments demonstrate that elmo representations work extremely well in practice. we first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. the addition of elmo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. for tasks where direct comparisons are possible, elmo outperforms cove, which computes contextualized representations using a neural machine translation encoder. finally, an analysis of both elmo and cove reveals that deep representations outperform arxiv:1802.05365v2 22 mar 2018 those derived from just the top layer of an lstm. our trained models and code are publicly available, and we expect that elmo will provide similar gains for many other nlp problems. 1 2 related work due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors area standard component of most state-ofthe-art nlp architectures, including for question answering, textual entailment and semantic role labeling. however, these approaches for learning word vectors only allow a single contextindependent representation for each word.previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information uses a bidirectional long short term memory to encode the context around a pivot word. other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation system or an unsupervised language model. both of these approaches benefit from large datasets, although the mt approach is limited by the size of parallel corpora. in this paper, we take full advantage of access to plentiful monolingual data, and train our bilm on a corpus with approximately 30 million sentences. we also generalize these approaches to deep contextual representations, which we show work well across abroad range of diverse nlp tasks. 1 link previous work has also shown that different layers of deep birnns encode different types of information. for example, introducing multi-task syntactic supervision at the lower levels of a deep lstm can improve overall performance of higher level tasks such as dependency parsing or ccg super tagging. in an rnn-based encoder-decoder machine translation system, showed that the representations learned at the first layer in a 2layer lstm encoder are better at predicting pos tags then second layer. finally, the top layer of an lstm for encoding word context has been shown to learn representations of word sense. we show that similar signals are also induced by the modified language model objective of our elmo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. and pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. in contrast, after pretraining the bilm with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal bilm representations for cases where downstream training data size dictates a smaller supervised model."
    ]
  ]
}