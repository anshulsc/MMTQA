{
  "columns": [
    "Modello",
    "Innovazione chiave",
    "Metodologia",
    "Principali risultati"
  ],
  "data": [
    [
      "Timer",
      "Decoder pre-addestrato solo transformer",
      "Formato unificato di sequenza per dati eterogenei",
      "Ottima performance con pochi esempi al 1-5% dei dati"
    ],
    [
      "MOMENT",
      "Famiglia di modelli open-source",
      "Strategie di addestramento multi-dataset",
      "Performance superiori con supervisione limitata"
    ],
    [
      "TimeMixer",
      "Architettura MLP a multiscale",
      "Blocchi di miscelazione decomponibili per pattern temporali",
      "SOTA su 18 benchmark"
    ],
    [
      "TimesNet",
      "Analisi multi-periodicit√†",
      "Trasformazione da 1D a 2D per pattern temporali",
      "Performance unificate su 5 compiti principali"
    ],
    [
      "PatchTST",
      "Elaborazione delle serie temporali basata su patch",
      "Transformer indipendente dal canale",
      "Riduzione del 21% dell'errore MSE nella previsione a lungo termine"
    ]
  ]
}