{
  "columns": [
    "Model",
    "Key Innovation",
    "Methodology",
    "Key Results"
  ],
  "data": [
    [
      "Timer",
      "Pre-trained decoder-only transformer",
      "Single sequence format for diverse data",
      "Strong few-shot performance using only 1-5% of the data"
    ],
    [
      "MOMENT",
      "Open source foundation model family",
      "Training strategy using multiple datasets",
      "Superior performance in limited supervised learning"
    ],
    [
      "TimeMixer",
      "Multi-scale MLP architecture",
      "Decomposable mixing block for temporal patterns",
      "SOTA on 18 standard benchmarks"
    ],
    [
      "TimesNet",
      "Multi-periodicity analysis",
      "1D to 2D transformation for temporal patterns",
      "State-of-the-art performance in 5 major tasks"
    ],
    [
      "PatchTST",
      "Patch-based temporal aggregation",
      "Channel-independent Transformer",
      "21% reduction in MSE for long-term forecasting"
    ]
  ]
}