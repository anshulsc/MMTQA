{
  "columns": [
    "Modèle",
    "Innovation clé",
    "Méthodologie",
    "Résultats principaux"
  ],
  "data": [
    [
      "Timer",
      "Transformer pré-entraîné à décodeur seul",
      "Format de séquence unifié pour les données hétérogènes",
      "Bonne performance en few-shot avec 1-5 % des données"
    ],
    [
      "MOMENT",
      "Famille de modèles de base open-source",
      "Stratégies d'entraînement multi-jouets",
      "Performance supérieure en apprentissage à supervision limitée"
    ],
    [
      "TimeMixer",
      "Architecture MLP à plusieurs échelles",
      "Blocs de mélange décomposables pour les motifs temporels",
      "Meilleure performance sur 18 benchmarks"
    ],
    [
      "TimesNet",
      "Analyse multi-périodique",
      "Transformation 1D à 2D pour les motifs temporels",
      "Performance unifiée sur 5 tâches majeures"
    ],
    [
      "PatchTST",
      "Traitement des séries temporelles par patch",
      "Transformer indépendant du canal",
      "Réduction de 21 % de l'erreur quadratique moyenne dans la prévision à long terme"
    ]
  ]
}