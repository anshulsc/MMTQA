{
  "columns": [
    "Model",
    "Key Development",
    "Methodology",
    "Key Results"
  ],
  "data": [
    [
      "Timer",
      "Pre-trained Transformer, which is decoder only",
      "A single sequential form for various data",
      "Strong performance in 'few-shot' with 1-5% data"
    ],
    [
      "MOMENT",
      "Knowledge-based foundation model family",
      "Training strategies on massive data",
      "Excellent performance under minimal supervision"
    ],
    [
      "TimeMixer",
      "Multi-layer MLP architecture",
      "Shareable state embeddings for time series",
      "SOTA on 18 benchmarks"
    ],
    [
      "TimesNet",
      "Multi-periodicity analysis",
      "Transformation from 1D to 2D for time series",
      "Unified performance on 5 key tasks"
    ],
    [
      "PatchTST",
      "'Patch'-based time series sequencing method",
      "Channel-independent Transformer",
      "21% reduction in MSE for long-term forecasting"
    ]
  ]
}