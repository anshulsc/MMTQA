{
  "columns": [
    "Model",
    "Key Innovation",
    "Methodology",
    "Key Result"
  ],
  "data": [
    [
      "Timer",
      "Pre-train only the decoder transformer",
      "Unify sequence format for different types of data",
      "Achieve robust few-shot performance using 1-5% of data"
    ],
    [
      "MOMENT",
      "Open-source foundational model family",
      "Multi-dataset training strategy",
      "Superior limited supervision performance"
    ],
    [
      "TimeMixer",
      "Multi-scale MLP architecture",
      "Decomposable mixing blocks used to process temporal patterns",
      "Achieved SOTA on 18 benchmark tasks"
    ],
    [
      "TimesNet",
      "Multi-periodicity analysis",
      "1D to 2D transformation used to process temporal patterns",
      "Achieved unified performance on 5 key tasks"
    ],
    [
      "PatchTST",
      "Patch-based time series processing",
      "Channel-independent transformer",
      "In long-term forecasting, MSE reduced by 21%"
    ]
  ]
}