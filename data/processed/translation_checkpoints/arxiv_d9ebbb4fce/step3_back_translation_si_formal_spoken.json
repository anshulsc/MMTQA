{
  "columns": [
    "Model",
    "Main Innovation",
    "Methodology",
    "Key Results"
  ],
  "data": [
    [
      "Timer",
      "Transformer with only a pre-trained decoder",
      "Unified sequence model for heterogeneous data",
      "Strong 'few-shot' performance with 1-5% data"
    ],
    [
      "MOMENT",
      "Open source foundation model family",
      "Means of training multiple datasets",
      "Superior performance under limited supervision"
    ],
    [
      "TimeMixer",
      "Multi-scale MLP architecture",
      "Decomposable mixing blocks for temporal data",
      "SOTA across 18 benchmarks"
    ],
    [
      "TimesNet",
      "Multi-periodicity analysis",
      "1D to 2D transformation for temporal data",
      "Unified performance across 5 key tasks"
    ],
    [
      "PatchTST",
      "Patch-based time series processing",
      "Channel-independent Transformer",
      "21% MSE reduction in long-term forecasting"
    ]
  ]
}