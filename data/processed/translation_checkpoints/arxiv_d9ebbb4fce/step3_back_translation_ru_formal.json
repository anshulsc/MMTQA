{
  "columns": [
    "Model",
    "Key Innovation",
    "Methodology",
    "Main Results"
  ],
  "data": [
    [
      "Timer",
      "Pre-trained transformer based only on the decoder",
      "Unified sequence format for heterogeneous data",
      "High performance in few-shot mode (when using 1-5% of data)"
    ],
    [
      "MOMENT",
      "Family of foundation models with open source code",
      "Training strategies on multiple datasets",
      "Excellent performance under limited supervision"
    ],
    [
      "TimeMixer",
      "Multi-scale MLP architecture",
      "Decomposable mixing blocks for temporal patterns",
      "SOTA (State-of-the-Art) on 18 benchmark datasets"
    ],
    [
      "TimesNet",
      "Multi-periodicity analysis",
      "1D to 2D transformation for temporal pattern analysis",
      "Unified performance across 5 main tasks"
    ],
    [
      "PatchTST",
      "Patch-based time series processing",
      "Channel-independent transformer",
      "21% MSE reduction in long-term forecasting tasks"
    ]
  ]
}