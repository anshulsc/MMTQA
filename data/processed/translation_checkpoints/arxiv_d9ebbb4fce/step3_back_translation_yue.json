{
  "columns": [
    "Model",
    "Key Innovation",
    "Methodology",
    "Main Results"
  ],
  "data": [
    [
      "Timer",
      "Pre-trained Decoder-only Transformer",
      "Unified sequence format for heterogeneous data",
      "Exhibits strong few-shot performance using only 1-5% of the data"
    ],
    [
      "MOMENT",
      "Open-source foundation model series",
      "Multi-dataset training strategy",
      "Superior limited supervision performance"
    ],
    [
      "TimeMixer",
      "Multi-scale MLP architecture",
      "Decomposable mixing block for temporal patterns",
      "Achieves SOTA in 18 benchmarks"
    ],
    [
      "TimesNet",
      "Multi-periodicity analysis",
      "Converts 1D to 2D to handle temporal patterns",
      "Achieves unified performance across 5 major tasks"
    ],
    [
      "PatchTST",
      "Patch-based time series processing",
      "Channel-independent Transformer",
      "In long-term forecasting, MSE reduced by 21%"
    ]
  ]
}