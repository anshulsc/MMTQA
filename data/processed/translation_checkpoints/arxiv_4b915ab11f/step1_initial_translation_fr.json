{
  "columns": [],
  "data": [
    [
      "LLM préentraîné",
      "Taille du corpus",
      "Budget d'entraînement (A100·heures)",
      "Architecture du modèle"
    ],
    [
      "BloomBergGPT",
      "363B Finance tokens + 345B public tokens",
      "1300000",
      "50B-BLOOM"
    ],
    [
      "XuanYuan2.0",
      "366B pour l'entraînement préalable + 13B pour l'entraînement fin",
      "Not released",
      "176B-BLOOM"
    ],
    [
      "Fin-T5",
      "80B Finance tokens",
      "Days/weeks",
      "770M-T5"
    ]
  ]
}