{
  "columns": [],
  "data": [
    [
      "Pretrained LLM",
      "Tamaño del corpus",
      "Presupuesto de entrenamiento (A100·horas)",
      "Arquitectura del modelo"
    ],
    [
      "BloomBergGPT",
      "363B Finance tokens + 345B public tokens",
      "1300000",
      "50B-BLOOM"
    ],
    [
      "XuanYuan2.0",
      "366B para pre-entrenamiento + 13B para ajuste fino",
      "No publicado",
      "176B-BLOOM"
    ],
    [
      "Fin-T5",
      "80B Finance tokens",
      "Días/semanas",
      "770M-T5"
    ]
  ]
}