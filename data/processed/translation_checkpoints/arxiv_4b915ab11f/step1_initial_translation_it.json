{
  "columns": [],
  "data": [
    [
      "Pretrained LLM",
      "Dimensione del corpus",
      "Budget di addestramento (A100Â·ore)",
      "Architettura del modello"
    ],
    [
      "BloomBergGPT",
      "363B Finance tokens + 345B public tokens",
      "1300000",
      "50B-BLOOM"
    ],
    [
      "XuanYuan2.0",
      "366B per pre-addestramento + 13B per finetuning",
      "Non rilasciato",
      "176B-BLOOM"
    ],
    [
      "Fin-T5",
      "80B Finance tokens",
      "Giorni/settimane",
      "770M-T5"
    ]
  ]
}