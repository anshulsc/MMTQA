[
  {
    "question_id": "arxiv_4b915ab11f_001",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "Tī chiah kó͘ chū liat chhut ê khóng-ìng tiong, ūi-tiāⁿ ê ki-chhú kiàn-siat koán-chè (parameter count) siōng tōa ê sī 'XuanYuan2.0'?",
    "answer": [
      [
        "XuanYuan2.0"
      ]
    ],
    "evidence_cells": [
      "A2",
      "D1",
      "D2",
      "D3"
    ],
    "reasoning_category": "Comparative Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_002",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "BloomBergGPT 训练所用的语料库，总共有几粒（tokens）？",
    "answer": [
      [
        "708B"
      ]
    ],
    "evidence_cells": [
      "B1"
    ],
    "reasoning_category": "Numerical Aggregation"
  },
  {
    "question_id": "arxiv_4b915ab11f_003",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "T5 bē-á-tē kiàn-siâⁿ ê model, i ê hùn-liān sī-sài hē-sòan-kè sī-gōa-chē?",
    "answer": [
      [
        "Days/weeks"
      ]
    ],
    "evidence_cells": [
      "A3",
      "C3",
      "D3"
    ],
    "reasoning_category": "Multi-Hop Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_004",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "Tī ūNN-ê ki-su Lô-chôa Lô-chôa bô͘-hêng, siáⁿ-chi̍t-ê chū-liāu-thâu ê sèng-chiong ūi-tio̍h chha-siat? Kū-liāu ê sèng-chiong ūi-tio̍h chha-siat sī siáⁿ-chi̍t-ê?",
    "answer": [
      [
        "366B"
      ]
    ],
    "evidence_cells": [
      "B2"
    ],
    "reasoning_category": "Temporal Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_005",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "E chiah-choa si koah-lō͘ ê LLM, i teh iōng BLOOM hō͘-chú ê kiàn-chek, koh ū siau-khòaⁿ ê chhoán-liāu hù-châi ê sìn-sit ê A100·钟?",
    "answer": [
      [
        "BloomBergGPT"
      ]
    ],
    "evidence_cells": [
      "A1",
      "C1",
      "D1",
      "C2",
      "D2"
    ],
    "reasoning_category": "Conditional Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_006",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "BloomBergGPT 總訓練資料集中，有百分之多少是公開資料？",
    "answer": [
      [
        "48.73%"
      ]
    ],
    "evidence_cells": [
      "B1"
    ],
    "reasoning_category": "Proportional/Ratio Analysis"
  },
  {
    "question_id": "arxiv_4b915ab11f_007",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "open_ended_reasoning",
    "question": "Nāi-kú, nāi-kú kóng-hoat XuanYuan2.0 ê hùn-liān hû-tâi chiâⁿ-chòe 2,500,000 ê A100·tiám-chū ê sî, che ē cháiⁿ-iáⁿ hō͘ hùn-liān hû-tâi ê pâi-miâ ū sím-mi̍h éng-hiáng?",
    "answer": [
      [
        "Bo̍k-chêng, BloomBergGPT sī siâⁿ-tō͘ ū bīn-chí ê hû-tâi (1,300,000 A100·tiám-chū), chiâⁿ-chòe tē-it. Nāi-kú, nāi-kú kóng-hoat XuanYuan2.0 ê hû-tâi sī 2,500,000 A100·tiám-chū, chiâⁿ-chòe chòe chhòe tē-it ê chiâⁿ-tō͘, chhiau-kòe BloomBergGPT. Sin ê pâi-miâ tùi ū bīn-chí ê hû-tâi ê bô͘-hêng chiâⁿ-chòe 1. XuanYuan2.0 (2,500,000) kap 2. BloomBergGPT (1,300,000)."
      ]
    ],
    "evidence_cells": [
      "A1",
      "C1",
      "A2",
      "C2"
    ],
    "reasoning_category": "Hypothetical Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_008",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "open_ended_reasoning",
    "question": "Chit-ê chu-liàu siá-chòe, muí-ê model ê khòan-îng phó-sek (model architecture) ê iōng-liōng ê sò͘-ba̍t, kap i teh chòe-liâu ê chu-liàu-khò͘ (training corpus) ê chióng-liōng ū tùi-tūi ê koan-liân bô?",
    "answer": [
      [
        "Bô, chāi chit-ê chu-liàu lāi-bīn bōe khòaⁿ-chóe ū tùi-tūi ê chián-chhoán koan-liân. Chhiūⁿ-kòe, XuanYuan2.0 ê khòan-îng phó-sek (176B iōng-liōng) khah tōa-ê khah-kè BloomBergGPT (50B iōng-liōng), tān-sī i teh chòe-liâu ê chu-liàu-khò͘ khah chió (379B token tùi 708B token). Che hō͘ lâng jīn-ûi, tōa-ê model bōe it-tēng su-iàu khah tōa ê chòe-liâu chu-liàu-khò͘ chiàu chit-ê chu-liàu koat-tēng."
      ]
    ],
    "evidence_cells": [
      "B1",
      "D1",
      "B2",
      "D2"
    ],
    "reasoning_category": "Correlation Inference"
  },
  {
    "question_id": "arxiv_4b915ab11f_009",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "Mlh kui-ê chu̍t-lē ga̍k-chāu ū siōng tōa ê piàn-hòa, lîm-chāi ū tōa ê sò͘-jī, chi̍t ê bô hoat-tō͘ soat-bêng ê sû, kap chi̍t ê sî-kan-tùi ê tùi-lia̍t?",
    "answer": [
      [
        "Training budget (A100·hours)"
      ]
    ],
    "evidence_cells": [
      "C1",
      "C2",
      "C3"
    ],
    "reasoning_category": "Structural/Metadata Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_010",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "Na-lah modèl ê training budget sī chin tōa, pí pa̍t-ê modèl ê budget khah chē?",
    "answer": [
      [
        "BloomBergGPT"
      ]
    ],
    "evidence_cells": [
      "A1",
      "C1",
      "C2",
      "C3"
    ],
    "reasoning_category": "Outlier Detection"
  }
]