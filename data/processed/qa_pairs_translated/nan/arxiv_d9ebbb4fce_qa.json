[
  {
    "question_id": "arxiv_d9ebbb4fce_001",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "H C A T K L I S O T A K I L A K N A H N A K I C H ",
    "answer": [
      [
        "TimeMixer"
      ]
    ],
    "evidence_cells": [
      "A3",
      "D3"
    ],
    "reasoning_category": "Comparative Reasoning"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_002",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Tī lia̍t chhut ê model lāi-bīn, ū kúi ê tī i ê ki-tiāⁿ sêng-chiū he̍k hong-hoat lāi-bīn ū hâm-ji̍p 'transformer' ki-su?",
    "answer": [
      [
        "3"
      ]
    ],
    "evidence_cells": [
      "B1",
      "B5",
      "C5"
    ],
    "reasoning_category": "Numerical Aggregation"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_003",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Eh gí-chá ê ki-chí siáⁿ-mih, tī iōng chē-ê chàu-kù-ha̍p chòe chong-lāi ê chòe-chhoán? (Which model uses multi-dataset training strategies? What is its key innovation?)",
    "answer": [
      [
        "Open-source foundation model family"
      ]
    ],
    "evidence_cells": [
      "B2",
      "C2"
    ],
    "reasoning_category": "Multi-Hop Reasoning"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_004",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "H CUIT MODEL-ê KHA-lí-ha̍p-sèng ū 1 kui chiat 1D ê sî-kan-á-liāu choán chòe 2D ê iáⁿ-phōaⁿ lâi hun-sek kó͘-sū?",
    "answer": [
      [
        "TimesNet"
      ]
    ],
    "evidence_cells": [
      "A4",
      "C4"
    ],
    "reasoning_category": "Temporal Reasoning"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_005",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Liet chhut ê model, sī ū 'foundation model' ia̍h-sī 'pre-trained' chòe in chhòng-sin ê hong-hoat?",
    "answer": [
      [
        "Timer"
      ],
      [
        "MOMENT"
      ]
    ],
    "evidence_cells": [
      "A1",
      "B1",
      "A2",
      "B2"
    ],
    "reasoning_category": "Conditional Reasoning"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_006",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Tī chit ê lia̍t-toaⁿ lāi-bīn, ū chē-chē bah-hun-chè ê model sú-iōng transformer ki-pún ê kiàn-chek?",
    "answer": [
      [
        "60%"
      ]
    ],
    "evidence_cells": [
      "B1",
      "B5",
      "C5",
      "A1",
      "A2",
      "A3",
      "A4",
      "A5"
    ],
    "reasoning_category": "Proportional/Ratio Analysis"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_007",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Juah cui buay, chi̍t-ê gián-kiù tūi ê chú-iàu hān-chè sī chin sió-sió ê hùn-liān chu-liāu (1-5%), lán-ê modèle ê kiat-kó, chi̍t-ê kiat-kó sī tiám-chāi khò-í sī siōng iú-hāu ê soán-chek?",
    "answer": [
      [
        "Timer"
      ]
    ],
    "evidence_cells": [
      "A1",
      "D1"
    ],
    "reasoning_category": "Hypothetical Reasoning"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_008",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "open_ended_reasoning",
    "question": "Ūi-tī piáⁿ lāi-bīn, ē-tàng tōa-iok kám-chiap ki-lâi, chi̍t ê model ê hong-hoat (methodology) kah i ê chú-iàu kiat-kó (primary results) ūi-chòe tùi transformer kiàn-tē (architectures) sī ū siáⁿ-mih koan-hē?",
    "answer": [
      [
        "Chiu-liāu sī chí-chhut, kám-chiap transformer kiàn-tē ê chè-tō (adoption) kah tī te̍k-tēng, khùn-lân ê chêng-hêng chi-hā saⁿ tòe ti̍t khah hó ê tîⁿ-kó (performance). Pí-lún-lú, Timer ê chìn-chhú hùn-liān ê kut-chiá-chháu-chháu transformer (pre-trained decoder-only transformer) tō ē hō· lâi hó ê chiòng-chiáⁿ chiáⁿ-chiá (few-shot performance), chiâⁿ-chhiūⁿ PatchTST ê tan-ūi-to̍k-li̍p transformer (channel-independent transformer) tō ē hō· lâi khah chēⁿ-chēⁿ ê MSE kiám-siàu tī chē-tn̂g-kî ī-chhek (long-term forecasting). Che sī chí-bêng transformer chè-tō ê hong-hoat sī te̍k-pia̍t ū hāu-lu̍t tī su-iàu chiáⁿ-chiá ê chu-liāu tiâu-kiāⁿ (data efficiency, chio̍h-chiáⁿ chiáⁿ-chiá ê ha̍k-si̍p) ia̍h-sī liâu-chhiap chē-tn̂g-khí-lâi ê koan-lian (capturing long-range dependencies, chē-tn̂g-kî ī-chhek)."
      ]
    ],
    "evidence_cells": [
      "B1",
      "C1",
      "D1",
      "B5",
      "C5",
      "D5"
    ],
    "reasoning_category": "Correlation Inference"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_009",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Hiam-mih bûn-chiáu pau-hâm múi chi̍t ê model ê ki-pún su-hoat-ha̍k, ti̍t-chiap/kiat-kò͘ ê hong-sek ê chu-liāu?",
    "answer": [
      [
        "Methodology"
      ]
    ],
    "evidence_cells": [
      "C1",
      "C2",
      "C3",
      "C4",
      "C5"
    ],
    "reasoning_category": "Structural/Metadata Reasoning"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_010",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Chit-ê model ê chòe-chiong kiat-kó ū hiah-nī tùi-khì, lán ē-sái thàu-kòe cheng-chòe ê sòan-liōng phiau-chún lâi khòaⁿ-kìⁿ (pí-lūn chòe-chiong ê thiàm-chúi ê sò͘-liōng), m̄-koh m̄-sī pí-lūn kóng \"hó\" kam-chūⁿ \"chòe-thiàm\" ê thàu-kòe?",
    "answer": [
      [
        "PatchTST"
      ]
    ],
    "evidence_cells": [
      "A5",
      "D1",
      "D2",
      "D3",
      "D4",
      "D5"
    ],
    "reasoning_category": "Outlier Detection"
  },
  {
    "question_id": "arxiv_d9ebbb4fce_011",
    "table_id": "arxiv_d9ebbb4fce",
    "question_type": "value",
    "question": "Diat-koán 18 ê benchmark dataset,Which model outperformed all the models across 18 benchmark datasets? (Diat-koán 18 ê benchmark dataset, Which model outperformed all the models across 18 benchmark datasets?)",
    "answer": [
      [
        "TimeMixer"
      ]
    ],
    "evidence_cells": [
      "A3",
      "D3"
    ],
    "reasoning_category": "Structural/Metadata Reasoning"
  }
]