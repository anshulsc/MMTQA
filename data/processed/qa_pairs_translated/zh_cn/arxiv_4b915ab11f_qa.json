[
  {
    "question_id": "arxiv_4b915ab11f_001",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "在列出的模型中，哪一个模型的基础架构参数量最大？",
    "answer": [
      [
        "XuanYuan2.0"
      ]
    ],
    "evidence_cells": [
      "A2",
      "D1",
      "D2",
      "D3"
    ],
    "reasoning_category": "Comparative Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_002",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "用于训练 BloomBergGPT 的语料库总共有多少个 token？",
    "answer": [
      [
        "708B"
      ]
    ],
    "evidence_cells": [
      "B1"
    ],
    "reasoning_category": "Numerical Aggregation"
  },
  {
    "question_id": "arxiv_4b915ab11f_003",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "基于T5架构构建的模型，其训练预算是多少？",
    "answer": [
      [
        "Days/weeks"
      ]
    ],
    "evidence_cells": [
      "A3",
      "C3",
      "D3"
    ],
    "reasoning_category": "Multi-Hop Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_004",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "对于指定了两阶段训练数据过程的模型，预训练阶段的语料库大小是多少？",
    "answer": [
      [
        "366B"
      ]
    ],
    "evidence_cells": [
      "B2"
    ],
    "reasoning_category": "Temporal Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_005",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "请找出使用了基于BLOOM的架构，并且其训练预算以A100·小时计的LLM。",
    "answer": [
      [
        "BloomBergGPT"
      ]
    ],
    "evidence_cells": [
      "A1",
      "C1",
      "D1",
      "C2",
      "D2"
    ],
    "reasoning_category": "Conditional Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_006",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "BloomBergGPT 的总训练语料库中有多少比例是公开代币？",
    "answer": [
      [
        "48.73%"
      ]
    ],
    "evidence_cells": [
      "B1"
    ],
    "reasoning_category": "Proportional/Ratio Analysis"
  },
  {
    "question_id": "arxiv_4b915ab11f_007",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "open_ended_reasoning",
    "question": "如果宣辕2.0的训练预算被揭示为2,500,000 A100·小时，这将如何影响按训练预算对模型进行排名？",
    "answer": [
      [
        "目前，BloomBergGPT是唯一指定了数值预算（1,300,000 A100·小时）的模型，因此排名第一。如果宣辕2.0的预算为2,500,000 A100·小时，它将成为已知训练预算最高的模型，超过BloomBergGPT。按数值预算排名的模型新顺序将是1. 宣辕2.0 (2,500,000) 和 2. BloomBergGPT (1,300,000)。"
      ]
    ],
    "evidence_cells": [
      "A1",
      "C1",
      "A2",
      "C2"
    ],
    "reasoning_category": "Hypothetical Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_008",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "open_ended_reasoning",
    "question": "根据这些数据，模型的架构参数量与其训练语料库的总大小之间是否存在一致的相关性？",
    "answer": [
      [
        "不，数据中没有显示出持续的正相关性。例如，XuanYuan2.0的架构（176B参数）远大于BloomBergGPT（50B参数），但它是在一个较小的语料库上训练的（379B tokens vs. 708B tokens）。这表明根据这个数据集，更大的模型不一定需要更大的训练语料库。"
      ]
    ],
    "evidence_cells": [
      "B1",
      "D1",
      "B2",
      "D2"
    ],
    "reasoning_category": "Correlation Inference"
  },
  {
    "question_id": "arxiv_4b915ab11f_009",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "哪个数据列在数据格式上表现出最大的差异，其中包含一个大整数、一个未公开的字符串和一个基于时间的范围？",
    "answer": [
      [
        "Training budget (A100·hours)"
      ]
    ],
    "evidence_cells": [
      "C1",
      "C2",
      "C3"
    ],
    "reasoning_category": "Structural/Metadata Reasoning"
  },
  {
    "question_id": "arxiv_4b915ab11f_010",
    "table_id": "arxiv_4b915ab11f",
    "question_type": "value",
    "question": "与其他模型的可用预算信息相比，哪个模型的规定训练预算是一个显著的正异常值？",
    "answer": [
      [
        "BloomBergGPT"
      ]
    ],
    "evidence_cells": [
      "A1",
      "C1",
      "C2",
      "C3"
    ],
    "reasoning_category": "Outlier Detection"
  }
]