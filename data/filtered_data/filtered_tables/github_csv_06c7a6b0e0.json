{
  "columns": [
    "id",
    "title",
    "description",
    "project_name",
    "status_name",
    "priority_id",
    "type_id",
    "assignee_id",
    "labels"
  ],
  "data": [
    [
      13415814,
      "ResourceLimitCache leaks permits",
      "{{ResourceLimitCache}} limits its size by using a group of permits.  {{put}} requires free permit before it can add data.  However, {{removeIf}} does not release permits.",
      "HDDS",
      "Resolved",
      2,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13543455,
      "Log more information about failed EC block allocation",
      "We should include more information in the log if EC block allocation is rejected due to too many open pipelines.  ALLOCATE_BLOCK request's ExcludeList is not included in SCM audit log (maybe due to potentially large size?).  However, this piece of information may be important to understand why block allocation fails.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13517234,
      "Simplify DatanodeDetails#toString to improve log messages",
      "{{DatanodeDetails#toString}} outputs too much detail for it to be usable in each and every log message related to datanodes.  Thus log statements currently have to build their own output (e.g. for list of hosts, etc.), leading to duplication.\n\nThe goal of this task is to replace {{toString}} with less verbose ({{getHostnameAndIP}} + UUID) implementation, and keep the current one for occasional usage as {{toDebugString}}.  Need to be verify if this causes and regressions.",
      "HDDS",
      "Resolved",
      3,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13261237,
      "Avoid buffer copying in GrpcReplicationService",
      "In GrpcOutputStream, it writes data to a ByteArrayOutputStream and copies them to a ByteString.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "performance, pull-request-available"
    ],
    [
      13316901,
      "Ratis config key mismatch",
      "Some of the Ratis configurations in integration tests are not applied due to mismatch in config keys.\n # [Ratis|https://github.com/apache/incubator-ratis/blob/master/ratis-client/src/main/java/org/apache/ratis/client/RaftClientConfigKeys.java#L41-L53]: {{raft.client.rpc.watch.request.timeout}}\n [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java#L119-L122]: {{raft.client.watch.request.timeout}}\n # [Ratis|https://github.com/apache/incubator-ratis/blob/4db4f804aa90f9900cda08c79b54a45f80f4213b/ratis-server/src/main/java/org/apache/ratis/server/RaftServerConfigKeys.java#L470-L473]: {{raft.server.notification.no-leader.timeout}}\n [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/conf/DatanodeRatisServerConfig.java#L42]: {{raft.server.Notification.no-leader.timeout}}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13437800,
      "Bump Jackson Databind",
      "Upgrade Jackson Databind to latest.",
      "HDDS",
      "Resolved",
      3,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13562953,
      "Clean up test dependencies",
      " * Provide the same set of basic test dependencies for all modules.\n * Remove leftover dependencies (mostly related to JUnit4).\n",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13590128,
      "Log for EC reconstruction command lists the missing indexes as ASCII control characters",
      "Logs for EC reconstruction command lists the missing indexes as ASCII control characters like ^A ^B.\n\n\u00a0\n{noformat}\n2024-08-27 05:28:34,857 INFO [node1-UnderReplicatedProcessor]-org.apache.hadoop.hdds.scm.container.replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 15001, replicationConfig: EC{rs-3-2-1024k}, sources: [693753b9-cfb5-4bcc-9863-273cf3d32d05 replicaIndex: 3, 20213e45-1d8c-4224-96be-83d2c7f85c00 replicaIndex: 4, d6d0d2b5-dab9-48ec-8866-54b3d70f8b37 replicaIndex: 5], targets: [dd2d9383-a4c8-4aa9-9290-7d5494618a3a, 9b08f05b-2cbb-4bba-bcf5-1c1bd5d1ac01], missingIndexes: ^A^B] for container ContainerInfo{id=#15001, state=CLOSED, stateEnterTime=2024-08-27T05:26:28.228489Z, pipelineID=PipelineID=2b855dc6-d3fb-47a4-87d2-4b4b525bbae3, owner=ozone1724217699} to dd2d9383-a4c8-4aa9-9290-7d5494618a3a with datanode deadline 1724737384857 and scm deadline 1724737414857{noformat}\nThis is due to the change made as part of HDDS-10756",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13316718,
      "Avoid HddsProtos.PipelineID#toString",
      "{{PipelineID}} was recently changed to have integer-based ID in addition to the string ID.  Now log messages including {{PipelineID}} span multiple lines:\n\n{code:title=https://github.com/elek/ozone-build-results/blob/92d31c9b58065b37a371c71c97b346f99163318d/2020/07/11/1626/acceptance/docker-ozone-ozone-freon-scm.log#L218-L223}\ndatanode_1  | 2020-07-11 13:07:00,540 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: \"8101dcbf-1a28-4f20-863a-0616b4e4bc4b\"\ndatanode_1  | uuid128 {\ndatanode_1  |   mostSigBits: -9150790254504423648\ndatanode_1  |   leastSigBits: -8774694229384053685\ndatanode_1  | }\ndatanode_1  | .\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13291773,
      "Intermittent failure in TestReconWithOzoneManager due to BindException",
      "TestReconWithOzoneManager may fail with BindException:\n\n{code:title=https://github.com/apache/hadoop-ozone/pull/677/checks?check_run_id=507376007}\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.707 s <<< FAILURE! - in org.apache.hadoop.ozone.recon.TestReconWithOzoneManager\norg.apache.hadoop.ozone.recon.TestReconWithOzoneManager  Time elapsed: 19.706 s  <<< ERROR!\npicocli.CommandLine$ExecutionException: Error while calling command (org.apache.hadoop.ozone.recon.ReconServer@23f74a49): java.net.BindException: Port in use: 0.0.0.0:36263\n\t...\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:534)\n\tat org.apache.hadoop.ozone.recon.TestReconWithOzoneManager.init(TestReconWithOzoneManager.java:109)\n\t...\nCaused by: java.net.BindException: Port in use: 0.0.0.0:36263\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.constructBindException(HttpServer2.java:1200)\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.bindForSinglePort(HttpServer2.java:1222)\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.openListeners(HttpServer2.java:1281)\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.start(HttpServer2.java:1136)\n\tat org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:252)\n\tat org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:128)\n\tat org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:106)\n\tat org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50)\n\tat picocli.CommandLine.execute(CommandLine.java:1173)\n\t... 27 more\n{code}\n\n{code:title=test output}\n2020-03-14 06:17:08,677 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(284)) - HTTP server of ozoneManager listening at http://0.0.0.0:36263\n...\n2020-03-14 06:17:11,589 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(170)) - Starting Web-server for recon at: http://0.0.0.0:36263\n...\n2020-03-14 06:17:12,756 [main] INFO  recon.ReconServer (ReconServer.java:start(125)) - Starting Recon server\n2020-03-14 06:17:12,757 [main] INFO  http.HttpServer2 (HttpServer2.java:start(1139)) - HttpServer.start() threw a non Bind IOException\njava.net.BindException: Port in use: 0.0.0.0:36263\n...\n{code}",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13566481,
      "Refactor some constructors in SCM to avoid too many parameters",
      "{{SCMContext}} is created using builder.  To avoid too many parameters in the constructor, it should be changed to assign members directly from the {{Builder}} object.\n\nApply similar change to some other classes in SCM, too.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13471167,
      "Update Contributing guide",
      "The goal is to fix some outdated items in the contributors' guide, as well as make some minor improvements.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13290498,
      "Unit check fails to execute insight and mini-chaos-tests modules",
      "This was observed in unit check run for 0.5.0 RC.\n\n{code:title=https://github.com/apache/hadoop-ozone/runs/490978126?check_suite_focus=true}\n2020-03-06T19:13:08.6122969Z [ERROR] Failed to execute goal on project hadoop-ozone-insight: Could not resolve dependencies for project org.apache.hadoop:hadoop-ozone-insight:jar:0.5.0-beta: Could not find artifact org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]\n2020-03-06T19:13:08.6180318Z [ERROR] Failed to execute goal on project mini-chaos-tests: Could not resolve dependencies for project org.apache.hadoop:mini-chaos-tests:jar:0.5.0-beta: Failure to find org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in https://repository.apache.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced -> [Help 1]\n{code}\n\nUnit check skips {{integration-test}}, but these 2 modules depend on it.",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13538788,
      "Let reconfiguration handler update reconfigurable config objects",
      "HDDS-8668 implements reconfigurable config objects.  HDDS-8702 creates a dedicated reconfiguration handler, which handles individual reconfigurable properties so far.  The goal of this task is to allow registering config objects with the reconfiguration handler.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13250198,
      "StackOverflowError in OzoneClientInvocationHandler",
      "Happens if log level for {{org.apache.hadoop.ozone.client}} is set to TRACE.\n\n{code}\nSLF4J: Failed toString() invocation on an object of type [com.sun.proxy.$Proxy85]\nReported exception:\njava.lang.StackOverflowError\n...\n\tat org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)\n\tat org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)\n\tat com.sun.proxy.$Proxy85.toString(Unknown Source)\n\tat org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)\n\tat org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)\n\tat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)\n\tat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)\n\tat org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)\n\tat org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)\n\tat org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)\n\tat com.sun.proxy.$Proxy85.toString(Unknown Source)\n...\n{code}",
      "HDDS",
      "Resolved",
      5,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13563740,
      "Migrate FS contract tests to JUnit5",
      "Hadoop FS contract tests are implemented using JUnit4, but hopefully they can be changed to support JUnit5, too (HADOOP-19028).  With that, we can also upgrade Ozone's contract test implementations to JUnit5.",
      "HDDS",
      "Resolved",
      2,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13275694,
      "Read to ByteBuffer uses wrong offset",
      "{{OzoneFSInputStream#read(ByteBuffer)}} uses the target buffer's position for offsetting into the temporary array:\n\n{code:title=https://github.com/apache/hadoop-ozone/blob/b834fa48afef4ee4c73577c7af564e1e97cb9d5b/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSInputStream.java#L90-L97}\n  public int read(ByteBuffer buf) throws IOException {\n\n    int bufInitPos = buf.position();\n    int readLen = Math.min(buf.remaining(), inputStream.available());\n\n    byte[] readData = new byte[readLen];\n    int bytesRead = inputStream.read(readData, bufInitPos, readLen);\n    buf.put(readData);\n{code}\n\nGiven a buffer with capacity=10 and position=8, this results in the following:\n\n * {{readLen}} = 2 => {{readData.length}} = 2\n * {{bufInitPos}} = 8\n\nSo {{inputStream}} reads 2 bytes and writes it into {{readData}} starting at offset 8, which results in an {{IndexOutOfBoundsException}}.\n\noffset should always be 0, since the temporary array is sized exactly for the length to read, and it has no extra data at the start.",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13577472,
      "Integration check no longer needs Ozone repo",
      "HDDS-9242 changed {{integration}} check to run JUnit tests from all modules.  Thus it no longer needs Ozone to be present in local Maven repo.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13550921,
      "Speed up TestStorageContainerManagerHA",
      "{code}\n[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 419.934 s - in org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA\n{code}",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13288109,
      "Allow forced overwrite of local file",
      "{{ozone sh key get}} refuses to overwrite existing local file.  I would like to add a {{--force}} flag (default: false) to allow overriding this behavior, to make it easier to repeatedly get a key without forcing me to delete it locally first.",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13252711,
      "Overlapping chunk region cannot be read concurrently",
      "Concurrent requests to datanode for the same chunk may result in the following exception in datanode:\n\n{code}\njava.nio.channels.OverlappingFileLockException\n   at java.base/sun.nio.ch.FileLockTable.checkList(FileLockTable.java:229)\n   at java.base/sun.nio.ch.FileLockTable.add(FileLockTable.java:123)\n   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.addToFileLockTable(AsynchronousFileChannelImpl.java:178)\n   at java.base/sun.nio.ch.SimpleAsynchronousFileChannelImpl.implLock(SimpleAsynchronousFileChannelImpl.java:185)\n   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.lock(AsynchronousFileChannelImpl.java:118)\n   at org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils.readData(ChunkUtils.java:175)\n   at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerImpl.readChunk(ChunkManagerImpl.java:213)\n   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleReadChunk(KeyValueHandler.java:574)\n   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:195)\n   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:271)\n   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)\n   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)\n   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)\n{code}\n\nIt seems this is covered by retry logic, as key read is eventually successful at client side.\n\nThe problem is that:\n\nbq. File locks are held on behalf of the entire Java virtual machine. They are not suitable for controlling access to a file by multiple threads within the same virtual machine. ([source|https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileLock.html])\n\ncode ref: [{{ChunkUtils.readData}}|https://github.com/apache/hadoop/blob/c92de8209d1c7da9e7ce607abeecb777c4a52c6a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L175]",
      "HDDS",
      "Resolved",
      2,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13537881,
      "Expose read-only interface of OzoneAdmins",
      "{{OzoneAdmins}} write interface ({{setAdminUsernames}}) should be accessible only to the \"owner\" of these objects, e.g. {{OzoneManager}}.  Other objects can be restricted to a read-only interface.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13533432,
      "Disable LegacyReplicationManager by default",
      " * Change the default value of {{hdds.scm.replication.enable.legacy}} to false.\n * Fix any problems found by CI.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13414370,
      "Intermittent failure due to IllegalStateException: zip file closed in HDDSLayoutVersionManager",
      "First seen here. The PR's changed are unrelated.\n\n[https://github.com/apache/ozone/runs/4355766647]\n{code:java}\n[ERROR] testNodeWithOpenPipelineCanBeDecommissionedAndRecommissioned \u00a0Time elapsed: 44.016 s \u00a0<<< ERROR!\njava.lang.IllegalStateException: zip file closed\n\u00a0 \u00a0 at java.util.zip.ZipFile.ensureOpen(ZipFile.java:686)\n...\n    at org.reflections.Reflections.<init>(Reflections.java:168)\n\u00a0 \u00a0 at org.apache.hadoop.hdds.upgrade.HDDSLayoutVersionManager.registerUpgradeActions(HDDSLayoutVersionManager.java:64)\n\u00a0 \u00a0 at org.apache.hadoop.hdds.upgrade.HDDSLayoutVersionManager.<init>(HDDSLayoutVersionManager.java:51)\n\u00a0 \u00a0 at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.<init>(DatanodeStateMachine.java:137)\n\u00a0 \u00a0 at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:275)\n\u00a0 \u00a0 at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:207)\n\u00a0 \u00a0 at org.apache.hadoop.ozone.MiniOzoneClusterImpl.restartHddsDatanode(MiniOzoneClusterImpl.java:423)\n\u00a0 \u00a0 at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.testNodeWithOpenPipelineCanBeDecommissionedAndRecommissioned(TestDecommissionAndMaintenance.java:197)\n {code}",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13270983,
      "Let findbugs.sh skip frontend plugin for Recon",
      "Findbugs/Spotbugs only checks Java code.  We can skip frontend plugin execution for Recon to save ~2 minutes.  Makes a difference mostly when running it locally.",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13288114,
      "Save each output of smoketest executed multiple times",
      "Acceptance tests may invoke the same smoketest multiple times to verify behaviour in different states.  Currently output is saved to a file named based on _environment_, _test_ and _container_, so each execution's output overwrites the previous one.  We should check if the file already exists and add a suffix if necessary to avoid overwriting previous logs.",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13263399,
      "Fix TestKeyValueContainer#testRocksDBCreateUsesCachedOptions",
      "TestKeyValueContainer#testRocksDBCreateUsesCachedOptions, introduced in HDDS-2283, is failing:\n\n{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2283-cnrrq/unit/hadoop-hdds/container-service/org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.txt}\ntestRocksDBCreateUsesCachedOptions(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.135 s  <<< FAILURE!\njava.lang.AssertionError: expected:<1> but was:<11>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testRocksDBCreateUsesCachedOptions(TestKeyValueContainer.java:406)\n{noformat}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13296046,
      "Move Ozone Shell from ozone-manager to tools",
      "Ozone Shell is currently part of the {{ozone-manager}} module.  I think it would be more at home in the {{tools}} module.\n\nAlso rename the package name {{ozShell}} to {{shell}}, as package names should be all lowercase.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13434588,
      "Intermittent failure in TestOzoneManagerHAWithData#testOMRestart",
      "{code}\n[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 162.569 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOzoneManagerHAWithData\n[ERROR] org.apache.hadoop.ozone.om.TestOzoneManagerHAWithData.testOMRestart  Time elapsed: 75.918 s  <<< FAILURE!\njava.lang.AssertionError\n  ...\n  at org.apache.hadoop.ozone.om.TestOzoneManagerHAWithData.testOMRestart(TestOzoneManagerHAWithData.java:473)\n{code}\n\n{code:title=https://github.com/apache/ozone/blob/74d92c86be579e6d90535a13a276e1970ac644fc/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHAWithData.java#L473-L474}\n    Assert.assertTrue(\n        followerOMLastAppliedIndex < leaderOMSnaphsotIndex);\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13538318,
      "SCM HA transaction buffer may be closed without flush",
      "{code:title=https://github.com/apache/ozone/actions/runs/5136151298/jobs/9242526664?pr=4794#step:5:3753}\njava.lang.AssertionError: Found 2 leaked objects, check logs\n\tat org.apache.hadoop.hdds.utils.db.CodecBuffer.assertNoLeaks(CodecBuffer.java:70)\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:451)\n\tat org.apache.hadoop.hdds.upgrade.TestScmHAFinalization.shutdown(TestScmHAFinalization.java:117)\n{code}\n\n{code:title=org.apache.hadoop.hdds.upgrade.TestScmHAFinalization-output.txt}\n2023-05-31 18:35:07,518 [Finalizer] WARN  db.CodecBuffer (CodecBuffer.java:finalize(94)) - LEAK 1: org.apache.hadoop.hdds.utils.db.CodecBuffer@617e237d, refCnt=1, capacity=1\n2023-05-31 18:35:07,518 [Finalizer] WARN  db.CodecBuffer (CodecBuffer.java:finalize(94)) - LEAK 2: org.apache.hadoop.hdds.utils.db.CodecBuffer@435b14c4, refCnt=1, capacity=14\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13291618,
      "Remove unused dependency version strings",
      "After the repo was split from hadoop, there are a few unused dependencies/version strings left in pom.xml. They can be removed.\n\nExample: \n\n{code}\n    <hbase.one.version>1.2.6</hbase.one.version>\n    <hbase.two.version>2.0.0-beta-1</hbase.two.version>\n{code}\nThere may be more.",
      "HDDS",
      "Resolved",
      4,
      3,
      1699,
      "newbie"
    ],
    [
      13591983,
      "Internal error on S3 CompleteMultipartUpload if parts are not specified",
      "The following error can be received during multipart upload:\n{code:java}\nAn error occurred (500) when calling the CompleteMultipartUpload operation (reached max retries: 4): Internal Server Error {code}\nReproduce steps:\n # Create bucket:\u00a0 aws s3api --endpoint http://s3g:9878 create-bucket --bucket test\n # Create multipart upload: aws s3api --endpoint http://s3g:9878 create-multipart-upload --bucket test --key data\n # Upload Part 1: aws s3api --endpoint http://s3g:9878 upload-part --bucket test --part-number 1 --upload-id 5eee13a8-e326-4e3f-935b-b948e04b2ef2-108397230370979845 --key data --body /etc/hosts\n # Complete multipart upload without specifying parts: aws s3api --endpoint http://s3g:9878 complete-multipart-upload --bucket test --upload-id e901c545-81b0-4ccc-9d5b-2703243913a6-113126144083165185 --key data\n\nThe Amazon S3 has the following output in this case:\n{code:java}\nAn error occured (InvalidRequest) when calling the CompleteMultipartUpload: You must specify at least one part{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13536020,
      "Ratis underreplication due to maintenance is not deprioritised",
      "According to the following javadoc, both decommission and maintenance replicas should be deprioritised:\n\n{code:title=https://github.com/apache/ozone/blob/6d9002201e58dc995dc133941acaef2af03cb9d2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/replication/ContainerHealthResult.java#L145-L164}\n    /**\n     * The weightedRedundancy, is the remaining redundancy + the requeue count.\n     * When this value is used for ordering in a priority queue it ensures the\n     * priority is reduced each time it is requeued, to prevent it from blocking\n     * other containers from being processed.\n     * Additionally, so that decommission and maintenance replicas are not\n     * ordered ahead of under-replicated replicas, a redundancy of\n     * DECOMMISSION_REDUNDANCY is used for the decommission redundancy rather\n     * than its real redundancy.\n     * @return The weightedRedundancy of this result.\n     */\n    public int getWeightedRedundancy() {\n      int result = requeueCount;\n      if (dueToDecommission) {\n        result += DECOMMISSION_REDUNDANCY;\n      } else {\n        result += getRemainingRedundancy();\n      }\n      return result;\n    }\n{code}\n\nbut {{dueToDecommission=true}} is set only based on decommission replicas, ignoring maintenance replicas ({{maintenanceCount}}):\n\n{code:title=https://github.com/apache/ozone/blob/6d9002201e58dc995dc133941acaef2af03cb9d2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/replication/RatisContainerReplicaCount.java#L520-L533}\n  /**\n   * Checks whether insufficient replication is because of some replicas\n   * being on datanodes that were decommissioned.\n   * @param includePendingAdd if pending adds should be considered\n   * @return true if there is insufficient replication and it's because of\n   * decommissioning.\n   */\n  public boolean inSufficientDueToDecommission(boolean includePendingAdd) {\n    if (isSufficientlyReplicated(includePendingAdd)) {\n      return false;\n    }\n    int delta = redundancyDelta(true, includePendingAdd);\n    return decommissionCount >= delta;\n  }\n{code}",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13505605,
      "Eliminate duplicated config in LegacyReplicationManager",
      "{{ReplicationManager}} and {{LegacyReplicationManager}} both define some of the same configuration keys.  In the long run, we are planning to get rid of {{LegacyReplicationManager}} completely.  In the meantime, we can eliminate the duplication by making {{LegacyReplicationManager}} get config values from {{ReplicationManager}}'s config where applicable.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13486570,
      "Bump jackson-databind to 2.13.4.2",
      "Bump {{jackson2-databind}} to 2.13.4.2 due to CVE-2022-42003.",
      "HDDS",
      "Resolved",
      3,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13271512,
      "Conditionally enable profiling at the kernel level",
      "Extend {{entrypoint.sh}} to set the kernel parameters required for profiling if the {{ASYNC_PROFILER_HOME}} environment variable (used by {{ProfileServlet}}) is set.\n\nRef:\n\n{code:title=https://cwiki.apache.org/confluence/display/HADOOP/Java+Profiling+of+Ozone}\necho 1 > /proc/sys/kernel/perf_event_paranoid\necho 0 > /proc/sys/kernel/kptr_restrict\n{code}",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13264785,
      "Print out the ozone version during the startup instead of hadoop version",
      "Ozone components printing out the current version during the startup:\n\n\u00a0\n{code:java}\nSTARTUP_MSG: Starting StorageContainerManager\nSTARTUP_MSG:   host = om/10.8.0.145\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 3.2.0\nSTARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-{code}\nBut as it's visible the build / compiled information is about hadoop not about hadoop-ozone.\n\n(And personally I prefer to use a github compatible url instead of the SVN style -r. Something like:\n{code:java}\nSTARTUP_MSG: build =  https://github.com/apache/hadoop-ozone/commit/8541c5694efebb58f53cf4665d3e4e6e4a12845c ; compiled by '....' on ...{code}\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "Triaged, pull-request-available"
    ],
    [
      13412292,
      "Freon datanode chunk validator does not find pipeline from param",
      "Freon Datanode Chunk Validator does not find the pipeline provided via command-line option:\n\n{code}\n$ ozone freon dcg -n10 -t1 -p dcg1 --pipeline e92fa709-db01-433f-ae8b-8c42d328c819\n...\nSuccessful executions: 10\n\n$ ozone freon dcv -n10 -t1 -p dcg1 --pipeline e92fa709-db01-433f-ae8b-8c42d328c819\n...\nPipeline ID is defined, but there is no such pipeline: e92fa709-db01-433f-ae8b-8c42d328c819\n\n$ ozone admin pipeline list | grep -c e92fa709-db01-433f-ae8b-8c42d328c819\n1\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13260085,
      "test-single.sh cannot copy results",
      "Previously {{result}} directory was created by simply {{source}}-ing {{testlib.sh}}, but HDDS-2185 changed it to avoid lost results.  {{test-single.sh}} needs to be adjusted accordingly.\n\n{noformat}\n$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone\n$ docker-compose up -d --scale datanode=3\n$ ../test-single.sh scm basic/basic.robot\n...\ninvalid output path: directory \"hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone/result\" does not exist\n{noformat}",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13543974,
      "Create separate acceptance split for cert rotation",
      "{{Test execution of ozonesecure-ha/test-root-ca-rotation.sh}} failed 23 times recently (on {{master}}, not counting PRs).  Also, {{acceptance (HA-secure)}} now takes ~1,5 hours.  Both problems could be less severe if we executed cert. rotation in its own split.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13288858,
      "Add new Freon test for putBlock",
      "The goal of this task is to introduce a new Freon test that issues putBlock commands.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13260747,
      "Container Data Scrubber computes wrong checksum",
      "Chunk checksum verification fails for (almost) any file.  This is caused by computing checksum for the entire buffer, regardless of the actual size of the chunk.\n\n{code:title=https://github.com/apache/hadoop/blob/55c5436f39120da0d7dabf43d7e5e6404307123b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerCheck.java#L259-L273}\n            byte[] buffer = new byte[cData.getBytesPerChecksum()];\n...\n                v = fs.read(buffer);\n...\n                bytesRead += v;\n...\n                ByteString actual = cal.computeChecksum(buffer)\n                    .getChecksums().get(0);\n{code}\n\nThis results in marking all closed containers as unhealthy.",
      "HDDS",
      "Resolved",
      2,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13310385,
      "Intermittent failure in TestDeleteWithSlowFollower",
      "TestDeleteWithSlowFollower failed soon after it was re-enabled in HDDS-3330.\n\n{code:title=https://github.com/apache/hadoop-ozone/runs/753363338}\n[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.647 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\n[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 0.163 s  <<< FAILURE!\njava.lang.AssertionError\n  ...\n  at org.junit.Assert.assertNotNull(Assert.java:631)\n  at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)\n{code}\n\nCC [~shashikant] [~elek]",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13559437,
      "Avoid recreating typesafe config objects unnecessarily",
      "{{XceiverServerRatis}} creates {{DatanodeRatisServerConfig}} repeatedly for accessing individual properties.  It should create it only once and reuse.\n\nThere may be similar unnecessary creation in other classes, too.",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13256838,
      "OM Metric mismatch (MultipartUpload failures)",
      "{{incNumCommitMultipartUploadPartFails()}} increments {{numInitiateMultipartUploadFails}} instead of the counter for commit failures.\n\nhttps://github.com/apache/hadoop/blob/85b1c728e4ed22f03db255f5ef34a2a79eb20d52/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java#L310-L312",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13253310,
      "Partially started compose cluster left running",
      "If any container in the sample cluster [fails to start|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L24], all successfully started containers are left running.  This [prevents|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L59] any further acceptance tests from normal completion.  This is only a minor inconvenience, since acceptance test as a whole fails either way.",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13554501,
      "Integration check should reuse Ozone jars from build check",
      "Try to reuse Ozone jars created by _build_ check for running integration tests.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13248887,
      "Cannot build hadoop-hdds-config from scratch in IDEA",
      "Building {{hadoop-hdds-config}} from scratch (eg. right after checkout or after {{mvn clean}}) in IDEA fails with the following error:\n\n{code}\nError:java: Bad service configuration file, or exception thrown while constructing Processor object: javax.annotation.processing.Processor: Provider org.apache.hadoop.hdds.conf.ConfigFileGenerator not found\n{code}",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13287717,
      "Include output of timed out test in bundle",
      "Sometimes a unit/integration test does not complete, nor does it crash.  We should collect the output of such tests in the result bundle for analysis.\n\nExample:\n\n{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/469172863}\n2020-02-26T08:15:58.2297584Z [INFO] Running org.apache.hadoop.ozone.freon.TestRandomKeyGenerator\n2020-02-26T08:30:59.6189916Z [INFO] Running org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations\n...\n2020-02-26T08:32:47.6155975Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork\n{code}\n\nIn this case TestRandomKeyGenerator had this problem.  It might be a bit tricky to find such tests, since these are not explicitly listed at the end, unlike failed or crashed tests.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13535046,
      "Container DB open, but not found in DatanodeStoreCache",
      "Surefire fork Intermittently timeouts in {{TestDecommissionAndMaintenance}}.\n\nContainer DB added to cache:\n\n{code}\n2023-05-03 08:18:26,909 [EndpointStateMachine task thread for /0.0.0.0:43723 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db to cache\n{code}\n\nbut then not found and tried to open again:\n\n{code}\n2023-05-03 08:18:57,086 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db\njava.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1683101936 acquiring thread 139985634854656: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db/LOCK: No locks available\n\tat org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:182)\n\tat org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:212)\n\tat org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)\n\tat org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)\n\tat org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)\n\tat org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)\n\tat org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)\n\tat org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.notifyGroupRemove(ContainerStateMachine.java:1052)\n\tat org.apache.ratis.server.impl.RaftServerImpl.groupRemove(RaftServerImpl.java:423)\n\tat org.apache.ratis.server.impl.RaftServerProxy.lambda$groupRemoveAsync$12(RaftServerProxy.java:530)\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)\n\tat java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)\n\tat java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:529)\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:479)\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:459)\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:822)\n\tat org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)\n\tat org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)\n\tat org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:644)\n\tat java.lang.Thread.run(Thread.java:750)\n{code}\n\nThis continues until the fork is killed:\n\n{code}\n2023-05-03 08:33:24,505 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db\njava.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1683101936 acquiring thread 139985634854656: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db/LOCK: No locks available\n\tat org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:182)\n\tat org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:212)\n\tat org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)\n\tat org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)\n\tat org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)\n\tat org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)\n\tat org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)\n{code}\n\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/21/21757/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/24/21800/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/24/21805/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/27/21885/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/27/21895/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/28/21927/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/05/03/21994/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt\n* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/05/03/21995/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt",
      "HDDS",
      "Resolved",
      2,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13357398,
      "Skip coverage check for PRs and in forks",
      "Currently _coverage_ CI check:\n\n# calculates combined test coverage\n# uploads it to Sonar only for Apache Ozone repo and only for builds on push/schedule\n# stores combined coverage in GitHub Actions artifact\n\nThus for PR in Apache Ozone and for all builds in forks, it only stores coverage in the artifact.  These expire in 30 days and I don't think anybody really checks them manually.\n\nI propose to completely skip _coverage_ check for PRs and in forks, instead of only skipping upload to Sonar.  This would save ~12 minutes for such builds.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13391839,
      "Ozone version mismatch in Kubernetes test lib",
      "Kubernetes test library allows running the tests from source dir, but looks for outdated version of Ozone in {{target}}.  This results in invalid path:\n\n{code}\nscm-statefulset.yaml\n79:        hostPath:\n80-          path: 'realpath: ../../../../../target/ozone-0.6.0-SNAPSHOT: No such file\n{code}",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13318805,
      "FLAKY-UT: TestWatchForCommit#testWatchForCommitForGroupMismatchException",
      "[INFO] Running org.apache.hadoop.ozone.client.rpc.TestWatchForCommit\n[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 211.911 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestWatchForCommit\n[ERROR] testWatchForCommitForGroupMismatchException(org.apache.hadoop.ozone.client.rpc.TestWatchForCommit)  Time elapsed: 38.862 s  <<< ERROR!\njava.io.IOException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:740)\n\tat org.apache.hadoop.ozone.container.TestHelper.waitForPipelineClose(TestHelper.java:220)\n\tat org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForGroupMismatchException(TestWatchForCommit.java:344)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\nat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\nCaused by: org.apache.ratis.protocol.GroupMismatchException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:414)\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:372)\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:355)\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:738)\n\t... 29 more",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13579506,
      "Check OM version before making rewrite key request",
      "Integrate version framework to refuse client to request atomic calls if the server does not support it (new client against old server)",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13317177,
      "Use Duration for time in RatisClientConfig",
      "Change parameter and return type of time-related config methods in {{RatisClientConfig}} to {{Duration}}.  This results in more readable parameter values and type safety.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13376514,
      "Update commons-io to 2.8.0",
      "Similar to HADOOP-17683 we should update despite we don't use the vulnerable API.\n\nhttps://nvd.nist.gov/vuln/detail/CVE-2021-29425\n\nIn Apache Commons IO before 2.7, When invoking the method FileNameUtils.normalize with an improper input string, like \"//../foo\", or \"\\\\..\\foo\", the result would be the same value, thus possibly providing access to files in the parent directory, but not further above (thus \"limited\" path traversal), if the calling code would use the result to construct a path value.",
      "HDDS",
      "Resolved",
      3,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13529971,
      "ManagedWriteBatch is not closed properly in SCMHADBTransactionBuffer",
      "{{SCMHADBTransactionBufferStub}} may fail to close {{currentBatchOperation}} if {{dbStore == null}}.\n\n{{DBTransactionBuffer}} is not closed by {{SCMHAManagerImpl}}.",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13571472,
      "Remove unused VolumeInfo#configuredCapacity",
      "{{VolumeInfo#configuredCapacity}} is unused, can be removed.",
      "HDDS",
      "Resolved",
      5,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13402158,
      "Test cluster provider possibly returns null",
      "{{MiniOzoneClusterProvider}} may timeout (100 seconds) while waiting for a cluster to become available.  In this case it simply returns {{null}} without warning.  This results in NPE when trying to use the cluster in test.\n\n{code}\ntestAllDataNodeFailuresAfterScmPostFinalizeUpgrade  Time elapsed: 100.076 s  <<< ERROR!\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdds.upgrade.TestHDDSUpgrade.init(TestHDDSUpgrade.java:173)\n\tat org.apache.hadoop.hdds.upgrade.TestHDDSUpgrade.setUp(TestHDDSUpgrade.java:135)\n{code}",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13324290,
      "Update version number in upgrade tests",
      "Ozone 0.6.0 release is renamed to Ozone 1.0.0, but there are a few leftover references to 0.6.0, mostly in {{upgrade}} acceptance test.",
      "HDDS",
      "Resolved",
      4,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13581978,
      "Let zero OzoneQuota use byte as unit",
      "Just found that {{valueOf(0)}} will use {{EB}}.  (Fortunately, we added {{EB}}; otherwise, it becomes an {{ArrayIndexOutOfBoundsException}}).  Not a big deal, although we probably should fix it.\n{code}\n  public static void main(String[] args) {\n    final RawQuotaInBytes q = RawQuotaInBytes.valueOf(0);\n    System.out.println(\"q = \" + q);\n  }\n// q = 0 EB\n{code}",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13580456,
      "Remove unused org.glassfish:javax.servlet dependency declaration",
      "Dependency declaration for org.glassfish:javax.servlet is unused, can be removed.",
      "HDDS",
      "Resolved",
      4,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13492099,
      "Bump Spring framework from 5.2.20 to 5.3.21",
      "Bump Spring framework from 5.2.20 to 5.3.21",
      "HDDS",
      "Resolved",
      3,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13274622,
      "Unnecessary calls to isNoneEmpty and isAllEmpty",
      "{{isNoneEmpty}} and {{isAllEmpty}} check variable number of strings.  For single string they can be replaced with {{isNotEmpty}} and {{isEmpty}}.",
      "HDDS",
      "Resolved",
      5,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13315327,
      "Maven warning due to deprecated expression pom.artifactId",
      "{code:title=mvn clean}\n[INFO] Scanning for projects...\n[WARNING]\n[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-interface-client:jar:0.6.0-SNAPSHOT\n[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.\n[WARNING]\n[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-common:jar:0.6.0-SNAPSHOT\n[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.\n...\n{code}\n\nSame warning in {{hadoop-hdds/pom.xml}} was fixed during review of HDDS-3875, but the one in {{hadoop-ozone/pom.xml}} was left.",
      "HDDS",
      "Resolved",
      5,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13282220,
      "Unnecessary log messages in DBStoreBuilder",
      "DBStoreBuilder logs some table-related at INFO level.  This is fine for DBs that are created once per run, eg. OM or SCM, but Recon builds a new DB for each OM snapshot:\n\n{code}\nrecon_1     | 2020-01-29 15:20:32,466 [pool-7-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/recon/om.snapshot.db_1580311232241\nrecon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: userTable\nrecon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTable\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: volumeTable\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTable\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: bucketTable\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTable\nrecon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: keyTable\nrecon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTable\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: deletedTable\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTable\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: openKeyTable\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTable\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3Table\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Table\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTable\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTable\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: dTokenTable\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTable\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTable\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTable\nrecon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: prefixTable\nrecon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTable\nrecon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: default\nrecon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default\nrecon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default options. DBProfile.DISK\nrecon_1     | 2020-01-29 15:20:32,514 [pool-7-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB snapshot at /data/metadata/recon/om.snapshot.db_1580311232241.\n{code}",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13435616,
      "Spotbugs transitive dependencies may result in NoClassDefFound error",
      "The following transitive dependency allows using classes from {{commons-lang}} instead of {{commons-lang3}}.  Code compiles fine, but dependency is not available at runtime, resulting in {{NoClassDefFoundError}}.\n\n{code}\n\\- com.github.spotbugs:spotbugs:jar:3.1.12:provided\n   ...\n   +- commons-lang:commons-lang:jar:2.6:provided\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13526744,
      "Handle unchecked exception in KeyValueHandler more gracefully",
      "{{KeyValueHandler}} in general does not handle unchecked exceptions.  The exception reaches gRPC, which closes the connection abruptly.  As a result, client only gets some generic exception ({{StatusRuntimeException: UNKNOWN}}).\n\nSteps to reproduce: same as HDDS-8019.",
      "HDDS",
      "Resolved",
      3,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13431568,
      "Selective checks: run rat for readme change",
      "_rat_ check may fail if license is missing from non-root README files.  Thus it should be triggered for such changes in PRs.",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13584901,
      "Extract keywords for multipart upload tests",
      "Refactor {{MultipartUpload.robot}} by extracting reusable Robot keywords for test steps.\n\nAlso, reduce the number of temp files created for testing.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13270317,
      "Avoid hostname lookup for invalid local IP addresses",
      "{{OzoneSecurityUtil#getValidInetsForCurrentHost}} performs hostname lookup for all local network interfaces, even for invalid addresses.  This significantly slows down some secure tests ({{TestHddsSecureDatanodeInit}}, {{TestSecureOzoneCluster}}) when run on a machine with special IPv6 network interfaces due to timeout reaching IPv6 DNS servers.\n\nThis issue proposes to disable the lookup for invalid addresses.",
      "HDDS",
      "Resolved",
      4,
      4,
      1699,
      "pull-request-available"
    ],
    [
      13298662,
      "Extract test utilities to separate module",
      "TimedOutTestsListener cannot be added globally because it is in hadoop-hdds-common, which is not accessible in hadoop-hdds-config (since the latter is a dependency of the former).  The listener and related classes (GenericTestUtils, etc.) should be extracted into a separate module to be used by all others.",
      "HDDS",
      "Resolved",
      3,
      3,
      1699,
      "pull-request-available"
    ],
    [
      13249930,
      "Unused executor in SimpleContainerDownloader",
      "{{SimpleContainerDownloader}} has an {{executor}} that's created and shut down, but never used.",
      "HDDS",
      "Resolved",
      4,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13268397,
      "Ensure streams are closed",
      "* ContainerDataYaml: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6IKcVY8lQ4ZsQU&open=AW5md-6IKcVY8lQ4ZsQU\n* OmUtils: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-hdKcVY8lQ4Zr76&open=AW5md-hdKcVY8lQ4Zr76",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available, sonar"
    ],
    [
      13250036,
      "Missing or error-prone test cleanup",
      "Some integration tests do not clean up after themselves.  Some only clean up if the test is successful.",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13260334,
      "Container Data Scrubber spams log in empty cluster",
      "In an empty cluster (without closed containers) logs are filled with messages from completed data scrubber iterations (~3600 per second for me), if Container Scanner is enabled ({{hdds.containerscrub.enabled=true}}), eg.:\n\n{noformat}\ndatanode_1  | 2019-10-03 15:43:57 INFO  ContainerDataScanner:114 - Completed an iteration of container data scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 6763, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0\n{noformat} \n\nAlso CPU usage is quite high.\n\nI think:\n\n# there should be a small sleep between iterations\n# it should log only if any containers were scanned",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13551061,
      "Fail checks in Summary step instead of Test",
      "Ozone's CI check scripts ({{integration.sh}}, etc.) have 2 main conventions:\n * exit with non-zero code if there are failed tests\n * list failures in {{summary.txt}} or similar\n\nMost CI jobs have a \"Summary of failures\" step, which provides a quick overview by showing the contents of {{summary.txt}}.\n\nGithub automatically expands the failed step, which according to these conventions is the main \"Execute tests\" step.  The problem is that this can have very long output.\n\nIt would be better to exit with failure in the \"summary\" step, thus focusing the developer's attention on the quick overview, keeping the long output collapsed by default.",
      "HDDS",
      "Resolved",
      4,
      7,
      1699,
      "pull-request-available, usability"
    ],
    [
      13376413,
      "Acceptance test may exit with 0 in case of error",
      "If any acceptance test fails, {{test-all.sh}} (and in turn {{acceptance.sh}}) should exit with error code (1).  But if a successful test is run after a failing one, it will now wrongly exit with success (0).\n\n{code}\n$ export OZONE_TEST_SELECTOR='failing1\\|ozone-csi'\n$ ./hadoop-ozone/dev-support/checks/acceptance.sh\n...\n$ echo $?\n0\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13562108,
      "TokenRenewer should close OzoneClient after use",
      "Ozone's {{TokenRenewer}} implementations do not close {{OzoneClient}} after use.\n\nBoth O3FS and OFS define their own {{TokenRenewer}} implementation.\n\nHadoop finds implementations via {{ServiceLoader}}, and uses the first one that handles the kind of token being used:\n\n{code:title=https://github.com/apache/hadoop/blob/7db9895000860605a66dd6403005b0c61a6ed744/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java#L454-L470}\n  private static ServiceLoader<TokenRenewer> renewers =\n      ServiceLoader.load(TokenRenewer.class);\n\n\n  private synchronized TokenRenewer getRenewer() throws IOException {\n    if (renewer != null) {\n      return renewer;\n    }\n    renewer = TRIVIAL_RENEWER;\n    synchronized (renewers) {\n      Iterator<TokenRenewer> it = renewers.iterator();\n      while (it.hasNext()) {\n        try {\n          TokenRenewer candidate = it.next();\n          if (candidate.handleKind(this.kind)) {\n            renewer = candidate;\n            return renewer;\n          }\n{code}\n\nThe two implementations are the same (not FileSystem-specific), kind is the same for both, so we can remove the duplicate one.",
      "HDDS",
      "Resolved",
      3,
      1,
      1699,
      "pull-request-available"
    ],
    [
      13442590,
      "Create compat acceptance split",
      "Extract compatibility-related acceptance tests from {{misc}} into a separate suite.",
      "HDDS",
      "Resolved",
      3,
      7,
      1699,
      "pull-request-available"
    ],
    [
      13271103,
      "Handle LeaderNot ready exception in OzoneManager StateMachine and upgrade ratis to latest version.",
      "This Jira is to handle LeaderNotReadyException in OM and also update to latest ratis version.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13306022,
      "Improve error message when GC parameters are not set",
      "Currently when GC parameters or any -XX are not set, it logs \n\n\"No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS\n\nIt would be nice to improve this message with settings that are being set.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13316504,
      "Unable to list intermediate paths on keys created using S3G.",
      "Keys created via the S3 Gateway currently use the createKey OM API to create the ozone key. Hence, when using a hdfs client to list intermediate directories in the key, OM returns key not found error. This was encountered while using fluentd to write Hive logs to Ozone via the s3 gateway.\ncc [~bharat]",
      "HDDS",
      "Resolved",
      1,
      2,
      2640,
      "pull-request-available"
    ],
    [
      13212335,
      "Handle DeleteContainerCommand in the SCMDatanodeProtocolServer",
      "Right now, in the SCMDatanodeProtocolServer getCommandResponse() deleteContainerCommand is not handled, so deleteContainerCommand is not sent to Datanode.\n\n\u00a0\n\nThe deletecontainercommand request is sent for over replicated containers, so this over replication is currently broken because of this.\n\n\u00a0\n\nBecause of this we see below error:\n\n\u00a0\n{code:java}\njava.lang.IllegalArgumentException: Not implemented\n at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.getCommandResponse(SCMDatanodeProtocolServer.java:345)\n at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.sendHeartbeat(SCMDatanodeProtocolServer.java:272)\n at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolServerSideTranslatorPB.sendHeartbeat(StorageContainerDatanodeProtocolServerSideTranslatorPB.java:88)\n at org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos$StorageContainerDatanodeProtocolService$2.callBlockingMethod(StorageContainerDatanodeProtocolProtos.java:27753)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n{code}\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "SCM"
    ],
    [
      13266942,
      "Exclude webapps from hadoop-ozone-filesystem-lib-current uber jar",
      "This has caused issue for DN UI loading.\n\nhadoop-ozone-filesystem-lib-current-xx.jar is in the classpath which accidentally loaded Ozone datanode web application instead of Hadoop datanode application. This leads to the reported error.\u00a0",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13220659,
      "In s3 when bucket already exists, it should just return location ",
      "In S3 for a create bucket request, when bucket already exists it should just return the location.\n\nThis was broken by HDDS-1068.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13221718,
      "Implement actions need to be taken after chill mode exit wait time",
      "# Destroy and close the pipelines\n # Close all the containers on the pipeline.\n # trigger for pipeline creation",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13317055,
      "Validate KeyNames created in FileSystem requests.",
      "This jira is to validate KeyNames which are created with OzoneFileSystem.\nSimilar to how hdfs handles using DFSUtil. isValidName()",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13305772,
      "Fix JVMPause monitor start in OzoneManager",
      "Fix JVMPause monitor logic, right now it is started only in restart.\n\nThis should be started during OM start, and stopped during OM Stop. In restart() we can start this again.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13290187,
      "OM RpcClient fail with java.lang.IllegalArgumentException",
      "In OM HA cluster, when one of the om service is down, during creation of RpcClient it will fail with below error.\n\n\u00a0\n\n\u00a0\n{code:java}\njava.lang.IllegalArgumentException: java.net.UnknownHostException: om1\n at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:447)\n at org.apache.hadoop.ozone.om.ha.OMProxyInfo.<init>(OMProxyInfo.java:40)\n at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:115)\n at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83)\n at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:207)\n at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:198)\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:124)\n at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:249)\n at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:274)\n at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:82)\n at picocli.CommandLine.execute(CommandLine.java:1173)\n at picocli.CommandLine.access$800(CommandLine.java:141)\n at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)\n at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)\n at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)\n at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)\n at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)\n at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)\n at org.apache.hadoop.ozone.freon.Freon.execute(Freon.java:72)\n at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)\n at org.apache.hadoop.ozone.freon.Freon.main(Freon.java:98)\nCaused by: java.net.UnknownHostException: om1\n ... 22 more\n\u00a0\n{code}\n\u00a0",
      "HDDS",
      "Resolved",
      1,
      7,
      2640,
      "OMHATest, pull-request-available"
    ],
    [
      13223462,
      "In DatanodeStateMachine join check for not null",
      "[https://builds.apache.org/job/PreCommit-HDDS-Build/2565/testReport/org.apache.hadoop.ozone.scm.node/TestSCMNodeMetrics/testNodeReportProcessingFailure/]",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13417239,
      "Update log4j version to 2.16",
      "https://lists.apache.org/thread/d6v4r6nosxysyq9rvnr779336yf0woz4",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13235852,
      "Fix TestReplicationManager and checkstyle issues.",
      "When working on HDDS-1551, found some test failures which are not related to HDDS-1551.\n\nThis is caused by HDDS-700.\u00a0\n\n\u00a0\n\nThis has not caught by Jenkins run because our Jenkins run does not run UT's for all the sub-modules. In this case, it should have run UT's for hadoop-hdds-server-scm, as there are some changes in src/test files in that module, but still, it has not run for it. I think Jenkins run for ozone project is not properly setup.\n\n[https://ci.anzix.net/job/ozone/16895/testReport/]\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      3,
      2640,
      "pull-request-available"
    ],
    [
      13392036,
      "Support to upload/read keys from encrypted buckets through S3G",
      "When KMS is secured using hadoop.kms.authentication.type = KERBEROS. From S3 key put/get fails when decrypting the key due to missing Kerberos Credentials/KMS tokens.\n\n*Proposal to fix this:*\n1. Introduce keytab for s3g\n2. Make s3g acts as proxy for end users while decrypt kms key during put/get/mpu.\n\nThe idea is similar to NFSgateway security model.\nhttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html\n\n",
      "HDDS",
      "Resolved",
      3,
      3,
      2640,
      "pull-request-available"
    ],
    [
      13268112,
      "Add partName, partNumber for CommitMultipartUpload",
      "Right now when complete Multipart Upload is not printing partName and\u00a0 partNumber into the audit log. This will help in analyzing audit logs for MPU.\n\n\u00a0\n\n\u00a0\n\n2019-11-13 15:14:10,191 | INFO\u00a0 | OMAudit | user=root | ip=xx.xx.xx.xx | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=ozone-test, key=plc_1570850798896_2991, dataSize=5242880, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {\n\n\u00a0 containerBlockID\n\n{ \u00a0 \u00a0 containerID: 2 \u00a0 \u00a0 localID: 103129366531867089 \u00a0 }\n\n\u00a0 blockCommitSequenceId: 4978\n\n}\n\noffset: 0\n\nlength: 5242880\n\ncreateVersion: 0\n\npipeline {\n\n\u00a0 leaderID: \"\"\n\n\u00a0 members {\n\n\u00a0 \u00a0 uuid: \"5d03aed5-cfb3-4689-b168-0c9a94316551\"\n\n\u00a0 \u00a0 ipAddress: \"xx.xx.xx.xx\"\n\n\u00a0 \u00a0 hostName: \"xx.xx.xx.xx\"\n\n\u00a0 \u00a0 ports\n\n{ \u00a0 \u00a0 \u00a0 name: \"RATIS\" \u00a0 \u00a0 \u00a0 value: 9858 \u00a0 \u00a0 }\n\n\u00a0 \u00a0 ports\n\n{ \u00a0 \u00a0 \u00a0 name: \"STANDALONE\" \u00a0 \u00a0 \u00a0 value: 9859 \u00a0 \u00a0 }\n\n\u00a0 \u00a0 networkName: \"5d03aed5-cfb3-4689-b168-0c9a94316551\"\n\n\u00a0 \u00a0 networkLocation: \"/default-rack\"\n\n\u00a0 }\n\n\u00a0 members {\n\n\u00a0 \u00a0 uuid: \"a71462ae-7865-4ed5-b84e-60616df60a0d\"\n\n\u00a0 \u00a0 ipAddress: \"9.134.51.25\"\n\n\u00a0 \u00a0 hostName: \"9.134.51.25\"\n\n\u00a0 \u00a0 ports\n\n{ \u00a0 \u00a0 \u00a0 name: \"RATIS\" \u00a0 \u00a0 \u00a0 value: 9858 \u00a0 \u00a0 }\n\n\u00a0 \u00a0 ports\n\n{ \u00a0 \u00a0 \u00a0 name: \"STANDALONE\" \u00a0 \u00a0 \u00a0 value: 9859 \u00a0 \u00a0 }\n\n\u00a0 \u00a0 networkName: \"a71462ae-7865-4ed5-b84e-60616df60a0d\"\n\n\u00a0 \u00a0 networkLocation: \"/default-rack\"\n\n\u00a0 }\n\n\u00a0 members {\n\n\u00a0 \u00a0 uuid: \"79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03\"\n\n\u00a0 \u00a0 ipAddress: \"9.134.51.215\"\n\n\u00a0 \u00a0 hostName: \"9.134.51.215\"\n\n\u00a0 \u00a0 ports\n\n{ \u00a0 \u00a0 \u00a0 name: \"RATIS\" \u00a0 \u00a0 \u00a0 value: 9858 \u00a0 \u00a0 }\n\n\u00a0 \u00a0 ports\n\n{ \u00a0 \u00a0 \u00a0 name: \"STANDALONE\" \u00a0 \u00a0 \u00a0 value: 9859 \u00a0 \u00a0 }\n\n\u00a0 \u00a0 networkName: \"79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03\"\n\n\u00a0 \u00a0 networkLocation: \"/default-rack\"\n\n\u00a0 }\n\n\u00a0 state: PIPELINE_OPEN\n\n\u00a0 type: RATIS\n\n\u00a0 factor: THREE\n\n\u00a0 id\n\n{ \u00a0 \u00a0 id: \"ec6b06c5-193f-4c30-879b-5a12284dc4f8\" \u00a0 }\n\n}\n\n]} | ret=SUCCESS |",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13260495,
      "Use new ReadWrite lock in OzoneManager",
      "Use new ReadWriteLock added in HDDS-2223.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13279858,
      "Use regex to match with ratis properties when creating ratis client",
      "This Jira is to use regex which are matching with ratis client and ratis grpc properties and set them when creating ratis client.\u00a0\n\nAdvantages:\n # We can use ratis properties directly, don't need to create corresponding ozone config.\n # When new properties are added in ratis client, we can set them and use them with out any ozone code changes.\n\nIn this Jira not removed the existing properties, if this looks fine, we can remove a clean up Jira to remove ozone config for ratis client or leave as it is for existing ones.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available, teragentest"
    ],
    [
      13313517,
      "Add resource core-site during loading of ozoneconfiguration",
      "If users add the properties of ozone to core-site, then during loading of OzoneConfiguration, it addsResource ozone-default.xml. This overrides the properties of ozone which are added to core-site.\n\nTo avoid this kind of override issue, we can addResource core-site.xml after ozone-default.xml",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13257236,
      "MR job failing on secure Ozone cluster",
      "Failing with below error:\nCaused by: Client cannot authenticate via:[TOKEN, KERBEROS]\norg.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\nat org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)\nat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)\nat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)\nat org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\nat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)\nat org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\nat org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\nat org.apache.hadoop.ipc.Client.call(Client.java:1403)\nat org.apache.hadoop.ipc.Client.call(Client.java:1367)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\nat com.sun.proxy.$Proxy79.submitRequest(Unknown Source)\nat sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\nat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\nat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\nat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\nat com.sun.proxy.$Proxy79.submitRequest(Unknown Source)\nat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:332)\nat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1163)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\nat com.sun.proxy.$Proxy80.getServiceList(Unknown Source)\nat org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:248)\nat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:167)\nat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)\nat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)\nat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)\nat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)\nat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\nat org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:102)\nat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:155)\nat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\nat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\nat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\nat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\nat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\nat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\nat org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:268)\nat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)\nat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)\nat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\nat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748)",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13247180,
      "Freon RandomKeyGenerator even if keySize is set to 0, it returns some random data to key",
      "\u00a0\n{code:java}\n***************************************************\nStatus: Success\nGit Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf\nNumber of Volumes created: 1\nNumber of Buckets created: 1\nNumber of Keys added: 1\nRatis replication factor: THREE\nRatis replication type: STAND_ALONE\nAverage Time spent in volume creation: 00:00:00,002\nAverage Time spent in bucket creation: 00:00:00,000\nAverage Time spent in key creation: 00:00:00,002\nAverage Time spent in key write: 00:00:00,101\nTotal bytes written: 0\nTotal Execution time: 00:00:05,699\n\u00a0\n{code}\n***************************************************\n\n[root@ozoneha-2 ozone-0.5.0-SNAPSHOT]# bin/ozone sh key list /vol-0-28271/bucket-0-95211\n\n[\n\n{ \u00a0 \"version\" : 0, \u00a0 \"md5hash\" : null, \u00a0 \"createdOn\" : \"Fri, 26 Jul 2019 01:02:08 GMT\", \u00a0 \"modifiedOn\" : \"Fri, 26 Jul 2019 01:02:09 GMT\", \u00a0 \"size\" : 36, \u00a0 \"keyName\" : \"key-0-98235\", \u00a0 \"type\" : null }\n\n]\n\n\u00a0\n\nThis is because of the below code in RandomKeyGenerator:\n{code:java}\nfor (long nrRemaining = keySize - randomValue.length;\n nrRemaining > 0; nrRemaining -= bufferSize) {\n int curSize = (int) Math.min(bufferSize, nrRemaining);\n os.write(keyValueBuffer, 0, curSize);\n}\nos.write(randomValue);\nos.close();{code}\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13389978,
      "Avoid refresh pipeline for S3 headObject",
      "S3 head uses OM API lookup key which refreshes pipeline info by contacting SCM.\n\nFor S3 head we donot require any pipeline info, we need very basic details like length, type, etag and last modification time. \n\nBy removing pipeline info which is not required for HEAD object, HEAD API performance can be improved.\n\nThis is identified during looking up graphs from [~kerneltime] testing",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13225883,
      "Convert all OM Volume related operations to HA model",
      "In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.\n # StartTransaction, where we validate request and check for any errors and return the response.\n # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.\n\nIn this way, all requests which are failed with like volume not found or some conditions which i have not satisfied like when deleting volume should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13284749,
      "OM HA stability issues",
      "To conclude a little,\u00a0_+{color:#ff0000}major issues{color}+_\u00a0that I find:\n # When I do a long running s3g writing to cluster with OM HA and I stop the Om leader to force a re-election, the writing will stop and can never recover.\n\n--updates 2020-02-20:\n\nhttps://issues.apache.org/jira/browse/HDDS-3031\u00a0{color:#ff0000}fixes{color} this issue.\n\n\u00a0\n\n2. If I force a OM re-election and do a scm restart after that, the cluster cannot see any leader datanode and no datanodes are able to send pipeline reports, which makes the cluster unavailable as well. I consider this a multi-failover case when the leader OM and SCM are on the same node and there is a short outage happen to the node.\n\n\u00a0\n\n--updates 2020-02-20:\n\n\u00a0When you do a jar swap for a new version of Ozone and enable OM HA while keeping the same ozone-site.xml as last time, if you've written some data into the last Ozone cluster (and therefore there are existing versions and metadata for om and scm), SCM cannot be up after the jar swap.\n\n{color:#ff0000}Error logs{color}:\u00a0PipelineID=aae4f728-82ef-4bbb-a0a5-7b3f2af030cc not found in scm out logs when scm process cannot be started.\n\n\u00a0\n\n--updates 2020-02-24:\n\nAfter I add some logs to SCM starter:\nAssuming SCM is only bounced after the leader OM is stopped\n1. If SCM is bounced {color:#de350b}after{color} former leader OM is restarted, meaning all OMs are up, SCM will be bootstrapped correctly but there will be missing pipeline report from the node who doesn't have OM process on it (it's always him tho). This would cause all pipelines stay at ALLOCATED state and cluster will be in safemode. At this point, if I {color:#de350b}restart the blacksheep datanode{color}, it will come back and send the pipeline report to SCM and all pipelines will be at OPEN state.\n2. If SCM is bounced {color:#de350b}before{color} the former leader OM is restarted, meaning not all OMs in ratis ring are up, SCM {color:#de350b}cannot{color} be bootstrapped correctly and it shows Pipeline not found.\n\n\u00a0\n\nOriginal posting:\n\nUse S3 gateway to keep writing data into a specific s3 gateway endpoint. After the writer starts to work, I kill the OM process on the OM leader host. After that, the s3 gateway can never allow writing data and keeps reporting InternalError for all new coming keys.\n\nProcess Process-488:\n{noformat}\n S3UploadFailedError: Failed to upload ./20191204/file1056.dat to ozone-test-reproduce-123/./20191204/file1056.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\n Process Process-489:\n S3UploadFailedError: Failed to upload ./20191204/file9631.dat to ozone-test-reproduce-123/./20191204/file9631.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\n Process Process-490:\n S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\n Process Process-491:\n S3UploadFailedError: Failed to upload ./20191204/file4220.dat to ozone-test-reproduce-123/./20191204/file4220.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\n Process Process-492:\n S3UploadFailedError: Failed to upload ./20191204/file5523.dat to ozone-test-reproduce-123/./20191204/file5523.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\n Process Process-493:\n S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\n{noformat}\n\nThat's a partial list and note that all keys are different. I also tried re-enable the OM process on previous leader OM, but it doesn't help since the leader has changed. Also attach partial OM logs:\n{noformat}\n 2020-02-12 14:57:11,128 [IPC Server handler 72 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 72 on 9862, call Call#4859 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561\n org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n 2020-02-12 14:57:11,918 [IPC Server handler 159 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 159 on 9862, call Call#4864 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561\n org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n 2020-02-12 14:57:15,395 [IPC Server handler 23 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 23 on 9862, call Call#4869 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561\n org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n{noformat}\n\u00a0\n\n\u00a0\n\nAlso attach the ozone-site.xml config to enable OM HA:\n{noformat}\n<property>\n <name>ozone.om.service.ids</name>\n <value>OMHA</value>\n </property>\n <property>\n <name>ozone.om.nodes.OMHA</name>\n <value>om1,om2,om3</value>\n </property>\n <property>\n <name>ozone.om.node.id</name>\n <value>om1</value>\n </property>\n <property>\n <name>ozone.om.address.OMHA.om1</name>\n <value>9.134.50.210:9862</value>\n </property>\n <property>\n <name>ozone.om.address.OMHA.om2</name>\n <value>9.134.51.215:9862</value>\n </property>\n <property>\n <name>ozone.om.address.OMHA.om3</name>\n <value>9.134.51.25:9862</value>\n </property>\n <property>\n <name>ozone.om.ratis.enable</name>\n <value>true</value>\n </property>\n <property>\n <name>ozone.enabled</name>\n <value>true</value>\n <tag>OZONE, REQUIRED</tag>\n <description>\n Status of the Ozone Object Storage service is enabled.\n Set to true to enable Ozone.\n Set to false to disable Ozone.\n Unless this value is set to true, Ozone services will not be started in\n the cluster.\n\nPlease note: By default ozone is disabled on a hadoop cluster.\n </description>\n </property>\n{noformat}",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "OMHATest"
    ],
    [
      13184761,
      "Datanode loops forever if it cannot create directories",
      "Datanode starts but runs in a tight loop forever if it cannot create the DataNode ID directory e.g. due to permissions issues. I encountered this by having a typo in my ozone-site.xml for {{ozone.scm.datanode.id}}.\n\nIn\u00a0just a few minutes the DataNode had generated over 20GB of log+out files with the following exception:\n{code:java}\n2018-09-12 17:28:20,649 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 2\n63:\njava.io.IOException: Unable to create datanode ID directories.\nat org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748)\n2018-09-12 17:28:20,648 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Execution exception when running task in Datanode State Mach\nine Thread - 160\n2018-09-12 17:28:20,650 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 1\n60:\njava.io.IOException: Unable to create datanode ID directories.\nat org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748){code}\n\nWe should just exit since this is a fatal issue.",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "newbie"
    ],
    [
      13236710,
      "Merge code for HA and Non-HA OM requests for bucket",
      "In this Jira, we shall use the new code added in HDDS-1551 for Non-HA flow.\n\n\u00a0\n\nThis Jira modifies the\u00a0bucket requests only, further requests will be handled in further Jira's.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13318768,
      "Generate encryption info for the bucket outside bucket lock",
      "This Jira is to generate FileEncryption for a key outside the bucket lock.\nAs right now, we hold the lock when making a network call to KMS to obtain encryption info.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13319822,
      "Deprecate ozone.s3g.volume.name",
      "HDDS-3612 introduced bucket links.\nAfter this feature now we don't need this parameter, any volume/bucket can be exposed to S3 via using bucket links.\n\nozone bucket link srcvol/srcbucket destvol/destbucket\n\nSo now to expose any ozone bucket to S3G\n\nFor example, the user wants to expose a bucket named bucket1 under volume1 to S3G, they can run below command\n\n{code:java}\nozone bucket link volume1/bucket1 s3v/bucket2\n{code}\n\nNow, the user can access all the keys in volume/bucket1 using s3v/bucket2 and also ingest data to the volume/bucket1 using using s3v/bucket2\n\nThis Jira is opened to remove the config from ozone-default.xml\nAnd also log a warning message to use bucket links, when it does not have default value s3v.\n",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13313784,
      "Fix typo in pom.xml",
      "ratis.thirdpary.version -> ratis.thirdparty.version",
      "HDDS",
      "Resolved",
      4,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13257232,
      "Rename classes under package org.apache.hadoop.utils",
      "Rename classes under package org.apache.hadoop.utils -> org.apache.hadoop.hdds.utils in hadoop-hdds-common\n\n\u00a0\n\nNow, with current way, we might collide with hadoop classes.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13318363,
      "Create volume required for S3G during OM startup",
      "Create volume required for S3G operations during OM startup",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13251698,
      "Remove RatisClient in OM HA",
      "In OM, we use ratis server api's to submit request. We can remove the RatisClient code from OM, which is no more used in submitting requests to ratis.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13421184,
      "SCM StateMachine failing to reinitialize doesn't terminate the process",
      "When SCM state machine fails to reinitialize due to any error, it simply logs an error message saying \"Failed to reinitialize SCMStateMachine.\" and continues with starting up other servers. In this case, it should not proceed if the state machine cannot be initialized. Also, the exception should be printed along with the error message to help with debugging the issue.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13377602,
      "Use scm#checkLeader before processing client requests ",
      "Right now to check leader we use ScmContext#isLeader, this gets updated by notifyLeaderChanged.\n\nBut SCM server should start accepting requests when it is leader and isLeaderReady. \n\nWe need isLeaderReady also because Statemachine should apply all the log committed transactions to start accepting requests.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13222489,
      "Test ScmChillMode testChillModeOperations failed",
      "[https://ci.anzix.net/job/ozone-nightly/35/testReport/junit/org.apache.hadoop.ozone.om/TestScmChillMode/testChillModeOperations/]\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13212686,
      "Allow option for force in DeleteContainerCommand",
      "Right now, we check container state if it is not open, and then\u00a0we delete container.\n\nWe need a way to delete the containers which are open, so adding a force flag will allow deleting a container without any state checks. (This is required for delete replica's when SCM detects over-replicated, and that container to delete can be in open state)",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13322617,
      " Normalize Keypath for listKeys.",
      "When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.\n\nWhen listKeys uses given keyName(not normalized key path) as prefix and Starkey the list-keys will return empty result.\n\nSimilar to HDDS-4102, we should normalize startKey and keyPrefix.\n\n\n",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13297648,
      "S3A failing complete multipart upload with Ozone S3",
      "\n{code:java}\njavax.xml.bind.UnmarshalException: unexpected element (uri:\"\", local:\"CompleteMultipartUpload\"). Expected elements are <{http://s3.amazonaws.com/doc/2006-03-01/}CompleteMultipartUpload>,<{http://s3.amazonaws.com/doc/2006-03-01/}Part>\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleEvent(UnmarshallingContext.java:744)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:262)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:257)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportUnexpectedChildElement(Loader.java:124)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext$DefaultRootLoader.childElement(UnmarshallingContext.java:1149)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:574)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:556)\n        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:168)\n        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:509)\n        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:374)\n        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl$NSContentDriver.scanRootElementHook(XMLNSDocumentScannerImpl.java:613)\n        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3132)\n        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:852)\n\n{code}\n\n\nIt seems http://s3.amazonaws.com/doc/2006-03-01/ is expected in the element.\nBut in class CompleteMultipartUploadRequest,  namespace http://s3.amazonaws.com/doc/2006-03-01/ is not defined here.\n\nReported by [~sammichen]\n\n\n\n\n\n",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13286899,
      "SCM crash during startup does not print any error message to log",
      "SCM start up failed due to a pipelineNotFoundException, there is no error message logged in to SCM log.\n\nIn the log file, we can see just below log message no reason for the crash is logged.\n\n\u00a0\n\n\u00a0\n{code:java}\n2020-02-20 15:37:56,079 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down StorageContainerManager at xx.xx.xx/10.65.51.49\n{code}\nIn the out file, we can see below, but not complete exception message.\n{code:java}\nPipelineID=xxxxx\u00a0not found{code}\n\u00a0\n\nThe actual reason for failure is not clearly logged if an exception has occurred during SCM startup.\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "OMHATest, pull-request-available"
    ],
    [
      13259144,
      "Adding container related metrics in SCM",
      "This jira aims to add more container related metrics to SCM.\n Following metrics will be added as part of this jira:\n * Number of successful create container calls\n * Number of failed create container calls\n * Number of successful delete container calls\n * Number of failed delete container calls\n * Number of list container ops.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13246100,
      "Implement S3 Abort MPU request to use Cache and DoubleBuffer",
      "Implement S3\u00a0Abort MPU\u00a0request to use OM Cache, double buffer.\n\n\u00a0\n\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13237383,
      "Implement Key Write Requests to use Cache and DoubleBuffer",
      "Implement\u00a0Key write requests to use OM Cache, double buffer.\u00a0\n\nIn this Jira will add the changes to implement\u00a0key operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13251345,
      "Implement default acls for bucket/volume/key for OM HA code",
      "This Jira is to implement default ACLs for Ozone volume/bucket/key.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13261233,
      "Provide config for fair/non-fair for OM RW Lock",
      "Provide config in OzoneManager Lock for fair/non-fair for OM RW Lock.\n\nCreated based on review comments during HDDS-2244.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13246688,
      "Fix TestOzoneManagerHA and TestOzoneManagerSnapShotProvider",
      "All tests in TestOzoneManagerHA are failing with below exception.\n\n\u00a0\n\nBroken by HDDS-1649. Not sure why this test is not running in CI.\u00a0\n\nFrom PR HDDS-1845 run, not seeing this test run.\u00a0\n\n[https://ci.anzix.net/job/ozone/17452/testReport/org.apache.hadoop.ozone.om/]\n\n\u00a0\n{code:java}\njava.lang.Exception: test timed out after 300000 milliseconds\nat java.lang.Object.wait(Native Method)\n at java.lang.Object.wait(Object.java:502)\n at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)\n at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)\n at org.apache.hadoop.ipc.Client.call(Client.java:1457)\n at org.apache.hadoop.ipc.Client.call(Client.java:1367)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\n at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\n at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:326)\n at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1155)\n at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:234)\n at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:156)\n at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)\n at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:126)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)\n at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)\n at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n\u00a0\n{code}\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13218198,
      "Pipeline Rule where atleast one datanode is reported in the pipeline",
      "h2. Pipeline Rule with configurable percentage of pipelines with at least one datanode reported:\n\nIn this rule we consider when at least \u00a090% of pipelines have at least one datanode reported. \n\n\u00a0\n\nThis rule satisfies, when we exit chill mode, Ozone clients will have at least one open replica for reads to succeed. (We can increase this threshold default from 90%, if we want to see fewer failures during reads after exit chill mode.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13405707,
      "parse and dump SCM ratis segment file to printable text",
      "With RATIS-755, a log dump utility for ratis logs has been added. however to parse SM data, a toString supplier is needed to dump the log to printable form. This can be in the form of JSON , XML.\n\nThis also will be very useful during debug.",
      "HDDS",
      "Resolved",
      3,
      2,
      2640,
      "pull-request-available"
    ],
    [
      13280873,
      "Remove ozone ratis server specific config keys",
      "Once HDDS-2903 went in, now we can use direct ratis server configurations in XceiverClientRatis. This Jira is to clean up the old configuration and add any new required configuration.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available, teragentest"
    ],
    [
      13186250,
      "Implement CopyObject REST endpoint",
      "The Copy object is a simple call to Ozone Manager.  This API can only be done after the PUT OBJECT Call.\n\nThis implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.\n\nIf the Put Object call has this header, then Put Object call will issue a rename. \n\nWork Items or JIRAs\nDetect the presence of the extra header - x-amz-copy-source\nMake sure that destination bucket exists.\n\nThe AWS reference is here:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\n\n(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody would be interested, I can be more specific, explain what we need or help).",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "newbie"
    ],
    [
      13248162,
      "Support Bucket ACL operations for OM HA.",
      "-HDDS-15+40+-\u00a0adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13407835,
      "OM Validate S3 Auth for write requests",
      "Based on logic introduced in HDDS-4440 on the write path, if the requests has S3 Auth information then extract and validate. All subsequent identity to be based on S3 Auth and not RPC thread local auth information.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13378748,
      "SCM subsequent init failed when previous scm init failed",
      "The problem is SCM init because we use a new clusterID when the version writing failed.\n\n\n{code:java}\nCould not initialize SCM version file\njava.io.IOException: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING\n\tat org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)\n\tat org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)\n\tat org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:71)\n\tat org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:354)\n\tat org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:371)\n\tat org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.initialize(SCMRatisServerImpl.java:115)\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:925)\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:173)\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:110)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat picocli.CommandLine.executeUserObject(CommandLine.java:1952)\n\tat picocli.CommandLine.access$1100(CommandLine.java:145)\n\tat picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)\n\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2326)\n\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2291)\n\tat picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)\n\tat picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)\n\tat picocli.CommandLine.parseWithHandler(CommandLine.java:2465)\n\tat org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)\n\tat org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:57)\nCaused by: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING\n{code}\n",
      "HDDS",
      "Resolved",
      2,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13239152,
      "Cleanup Volume Request 2 phase old code",
      "This Jira is to clean up the old 2 phase HA code for Volume requests.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13292028,
      "Datanode startup is slow due to iterating container DB 2-3 times",
      "During Datanode startup, for each container we iterate 2 times entire DB\n1. For Setting block length\n2. For finding delete Key count.\n\nAnd for open containers, we do step 1 again.\n\n*Code Snippet:*\n*ContainerReader.java:*\n\n*For setting Bytes Used:*\n{code:java}\n      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()\n          .getRangeKVs(null, Integer.MAX_VALUE,\n              MetadataKeyFilters.getNormalKeyFilter());\n\n      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {\n        BlockData blockData;\n        try {\n          blockData = BlockUtils.getBlockData(e.getValue());\n          return blockData.getSize();\n        } catch (IOException ex) {\n          return 0L;\n        }\n      }).sum();\n      kvContainerData.setBytesUsed(bytesUsed);\n{code}\n\n*For setting pending deleted Key count*\n\n{code:java}\n          MetadataKeyFilters.KeyPrefixFilter filter =\n              new MetadataKeyFilters.KeyPrefixFilter()\n                  .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n          int numPendingDeletionBlocks =\n              containerDB.getStore().getSequentialRangeKVs(null,\n                  Integer.MAX_VALUE, filter)\n                  .size();\n          kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n{code}\n\n*For open Containers*\n\n{code:java}\n          if (kvContainer.getContainerState()\n              == ContainerProtos.ContainerDataProto.State.OPEN) {\n            // commitSpace for Open Containers relies on usedBytes\n            initializeUsedBytes(kvContainer);\n          }\n{code}\n\n\n*Jstack of DN during startup*\n{code:java}\n\"Thread-8\" #34 prio=5 os_prio=0 tid=0x00007f5df5070000 nid=0x8ee runnable [0x00007f4d840f3000]\n   java.lang.Thread.State: RUNNABLE\n        at org.rocksdb.RocksIterator.next0(Native Method)\n        at org.rocksdb.AbstractRocksIterator.next(AbstractRocksIterator.java:70)\n        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:195)\n        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:155)\n        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:158)\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:191)\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:168)\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:146)\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:101)\n        at java.lang.Thread.run(Thread.java:748)\n{code}\n",
      "HDDS",
      "Resolved",
      1,
      4,
      2640,
      "billiontest, pull-request-available"
    ],
    [
      13376486,
      "For AccessControlException do not perform failover",
      "For AccessControlException donot perform failOver, as there is no real need.\n{code:java}\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1454)\n        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.recommissionNodes(SCMClientProtocolServer.java:459)\n        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.recommissionNodes(StorageContainerLocationProtocolServerSideTranslatorPB.java:646)\n        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:317)\n        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)\n        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:155)\n        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46954)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n        at java.base/java.security.AccessController.doPrivileged(Native Method)\n        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.8:9860 after 14 failover attempts. Trying to failover after sleeping for 2000ms.\nAccess denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.\n{code}\n",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13286563,
      "Fix Retry handling in ozone RPC Client",
      "Right now for all other exceptions other than serviceException we use FailOverOnNetworkException.\n\nThis Exception policy is created with 15 max fail overs and 15 retries.\u00a0\n\n\u00a0\n{code:java}\nretryPolicyOnNetworkException.shouldRetry(\n exception, retries, failovers, isIdempotentOrAtMostOnce);{code}\n*2 issues with this:*\n # When shouldRetry returns action FAILOVER_AND_RETRY, it will stuck with same OM, and does not perform failover to next OM.\u00a0 As OMFailoverProxyProvider#performFailover() is a dummy call does not perform any failover.\n # When ozone.client.failover.max.attempts is set to 15, now with 2 policies with each set to 15, we will retry 15*2 times in worst scenario.\u00a0\n\n\u00a0\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "OMHA, OMHATest, pull-request-available"
    ],
    [
      13186256,
      "Implement HeadBucket REST endpoint",
      "This operation is useful to determine if a bucket exists and you have permission to access it. The operation returns a 200 OK if the bucket exists and you have permission to access it. Otherwise, the operation might return responses such as 404 Not Found and 403 Forbidden.  \n\nSee the reference here:\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "newbie"
    ],
    [
      13327540,
      "Get API not working from S3A filesystem with Ozone S3",
      "TroubleShooting S3A mentions S3 compatible servers that donot support Etags will see this server\n\nRefer [link|https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html] and look for below section content.\nUsing a third-party S3 implementation that doesn\u2019t support eTags might result in the following error.\n\norg.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://my-bucket/test/file.txt':\n Change detection policy requires ETag\n  at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:153)\n  at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:200)\n  at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:346)\n  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$2(Invoker.java:195)\n  at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\n  at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\n  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:193)\n  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:215)\n  at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:339)\n  at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:372)\n\n\n{code:java}\norg.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag\n\tat org.apache.hadoop.fs.s3a.impl.ChangeTracker.processNewRevision(ChangeTracker.java:275)\n\tat org.apache.hadoop.fs.s3a.impl.ChangeTracker.processMetadata(ChangeTracker.java:261)\n\tat org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:195)\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:208)\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:359)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$3(Invoker.java:223)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$5(Invoker.java:347)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)\n\tat org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:343)\n\tat org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:221)\n\tat org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:265)\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:351)\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:464)\n\tat java.io.DataInputStream.read(DataInputStream.java:100)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:494)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:416)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:351)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:286)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:271)\n\tat org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)\n\tat org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)\n\tat org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:266)\n\tat org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)\n\tat org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:237)\n\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:177)\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:328)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:391)\nget: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag\n{code}\n",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "OzoneS3, S3A"
    ],
    [
      13317592,
      "Update proto.lock files",
      "HDDS-3807 and HDDS-3612 introduced new additions to proto files but failed to update proto.lock files.\u00a0\u00a0",
      "HDDS",
      "Resolved",
      3,
      3,
      2640,
      "pull-request-available"
    ],
    [
      13319052,
      "Ozone s3 API return 400 Bad Request for head-bucket for non existing bucket",
      "Ozone s3 API returns 400 Bad Request for head-bucket for non-existing bucket.\n\nhrt_qa$ aws s3api  --ca-bundle=/usr/local/share/ca-certificates/ca.crt --endpoint https://s3g:9879/  head-bucket --bucket fsdghj\n\nAn error occurred (400) when calling the HeadBucket operation: Bad Request\n\nIt should return 404 as per AWS documentation:\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/head-bucket.html\n\nA client error (404) occurred when calling the HeadBucket operation: Not Found ",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13318795,
      "Update S3 related documentation",
      "HDDS-3993 created volume required for S3G during the OM startup.\nSo, remove the step that s3v volume needs to be created.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13319173,
      "Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true",
      "Sets\u00a0ozone.om.enable.filesystem.paths=true, then starts the Ozone cluster.\n{code:java}\n[root~]$ ozone fs -mkdir o3fs://bucket2.vol2.ozone1/subdir2\n[root~]$ ozone fs -mv o3fs://bucket2.vol2.ozone1/subdir2 o3fs://bucket2.vol2.ozone1/subdir2-renamedmv: Key not found /vol2/bucket2/subdir2\n{code}\n\u00a0",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13318794,
      "S3G startup fails when multiple service ids are configured.",
      "This Jira is to fix this TODO.\n\nOzoneServiceProvider.java L59:\n{code:java}\n      // HA cluster.\n      //For now if multiple service id's are configured we throw exception.\n      // As if multiple service id's are configured, S3Gateway will not be\n      // knowing which one to talk to. In future, if OM federation is supported\n      // we can resolve this by having another property like\n      // ozone.om.internal.service.id.\n      // TODO: Revisit this later.\n      if (serviceIdList.size() > 1) {\n        throw new IllegalArgumentException(\"Multiple serviceIds are \" +\n            \"configured. \" + Arrays.toString(serviceIdList.toArray()));\n{code}\n\n      ",
      "HDDS",
      "Resolved",
      2,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13366252,
      "[SCM HA Security] Make storeValidCertificate method idempotent",
      "This Jira is to make storeValidCertificate idempotent so that during replay it does not cause any issues.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13345620,
      "TableCache Refactor to fix issues in cleanup never policy",
      "Right now we have 2 clean up policies.\n1. Never\n2. Manual\n\nNever = Full Table Cache\nManual = Partial Table Cache\n\nIn OM, the main purpose of Table cache is for correctness. (Because OM return response after adding to cache, does not wait for double buffer flush to complete)\n\nThe current implementation has few problems.\n1. Cleanup Policy Never uses ConcurrentSkipListMap, and its computeIfPresent is not atomic, so there can be a race condition between cleanup and requests adding to cache. (This might cause cleaning up entries which are not flushed to DB, and this can cause correctness issue)\n2. Cleanup for override entries for full cache, never removes epoch entries.\n\n*Proposal:*\n1. Make TableCache based on cache type and have separate implementation for full cache and partial cache.\n2. Fix FullCache issue, using the lock.\n3. Fix evict cache logic for full cache to cleanup epoch entries for override entries.\n\n",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13386148,
      "Return latest version of key location for client on createKey/createFile",
      "HDDS-5243 was a patch for omitting unnecessary key locations for clients on reading. But the same warning of large response size observed in our cluster for putting data. The patch can also be ported for putting data, as long as until object versioning is supported.\n\nMy hypothesis is: The large message was originally, and possibly maybe due to this warning and sudden connection close from client side on reading large message in Hadoop IPC layer, from Ozone Manager - which causes hopeless 15 retries from RetryInvocationHandler. The retries create another entry in OpenKeyTable but they never moved to KeyTable because the key never gets commited.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13222490,
      "Fix asf license errors",
      "HDDS-1250 added a few new files. In few of them, it is missing adding asf license header.\n\n[https://github.com/apache/hadoop/pull/591]\n\n\u00a0\n\nYetus\u00a0has not reported about them. I think Yetus is broken in warning asf license errors.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13245077,
      "Add Eviction policy for table cache",
      "In this Jira we will add eviction policy for table cache.\n\nIn this Jira, we will add 2 eviction policies for the cache.\n\nNEVER, // Cache will not be cleaned up. This mean's the table maintains\u00a0full cache.\nAFTERFLUSH // Cache will be cleaned up, once after flushing to DB.\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13217050,
      "After allocating container, we are not adding to container DB.",
      "If we don't\u00a0do that, we get an error when handling container report for open containers.\n\nAs they don't exist in container DB.\n\n\u00a0\n{code:java}\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.lang.Thread.run(Thread.java:748)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 2019-02-21 00:00:32 ERROR ContainerReportHandler:173 - Received container report for an unknown container 1 from datanode e2733c00-162b-4993-a986-f6104f5008d8{ip: 172.18.0.2, host: 4f4e683d86c3} {}\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #1\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:543)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.updateContainerReplica(ContainerStateMap.java:230)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerStateManager.updateContainerReplica(ContainerStateManager.java:565)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerReplica(SCMContainerManager.java:393)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ReportHandlerHelper.processContainerReplica(ReportHandlerHelper.java:74)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:159)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:110)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:51)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n{code}\n\u00a0\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13231110,
      "Use strongly typed codec implementations for the S3Table",
      "HDDS-864 added the implementation for Strongly typed codec implementation for the tables of OmMetadataManager.\n\n\u00a0\n\nTables which are added as part of S3 Implementation are not using this. This Jira is address to this.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13265830,
      "Remove usage of LogUtils class from ratis-common",
      "MiniOzoneChaoasCluster.java for setting log level it uses LogUtils from ratis-common. But this is removed from LogUtils as part of Ratis-508.\n\nWe can avoid depending on ratis for this, and use GenericTestUtils from hadoop-common test.\n\nLogUtils.setLogLevel(GrpcClientProtocolClient.LOG, Level.WARN);",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13248984,
      "Support Prefix ACL operations for OM HA.",
      "+-HDDS-1608-+\u00a0adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13242320,
      "Cleanup 2phase old HA code for Key requests.",
      "HDDS-1638 brought in HA code for Key operations like allocateBlock,createKey etc.,\u00a0\n\nOld code changes which are added as part of HDDS-1250 and HDDS-1262 for allocateBlock and openKey.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13361423,
      "[SCM HA Security] Add failover proxy to SCM Security Server Protocol",
      "This Jira is to add support for FailOverProxyProvider for SCMSecurityServer which is used by OM and Datanode. (In further jira's when security work is implemented, this API will be used by SCM also)",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13381669,
      "Handle SIGTERM to ensure clean shutdown of SCM",
      "Handle SIGTERM 15 with shutdown hook to properly/clean shutdown SCM.\n\nIn this way in SCM HA, the snapshot will be called and pending transactions will be flushed to DB.\n\n",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13218453,
      "Healthy pipeline Chill Mode rule to consider only pipelines with replication factor three",
      "Few offline comments from [~nandakumar131]\n # We should not process pipeline report from datanode\u00a0again during calculations.\n # We should consider only replication factor 3 ratis\u00a0pipelines.",
      "HDDS",
      "Resolved",
      3,
      4,
      2640,
      "pull-request-available"
    ],
    [
      13226268,
      "Convert all OM Bucket related operations to HA model",
      "In this jira, we shall convert all OM Bucket related operations to OM HA model, which is a 2 step.\n # StartTransaction, where we validate request and check for any errors and return the response.\n # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.\n\nIn this way, all requests which are failed with like\u00a0bucket not found or some conditions which i have not satisfied like when deleting\u00a0bucket should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13220954,
      "In OM HA AllocateBlock call where connecting to SCM from OM should not happen on Ratis",
      "In OM HA, currently when allocateBlock\u00a0is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB. The problem with this is, every OM allocateBlock and appends new BlockInfo into OMKeyInfom and also this a correctness issue. (As all OM's should have the same block information for a key, even though eventually this might be changed during key commit)\n\n\u00a0\n\nThe proposed approach is:\n\n1. Calling SCM for allocation of block will happen outside of ratis, and this block information is passed and\u00a0writing to DB will happen via Ratis.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13287468,
      "UpdateID check should be skipped for non-HA OzoneManager",
      "Delete key is failing . Here is the stack trace of the failure:\n\n\u00a0\n\n\u00a0\n{noformat}\nINFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 26 which is not greater than the current value of 433 for OMKeyInfo{volume='vol-test-restartcomponentozonereaddata-1582093704', bucket='buck-test-restartcomponentozonereaddata-1582093704', key='ReadOzoneFile_1582093709', dataSize='10485760', creationTime='1582093712218', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862. Trying to failover immediately.\n\u00a0\n..\n..\n..\n..\n\u00a0\n20/02/19 03:37:17 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 22 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862 after 15 failover attempts. Trying to failover immediately. E 2020-02-19 03:37:17,895 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(287)) - Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E 20/02/19 03:37:17 ERROR ha.OMFailoverProxyProvider: Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E Trying to set updateID to 23 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'}]\n{noformat}\n\u00a0\n\n\u00a0",
      "HDDS",
      "Resolved",
      1,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13242825,
      "Use ExecutorService in OzoneManagerStateMachine",
      "In the current code in applyTransaction we have\u00a0\n\nCompletableFuture<Message> future = CompletableFuture\n .supplyAsync(() -> runCommand(request, trxLogIndex)); We are using ForkJoin#commonPool.\n\nWith the current approach we have 2 issues:\n # Thread exhausts when using this common pool.\n # Not a good practice of using common pool. Found some issues in our testing by using similarly in RatisPipelineUtils.\n # OM DB's across replica can be out of sync when the apply transactions are applied in out of order.",
      "HDDS",
      "Resolved",
      3,
      7,
      2640,
      "pull-request-available"
    ],
    [
      13408956,
      "MPU getKey can fail, if completeMPU result is still in cache",
      "Failure was observed on this CI run: [https://github.com/apache/ozone/runs/4015387580?check_suite_focus=true]\n\nThe output bundles expired before I could add them to this Jira, but the failure can be reproduced on master by applying the patch attached to this Jira and running the test. The patch speeds up repeated execution of the test by repeatedly writing keys without having to spin up a new mini ozone cluster in between each write.\u00a0 It usually takes about 4 minutes and 1200 iterations to reproduce.\n\nThe failing assertion is:\n{code:java}\nOzoneInputStream inputStream = bucket.readKey(keyName);\nAssert.assertTrue(inputStream instanceof MultipartCryptoKeyInputStream);{code}\nIndicating a plain OzoneInputStream is returned, since the class has no other sub classes.\n\n*I have found the reason for this.*\n1. If complete MPU is completed, it adds the entry to keyTable.\n2. Now if getKey happens on this, if doublebuffer flush not completed flush and cleaned up cache if entry is still in keyTable, the key info returned as Not Mpu Key, this is due to a bug in OmKeyInfo#copyObject Which is not using isMultipartKey.",
      "HDDS",
      "Resolved",
      3,
      1,
      2640,
      "pull-request-available"
    ],
    [
      13249046,
      "Remove hadoop script from ozone distribution",
      "/bin/hadoop script is included in the ozone distribution even if we a dedicated /bin/ozone\n\n[~arp] reported that it can be confusing, for example \"hadoop classpath\" returns with a bad classpath (ozone classpath <projectname>) should be used instead.\n\nTo avoid such confusions I suggest to remove the hadoop script from distribution as ozone script already provides all the functionalities.\n\nIt also helps as to reduce the dependencies between hadoop 3.2-SNAPSHOT and ozone as we use the snapshot hadoop script as of now.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13254999,
      "Failing acceptance test - smoketests.ozonesecure-s3.MultipartUpload",
      "{{\"smoketests.ozonesecure-s3.MultipartUpload.Test Multipart Upload with the simplified aws s3 cp API\"}} acceptance test is failing.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "TriagePending"
    ],
    [
      13249697,
      "Acceptance tests fail if scm webui shows invalid json",
      "Acceptance test of a nightly build is failed with the following error:\n\n{code}\nCreating ozonesecure_datanode_3 ... \n\u001b[7A\u001b[2K\nCreating ozonesecure_kdc_1      ... \u001b[32mdone\u001b[0m\n\u001b[7B\u001b[6A\u001b[2K\nCreating ozonesecure_om_1       ... \u001b[32mdone\u001b[0m\n\u001b[6B\u001b[8A\u001b[2K\nCreating ozonesecure_scm_1      ... \u001b[32mdone\u001b[0m\n\u001b[8B\u001b[1A\u001b[2K\nCreating ozonesecure_datanode_3 ... \u001b[32mdone\u001b[0m\n\u001b[1B\u001b[5A\u001b[2K\nCreating ozonesecure_kms_1      ... \u001b[32mdone\u001b[0m\n\u001b[5B\u001b[4A\u001b[2K\nCreating ozonesecure_s3g_1      ... \u001b[32mdone\u001b[0m\n\u001b[4B\u001b[2A\u001b[2K\nCreating ozonesecure_datanode_2 ... \u001b[32mdone\u001b[0m\n\u001b[2B\u001b[3A\u001b[2K\nCreating ozonesecure_datanode_1 ... \u001b[32mdone\u001b[0m\n\u001b[3Bparse error: Invalid numeric literal at line 2, column 0\n{code}\n\nhttps://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-5b87q/acceptance/output.log\n\nThe problem is in the script which checks the number of available datanodes.\n\nIf the HTTP endpoint of the SCM is already started BUT not ready yet it may return with a simple HTML error message instead of json. Which can not be parsed by jq:\n\nIn testlib.sh:\n\n{code}\n  37   \u2502   if [[ \"${SECURITY_ENABLED}\" == 'true' ]]; then\n  38   \u2502     docker-compose -f \"${compose_file}\" exec -T scm bash -c \"kinit -k HTTP/scm@EXAMPL\n       \u2502 E.COM -t /etc/security/keytabs/HTTP.keytab && curl --negotiate -u : -s '${jmx_url}'\"\n  39   \u2502   else\n  40   \u2502     docker-compose -f \"${compose_file}\" exec -T scm curl -s \"${jmx_url}\"\n  41   \u2502   fi \\\n  42   \u2502     | jq -r '.beans[0].NodeCount[] | select(.key==\"HEALTHY\") | .value'\n{code}\n\nOne possible fix is to adjust the error handling (set +x / set -x) per method instead of using a generic set -x at the beginning. It would provide a more predictable behavior. In our case count_datanode should not fail evert (as the caller method: wait_for_datanodes can retry anyway).",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13235840,
      "Remove hdds-server-scm dependency from ozone-common",
      "I noticed that the hadoop-ozone/common project depends on hadoop-hdds-server-scm project.\n\nThe common projects are designed to be a shared artifacts between client and server side. Adding additional dependency to the common pom means that the dependency will be available for all the clients as well.\n\n(See the attached artifact about the current, desired structure).\n\nWe definitely don't need scm server dependency on the client side.\n\nThe code dependency is just one class (ScmUtils) and the shared code can be easily moved to the common.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13339955,
      "findbugs.sh couldn't be executed after a full build",
      "./hadoop-ozone/dev-support/checks/findbugs.sh -- which is a short-cut to execute the CI findbugs check locally -- couldn't be executed locally after a full build:\n\n{code}\n./hadoop-ozone/dev-support/checks/findbugs.sh\n....\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  3.451 s\n[INFO] Finished at: 2020-11-11T11:42:40+01:00\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs (spotbugs) on project hadoop-hdds: Execution spotbugs of goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs failed: Java returned: 1 -> [Help 1]\n[ERROR] \n{code}\n\nThe problem:\n\n`target/classes` directory should be either empty/missing or it should contain java classes to make spotbugs work.\n\nOn github it works well as an empty checkout is tested. But locally it's possible that a dummy classpath file is created under `hadoop-hdds/target/classes` which breaks spotbug local execution.\n\nThe solution is easy: execute the classpath descriptor generation only if `src/main/java` dir exists.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13290503,
      "Create isolated environment for OM to test it without SCM",
      "OmKeyGenerator class from Freon can generate keys (open key + commit key). But this test tests both OM and SCM performance. It seems to be useful to have a method to test only the OM performance with faking the response from SCM.  \n\nCan be done easily with the same approach what we have in HDDS-3023: A simple utility class can be implemented and with byteman we can replace the client calls with the fake method.",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13318220,
      "Test Kubernetes examples with acceptance tests",
      "hadoop-ozone/dist/src/main/k8s/example directory contains example kubernetes resources to start Ozone in kubernetes environment. To make sure those resources are working and up-to-date I propose to test them during standard build.\n\nK3s project provides a lightweight Kubernetes distribution which can be installed easily in Github Actions environment and kubernetes based clusters can be tested.",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13170081,
      "Remove hdfs command line from ozone distribution.",
      "As the ozone release artifact doesn't contain a stable namenode/datanode code the hdfs command should be removed from the ozone artifact.\n\nozone-dist-layout-stitching also could be simplified to copy only the required jar files (we don't need to copy the namenode/datanode server side jars, just the common artifacts",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "newbie"
    ],
    [
      13338373,
      "Update README.md after TLP separation",
      "README.md can be updated with the new mailing lists and references to \"Hadoop subproject\" can be removed.",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13370653,
      "Bump version of common-compress",
      "Please see: https://github.com/apache/ozone/pull/2139",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13355166,
      "Adjust classpath of ozone version to include log4j",
      "Please see: https://github.com/apache/ozone/pull/1850",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13374638,
      "Create github check to alert when dependency tree is changed",
      "Please see: https://github.com/apache/ozone/pull/2177",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13379260,
      "EC: Create ECReplicationConfig on client side based on input string",
      "HDDS-5073 improves the existing \"ozone sh\" client to support ReplicationConfig. The input string is parsed to ReplicationConfig by the constructors of the ReplicationConfig classes with string parameters.\n\nAfter merging this improvement to the EC branch we need to implement the same constructor for ECReplicationConfig.\n\nThere are multiple options here:\n\n 1. Create an enum with ALL the possible ECReplicationConfig\n 2. Use meaningful programmatic validation rules.\n\nDuring the EC sync we agreed that 2nd option can be more flexible as we may have very huge configuration matrix with all the EC parameters.\n ",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13266764,
      "Ozoneperf docker cluster should use privileged containers",
      "The profiler [servlet|https://github.com/elek/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] (which helps to run java profiler in the background and publishes the result on the web interface) requires privileged docker containers.\n\n\u00a0\n\nThis flag is missing from the ozoneperf docker-compose cluster (which is designed to run performance tests).\n\n\u00a0\n\n\u00a0",
      "HDDS",
      "Resolved",
      3,
      3,
      4052,
      "pull-request-available"
    ],
    [
      13237212,
      "Introduce a new ozone specific runner image",
      "Ozone compose files use apache/hadoop-runner to provide a fixed environment to run any Ozone distribution.\n\n It can be better to use separated hadoop-runner and ozone-runner:\n\n 1. To make it easier to include Ozone specific behaviour (For example goofys install, scm/om initialization)\n 2. To make it clean which feature is required by all the subprojects of Hadoop and which one is Ozone specific (base on the comment from [~eyang] in HADOOP-16092)\n 3. for hadoop-runner we maintain two tags (jdk11/jdk8/latest). And it seems to be hard to maintain all of them. jdk8 is required only for hadoop and with separating hadoop-runner/ozone-runner we can use only one simple branch for ozone-runner development (and we can create incremental fixed tags very easily)",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13256106,
      "XSS fragments can be injected to the S3g landing page  ",
      "VULNERABILITY DETAILS\nThere is a way to bypass anti-XSS filter for DOM XSS exploiting a \"window.location.href\".\n\nConsidering a typical URL:\n\nscheme://domain:port/path?query_string#fragment_id\n\nBrowsers encode correctly both \"path\" and \"query_string\", but not the \"fragment_id\".\u00a0\n\nSo if used \"fragment_id\" the vector is also not logged on Web Server.\n\nVERSION\nChrome Version: 10.0.648.134 (Official Build 77917) beta\n\nREPRODUCTION CASE\nThis is an index.html page:\n\n\n{code:java}\naws s3api --endpoint <script>document.write(window.location.href.replace(\"static/\", \"\"))</script> create-bucket --bucket=wordcount</pre>\n{code}\n\n\nThe attack vector is:\nindex.html?#<script>alert('XSS');</script>\n\n* PoC:\nFor your convenience, a minimalist PoC is located on:\nhttp://security.onofri.org/xss_location.html?#<script>alert('XSS');</script>\n\n* References\n- DOM Based Cross-Site Scripting or XSS of the Third Kind - http://www.webappsec.org/projects/articles/071105.shtml\n\n\nreference:-\u00a0\n\nhttps://bugs.chromium.org/p/chromium/issues/detail?id=76796",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13258339,
      "Hadoop31-mr acceptance test is failing due to the shading",
      "From the daily build:\n\n{code}\n \tException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/ozone/shaded/org/apache/http/client/utils/URIBuilder\n\tat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:138)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.getRemoteDestination(CommandWithDestination.java:195)\n\tat org.apache.hadoop.fs.shell.CopyCommands$Put.processOptions(CopyCommands.java:259)\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:175)\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:328)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:391)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.ozone.shaded.org.apache.http.client.utils.URIBuilder\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 15 more\n{code}\n\nIt can be reproduced locally with executing the tests:\n\n{code}\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31\n./test.sh\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13286749,
      "Use meaningful name for ChunkWriter threads",
      "ChunkWriter threads acreated with a naming schema 'pool-[x]-thread-[y]'. We can use better naming (especially as we have 60 threads...)",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13343531,
      "Use fixed thread pool for closed container replication ",
      "Number of threads for closed container replications can be adjusted by the settings  {{hdds.datanode.replication.streams.limit}}. But this number is ignored today due to the misuse of {{ThreadPoolExecutor}}:\n\n{code}\nnew ThreadPoolExecutor(\n        0, poolSize, 60, TimeUnit.SECONDS,\n        new LinkedBlockingQueue<>(),\n        new ThreadFactoryBuilder().setDaemon(true)\n            .setNameFormat(\"ContainerReplicationThread-%d\")\n            .build())\n{code}\n\nHere the minimal number of threads is 0 and the maximum number of the threads is the configured value.  Threads in the thread pool supposed to be scaled up, but it doesn't.\n\n[From the JDK docs|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#ThreadPoolExecutor(int,%20int,%20long,%20java.util.concurrent.TimeUnit,%20java.util.concurrent.BlockingQueue)]:\n\nbq. A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize()) according to the bounds set by corePoolSize (see getCorePoolSize()) and maximumPoolSize (see getMaximumPoolSize()). When a new task is submitted in method execute(java.lang.Runnable), [...] [AND]  If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full.\n\nSo if queue is not full (and {{LinkedBlockgingQueue}} is unbounded by default) the threads will never be created.\n\nFor a quick fix we can switch to use static thread pool instead of dynamic and always keep the required number of threads.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13371977,
      "Adjust download pages to use Apache Ozone (tlp) artifacts",
      "Please see: https://github.com/apache/ozone-site/pull/4",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13325776,
      "Create a script to check AWS S3 compatibility",
      "Ozone S3G implements the REST interface of AWS S3 protocol. Our robot test based scripts check if it's possible to use Ozone S3 with the AWS client tool.\n\nBut occasionally we should check if our robot test definitions are valid: robot tests should be executed with using real AWS endpoint and bucket(s) and all the test cases should be passed.\n\nThis patch provides a simple shell script to make this cross-check easier.  ",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13363276,
      "Enhance SCMServerProtocol with using ReplicationConfig",
      "In HDDS-4882 a new ReplicationConfig is introduced. This patch shows how can it be used between OM and SCM on the protocol.\n\nThis patch is not a full refactor of SCM it focuses on the SCM protocol side only. Pipeline manager can be improved in follow-up patches...",
      "HDDS",
      "Closed",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13170075,
      "add existing docker-compose files to the ozone release artifact",
      "Currently we use docker-compose files to run ozone pseudo cluster locally. After a full build, they can be found under hadoop-dist/target/compose.\n\nAs they are very useful, I propose to make them part of the ozone release to make it easier to try out ozone locally. \n\nI propose to create a new folder (docker/) in the ozone.tar.gz which contains all the docker-compose subdirectories + some basic README how they could be used.\n\nWe should explain in the README that the docker-compose files are not for production just for local experiments.",
      "HDDS",
      "Resolved",
      4,
      7,
      4052,
      "newbie"
    ],
    [
      13232697,
      "Use /etc/ozone for configuration inside docker-compose ",
      "As [~eyang] reported the docker-compose clusters write the config files with uid=1000. In case of the build is created with different user (eg id=401) the hadoop user inside the container (id=100) can't work to the ozone/etc/hadoop directory.\n\nI propose to generate the configuration file to /etc/hadoop (And add that directory to the classpath). In that case the volume mount of the ozone distribution folder can be read only.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13221705,
      "Adjust default values of pipline recovery for more resilient service restart",
      "As of now we have a following algorithm to handle node failures:\n\n1. In case of a missing node the leader of the pipline or the scm can detected the missing heartbeats.\n2. SCM will start to close the pipeline (CLOSING state) and try to close the containers with the remaining nodes in the pipeline\n3. After 5 minutes the pipeline will be destroyed (CLOSED) and a new pipeline can be created from the healthy nodes (one node can be part only one pipwline in the same time).\n\nWhile this algorithm can work well with a big cluster it doesn't provide very good usability on small clusters:\n\nUse case1:\n\nGiven 3 nodes, in case of a service restart, if the restart takes more than 90s, the pipline will be moved to the CLOSING state. For the next 5 minutes (ozone.scm.pipeline.destroy.timeout) the container will remain in the CLOSING state. As there are no more nodes and we can't assign the same node to two different pipeline, the cluster will be unavailable for 5 minutes.\n\nUse case2:\n\nGiven 90 nodes and 30 pipelines where all the pipelines are spread across 3 racks. Let's stop one rack. As all the pipelines are affected, all the pipelines will be moved to the CLOSING state. We have no free nodes, therefore we need to wait for 5 minutes to write any data to the cluster.\n\nThese problems can be solved in multiple ways:\n\n1.) Instead of waiting 5 minutes, destroy the pipeline when all the containers are reported to be closed. (Most of the time it's enough, but some container report can be missing)\n2.) Support multi-raft and open a pipeline as soon as we have enough nodes (even if the nodes already have a CLOSING pipelines).\n\nBoth the options require more work on the pipeline management side. For 0.4.0 we can adjust the following parameters to get better user experience:\n\n{code}\n  <property>\n    <name>ozone.scm.pipeline.destroy.timeout</name>\n    <value>60s</value>\n    <tag>OZONE, SCM, PIPELINE</tag>\n    <description>\n      Once a pipeline is closed, SCM should wait for the above configured time\n      before destroying a pipeline.\n    </description>\n\n  <property>\n    <name>ozone.scm.stale.node.interval</name>\n    <value>90s</value>\n    <tag>OZONE, MANAGEMENT</tag>\n    <description>\n      The interval for stale node flagging. Please\n      see ozone.scm.heartbeat.thread.interval before changing this value.\n    </description>\n  </property>\n {code}\n\nFirst of all, we can be more optimistic and mark node to stale only after 5 mins instead of 90s. 5 mins should be enough most of the time to recover the nodes.\n\nSecond: we can decrease the time of ozone.scm.pipeline.destroy.timeout. Ideally the close command is sent by the scm to the datanode with a HB. Between two HB we have enough time to close all the containers via ratis. With the next HB, datanode can report the successful datanode. (If the containers can be closed the scm can manage the QUASI_CLOSED containers)\n\nWe need to wait 29 seconds (worst case) for the next HB, and 29+30 seconds for the confirmation. --> 66 seconds seems to be a safe choice (assuming that 6 seconds is enough to process the report about the successful closing)",
      "HDDS",
      "Resolved",
      2,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13262135,
      "Publish normalized Ratis metrics via the prometheus endpoint",
      "Latest Ratis contains very good metrics about the status of the ratis ring.\n\nAfter RATIS-702 it will be possible to adjust the repoter of the Dropwizard based ratis metrics and export them directly to the /prom http endpoint (used by ozone insight and ratis).\n\nUnfortunately Dropwizard is very simple, there is no tag support. All of the instance specific strings are part of the metric name. For example:\n{code:java}\n\"ratis_grpc.log_appender.72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67@group\"\n + \"-72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67\"\n + \".grpc_log_appender_follower_75fa730a-59f0-4547\"\n + \"-bd68-216162c263eb_latency\", {code}\nIn this patch I will use a simple method: during the export of the dropwizard metrics based on the well known format of the ratis metrics, they are converted to proper prometheus metrics where the instance information is included as tags:\n{code:java}\nratis_grpc.log_appender.grpc_log_appender_follower_latency{instance=\"72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67\"}\n {code}\nWith this approach we can:\n\n\u00a01. monitor easily all the Ratis pipelines with one simple query\n\n\u00a02. Use the metrics for ozone insight which will show health state of the Ratis pipeline",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13378799,
      "Add SSL support to the Ozone streaming API",
      "HDDS-5142 will introduce a new streaming API for closed container replication / snapshot download and other data movement.\n\nFor server2server communication we need to support mTLS. We should configure pure mTLS on the netty server ",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13279555,
      "Apache NiFi PutFile processor is failing with secure Ozone S3G",
      "\u00a0\n\n(1) Create a simple PutS3Object processor in NiFi\n\n(2) The request from NiFi to S3g will fail with HTTP 500\n\n(3) The exception in the s3g log:\n\n\u00a0\n{code:java}\n s3g_1       | Caused by: java.io.IOException: Couldn't create RpcClient protocol\ns3g_1       | \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:197)\ns3g_1       | \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:173)\ns3g_1       | \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:74)\ns3g_1       | \tat org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:114)\ns3g_1       | \tat org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)\ns3g_1       | \tat jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)\ns3g_1       | \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\ns3g_1       | \tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\ns3g_1       | \tat org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)\ns3g_1       | \t... 92 more\ns3g_1       | Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid S3 identifier:OzoneToken owner=testuser/scm@EXAMPLE.COM, renewer=, realUser=, issueDate=0, maxDate=0, sequenceNumber=0, masterKeyId=0, strToSign=AWS4-HMAC-SHA256\ns3g_1       | 20200115T101329Z\ns3g_1       | 20200115/us-east-1/s3/aws4_request\ns3g_1       | (hash), signature=(sign), awsAccessKeyId=testuser/scm@EXAMPLE.COM{code}\n\u00a0",
      "HDDS",
      "Resolved",
      2,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13221693,
      "Fix the dynamic documentation of basic s3 client usage",
      "S3 gateway has a default web page to display a generic message if you open the endpoint in the browser:\n\nhttp://localhost:9878/static/\n\nIt also contains a simple example to use the endpoint:\n\n{code}\nThis is an endpoint of Apache Hadoop Ozone S3 gateway. Use it with any AWS S3 compatible tool with setting this url as an endpoint\n\nFor example with aws-cli:\n\naws s3api --endpoint http://localhost:9878/static/ create-bucket --bucket=wordcount\n\nFor more information, please check the documentation. \n{code}\n\nUnfortunately the endpoint is wrong here, the static should be removed from the url.\n\nThe trivial fix is to move the ) in the js code>  \n\n",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13223732,
      "OzoneFileSystem can't work with spark/hadoop2.7 because incompatible security classes",
      "The current ozonefs compatibility layer is broken by: HDDS-1299.\n\nThe spark jobs (including hadoop 2.7) can't be executed any more:\n\n{code}\n2019-03-25 09:50:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProviderTokenIssuer\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:348)\n        at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)\n        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)\n        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n        at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)\n        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:757)\n        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:724)\n        at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:45)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)\n        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)\n        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\n        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 43 more\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13237491,
      "Csi server fails because transitive Netty dependencies",
      "CSI server can't be started because an ClassNotFound exception.\n\nIt turned out that with using the new configuration api we got old netty jar files as transitive dependencies. (hdds-configuration depends on hadoop-common, hadoop-commons depends on the word)\n\nWe should exclude all the old netty version from the classpath of the CSI server.",
      "HDDS",
      "Resolved",
      1,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13247649,
      "Remove anti-affinity rules from k8s minkube example",
      "HDDS-1646 introduced real persistence for k8s example deployment files which means that we need anti-affinity scheduling rules: Even if we use statefulset instead of daemonset we would like to start one datanode per real nodes.\n\nWith minikube we have only one node therefore the scheduling rule should be removed to enable at least 3 datanodes on the same physical nodes.\n\nHow to test:\n\n{code}\n mvn clean install -DskipTests -f pom.ozone.xml\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/kubernetes/examples/minikube\nminikube start\nkubectl apply -f .\nkc get pod\n{code}\n\nYou should see 3 datanode instances.\n",
      "HDDS",
      "Resolved",
      1,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13338449,
      "TestContainerMetrics is flaky",
      "TestContainerMetrics is flaky since HDDS-4359. Failed in following master builds:\n\n{code}\n2020/10/26/3569/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/27/3581/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/28/3591/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/29/3619/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/30/3628/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/30/3642/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/31/3650/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n2020/10/31/3654/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\n{code}\n\nSome of the added assertions couldn't be guaranteed all the time:\n\n{code}\n      // ReadTime and WriteTime vary from run to run, only checking non-zero\n      Assert.assertNotEquals(0L, getLongCounter(\"ReadTime\", volumeIOMetrics));\n      Assert.assertNotEquals(0L, getLongCounter(\"WriteTime\", volumeIOMetrics));\n{code}\n\nIn very lucky case the read/write time can be zero.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13215917,
      "Remove default dependencies from hadoop-ozone project",
      "There are two ways to define common dependencies with maven:\n\n  1.) put all the dependencies to the parent project and inherit them\n  2.) get all the dependencies via transitive dependencies\n\nTLDR; I would like to switch from 1 to 2 in hadoop-ozone\n\nMy main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.\n\nUsing transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them. ",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13322745,
      "Improve performance of the BufferPool management of Ozone client",
      "Teragen reported to be slow with low number of mappers compared to HDFS.\n\nIn my test (one pipeline, 3 yarn nodes) 10 g teragen with HDFS was ~3 mins but with Ozone it was 6 mins. It could be fixed with using more mappers, but when I investigated the execution I found a few problems reagrding to the BufferPool management.\n\n 1. IncrementalChunkBuffer is slow and it might not be required as BufferPool itself is incremental\n 2. For each write operation the bufferPool.allocateBufferIfNeeded is called which can be a slow operation (positions should be calculated).\n 3. There is no explicit support for write(byte) operations\n\nIn the flamegraph it's clearly visible that with low number of mappers the client is busy with buffer operations. After the patch the rpc call and the checksum calculation give the majority of the time. ",
      "HDDS",
      "Resolved",
      1,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13249939,
      "Support copy during S3 multipart upload part creation",
      "Uploads a part by copying data from an existing object as data source\n\nDocumented here:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html",
      "HDDS",
      "Resolved",
      1,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13204046,
      "Create isolated classloder to use ozonefs with any older hadoop versions",
      "As of now we create a shaded ozonefs artifact which includes all the required class files to use ozonefs (Hadoop compatible file system for Ozone)\n\nBut the shading process of this artifact is very easy, it includes all the class files but no relocation rules (package name renaming) are configured. With this approach ozonefs can be used from the compatible hadoop version (this is hadoop 3.1 only, I guess) but can't be used with any older hadoop version as it requires the newer version of hadoop-common.\n\nI tried to configure a full shading (with relocation) but it's not a simple task. For example a pure (non-relocated) Configuration is required by the ozonefs itself, but an other, newer Configuration class is required by the ozone client code which is a dependency of OzoneFileSystem So we need a relocated and a non-relocated class in the same time.\n\nI tried out a different approach: I moved out all of the ozone specific classes from the OzoneFileSystem to an adapter class (OzoneClientAdapter). In case of an older hadoop version the adapter class itself can be loaded with an isolated classloader. The isolated classloader can load all the required classes from the jar file from a specific path. It doesn't require any specific package relocation as the default class loader doesn't load these classes. \n\nThe OzoneFileSystem (in case of older hadoop version) can load the adapter with the isolated classloader and only a few classes should be shared between the normal and isolated classloader (the interface of the adapter and the types in the method signatures). All of the other ozone classes and the newer hadoop dependencies will be hidden by the isolated classloader.\n\nThis patch is more like a proof of concept, I would like to start a discussion about this approach. I successfully used the generated artifact to use ozonefs from spark 2.4 default distribution (which includes hadoop 2.7). \n\nFor a final patch I would add some check to use the ozonefs without any classpath separation by default. (could be configured or chosen by automatically)\n\n\nFor using spark (+ hadoop 2.7 + kubernetes scheduler) together with ozone, you can check this screencast: https://www.youtube.com/watch?v=cpRJcSHIEdM&t=8s\n",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13241308,
      "pv-test example to test csi is not working",
      "[~rmaruthiyodan] reported two problems regarding to the pv-test example in csi examples folder.\n\npv-test folder contains an example nginx deployment which can use an ozone PVC/PV to publish content of a folder via http.\n\nTwo problems are identified:\n * The label based matching filter of service doesn't point to the nginx deployment\n * The configmap mounting is missing from nginx deployment",
      "HDDS",
      "Resolved",
      1,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13295515,
      "Use EventQueue for delayed/immediate safe mode rule notification",
      "SCM is built from loosely coupled components which communicate with async event with each other.\n\nUsing the same abstraction (EventQueue) has the benefit that we can use the same visibility / testing tools such as the 'ozone insight' definition (which makes visible all the messages) or the test handler (which can wait until all the event queue messages are processed) \n\nDuring the review of HDDS-3221 it was suggested (by me) to use the EventQueue instead of the new SafeModeNotification interface. \n\nThere was only one counter argument against it:\n\nbq. I personally find the event queue logic hard to follow due to its async nature (you cannot just follow method calls in the IDE). Its not bad, but more difficult when you don't yet understand it, while registering some instances to be notified is easy to follow in an IDE. This is of course a subjective opinion :)\n\nI respect this opinion, but I think it's better to use one abstraction and a consistent architecture inside one component (together with all the existing limitations). The EventQueue is not the only one possible solution, but an existing one. We can either design and switch to a new one or use the existing one.\n\nIn this patch I would like to show how the previous listener interface can be replaced by the EventQueue.\n\nIt (hopefully) shows that this is not complex, and in fact can help us to decouple different component from each other    ",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13363054,
      "Rename Apache Hadoop Ozone to Apache Ozone in pom and markdown files",
      "Please see: https://github.com/apache/ozone/pull/2005",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13343305,
      "Create freon test to measure closed container replication",
      "Create new freon test for container download",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13363282,
      "Provide testkrb5 image for faster ozonesecure tests",
      "Please see: https://github.com/apache/ozone-docker-testkrb5/pull/1",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13226202,
      "ConcurrentModificationException in TestMiniChaosOzoneCluster",
      "TestMiniChaosOzoneCluster is failing with the below exception\n{noformat}\n[ERROR] org.apache.hadoop.ozone.TestMiniChaosOzoneCluster  Time elapsed: 265.679 s  <<< ERROR!\njava.util.ConcurrentModificationException\n\tat java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)\n\tat java.util.ArrayList$Itr.next(ArrayList.java:859)\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl.stop(MiniOzoneClusterImpl.java:350)\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:325)\n\tat org.apache.hadoop.ozone.MiniOzoneChaosCluster.shutdown(MiniOzoneChaosCluster.java:130)\n\tat org.apache.hadoop.ozone.TestMiniChaosOzoneCluster.shutdown(TestMiniChaosOzoneCluster.java:92)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n{noformat}",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "ozone-flaky-test, pull-request-available"
    ],
    [
      13319704,
      "Update documentation for the GA release",
      "HDDS-3413 is opened to add OM HA related documentation to the Ozone docs but it turned out that it contains additional out-of-date (and missing) information.\n\nThis issue is opened to track a big documentation update.",
      "HDDS",
      "Resolved",
      1,
      3,
      4052,
      "pull-request-available"
    ],
    [
      13390689,
      "Multi-raft style placement with permutations for offline data generator",
      "Please see: https://github.com/apache/ozone/pull/2434",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13351790,
      "Support scanning content of DN rocksdb instances with current scheme.",
      "Please see: https://github.com/apache/ozone/pull/1786",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13368671,
      "Refactor Pipeline to use ReplicationConfig instead of factor/type",
      "HDDS-5011 introduces Java ReplicationConfig classes which can be used as a replacement of replicationType and replicationFactor.\n\nFirst task is replacing type/factor with ReplicationConfig in Pipeline and related managers (PipelineManager BackgroundPipelineCreatorV2, PipelineStateManager...)\n\nWe can do it on the master without the EC related stuff... (later we will add the small part which is required for EC",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13370344,
      "Bump Guava version",
      "Please see: https://github.com/apache/ozone/pull/2131",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13260331,
      "KeyDeletingService throws NPE if it's started too early",
      "1. OzoneManager starts KeyManager\n\n2. KeyManager starts KeyDeletingService\n\n3. KeyDeletingService uses OzoneManager.isLeader()\n\n4. OzoneManager.isLeader() uses omRatisServer\n\n5. omRatisServer can be null (bumm)\n\n\u00a0\n\nNow the initialization order in OzoneManager:\n\n\u00a0\n\nnew KeymanagerServer() *Includes start()!!!!*\n\nomRatisServer initialization\n\nstart() (includes KeyManager.start())\n\n\u00a0\n\nThe solution seems to be easy: start the key manager only from the OzoneManager.start() and not from the OzoneManager.instantiateServices()",
      "HDDS",
      "Resolved",
      3,
      3,
      4052,
      "pull-request-available"
    ],
    [
      13280277,
      "Remove default dependencies from hadoop-hdds/pom.xml",
      "There are two ways to add certain set of dependencies to all the maven projects.\n # You can add it to the parent project which will be inherited to all the children projects\n # You can add it only to the required project and will be used via transitive dependencies\n\nI think the 2nd approach is safest as we might need to create a new child project *without* Hadoop dependencies which is not possible with the 1st approach.",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13254192,
      "Make StorageContainerDatanodeProtocolService message based",
      "We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).\n\nStorageContainerDatanodeProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13216977,
      "Add optional web server to the Ozone freon test tool",
      "Recently we improved the default HttpServer to support prometheus monitoring and java profiling.\n\nIt would be very useful to enable the same options for freon testing:\n\n\u00a01. We need a simple way to profile freon and check the problems\n\n\u00a02. Long running freons should be monitored\n\nWe can create a new optional FreonHttpServer which includes all the required servlets by default.",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13288225,
      "Depend on lightweight ConfigurationSource interface instead of Hadoop Configuration",
      "To make it possible to create different client jars compiled with different version of Hadoop we need clear and Hadoop independent hdds-common (and hdds-client) projects.\n\n(For more details about the motivation, check this design doc: https://lists.apache.org/thread.html/rd0ea00f958368e888db1947eb71e514fb977df0b7baaad928ac50e94%40%3Cozone-dev.hadoop.apache.org%3E)\n\nOur current blocker is the usage of `org.apache.hadoop.conf.Configuration`. Configuration class is a heavyweight object from hadoop-common which introduce a lot of unnecessary dependencies. It also violates multiple [OOP principles|https://en.wikipedia.org/wiki/SOLID], for example the *Dependency inversion principle*.\n\nTo make our components more independent I propose to depend on a lightweight ConfigurationSource interface which includes all the required getXXX methods. OzoneConfiguration can implement that interface (and with older Hadoop we can create direct adapters).\n\n",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13250037,
      "S3 MPU part-list call fails if there are no parts",
      "If an S3 multipart upload is created but no part is upload the part list can't be called because it throws HTTP 500:\n\nCreate an MPU:\n\n{code}\naws s3api --endpoint http://localhost:9999 create-multipart-upload --bucket=docker --key=testkeu                                         \n{\n    \"Bucket\": \"docker\",\n    \"Key\": \"testkeu\",\n    \"UploadId\": \"85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234\"\n}\n{code}\n\nList the parts:\n\n{code}\naws s3api --endpoint http://localhost:9999 list-parts  --bucket=docker --key=testkeu --upload-id=85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234\n{code}\n\nIt throws an exception on the server side, because in the KeyManagerImpl.listParts the  ReplicationType is retrieved from the first part:\n\n{code}\n        HddsProtos.ReplicationType replicationType =\n            partKeyInfoMap.firstEntry().getValue().getPartKeyInfo().getType();\n{code}\n\nWhich is not yet available in this use case.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13237155,
      "Make the version of the used hadoop-runner configurable",
      "During an offline discussion with [~arp] and [~eyang] we agreed that it could be more safe to fix the tag of the used hadoop-runner images during the releases.\n\nIt also requires fix tags from hadoop-runner, but after that it's possible to use the fixed tags.\n\nThis patch makes it possible to define the required version/tag in pom.xml\n\n 1. the default hadoop-runner.version is added to all .env files  during the build\n 2. If a variable is added to the .env, it can be used from docker-compose files AND can be overridden by environment variables (it makes it possible to define custom version during a local run) ",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13271173,
      "TestTableCacheImpl is flaky",
      "Run(master): [https://github.com/apache/hadoop-ozone/runs/324342299]\n\n\u00a0\n{code:java}\n-------------------------------------------------------------------------------\nTest set: org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl\n-------------------------------------------------------------------------------\nTests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.955 s <<< FAILURE! - in org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl\ntestPartialTableCacheWithOverrideAndDelete[0](org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl)  Time elapsed: 0.039 s  <<< FAILURE!\njava.lang.AssertionError: expected:<2> but was:<6>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl.testPartialTableCacheWithOverrideAndDelete(TestTableCacheImpl.java:308)\n\n {code}\n*How to reproduce it locally?*\n\nReplace the last tableCache.evict call of testPartialTableCacheWithOverrideAndDelete to System.out.println(tableCache.size()).\n\nYou will see that the cache size is 2 even before the cleanup therefore the next GeneriTestUtils.waitFor is useless (it doesn't guarantee that the cleanup is finished).\n\n*Fix:*\n\nI propose to call the cleanup sync with using the Impl class instead of the interface. It simplifies the test but still validates the behavior.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13230702,
      "Provide k8s resources files for prometheus and performance tests",
      "Similar to HDDS-1412 we can further improve the available k8s resources with providing example resources to:\n\n1) install prometheus\n2) execute freon test and check the results.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13286422,
      "Support running full Ratis pipeline from IDE (IntelliJ) ",
      "HDDS-1522 introduced a method to run full cluster in IntelliJ. The runner configurations can be copied with a shell script and a basic ozone-site.xml and log configuration to make it easy to run ozone from IDE.\n\nUnfortunately this setup supports only one Datanode and it's harder to debug full Ozone pipeline (3 datanodes) from IDE.\n\nThis patch provides 3 different configuration for 3 datanodes with different ports to make it possible to run them on the same host from the IDE.",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13240844,
      "Smoketest results are generated with an internal user",
      "[~eyang] reported the problem in HDDS-1609 that the smoketest results are generated a user (the user inside the docker container) which can be different from the host user.\n\nThere is a minimal risk that the test results can be deleted/corrupted by an other users if the current user is different from uid=1000\n\nI opened this issue because [~eyang] said me during an offline discussion that HDDS-1609 is a more complex issue and not only about the ownership of the test results.\n\nI suggest to handle the two problems in different way. With this patch, the permission of the test result files can be fixed easily.\n\nIn HDDS-1609 we can discuss about general security problems and try to find generic solution for them.\n\nSteps to reproduce _this_ problem:\n # Use a user which is different from uid=1000\n # Create a new ozone build (mvn clean install -f pom.ozone.xml -DskipTests)\n # Go to a compose directory (cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/)\n # Execute tests (./test.sh)\n # check the ownership of the results (ls -lah ./results)\n\nCurrent result: the owner of the result files are the user uid=1000\n\nExpected result: the owner of the files should be always the current user (even if the current uid is different)\n\n\u00a0",
      "HDDS",
      "Resolved",
      4,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13215973,
      "Fix findbugs/checkstyle/accepteance errors in Ozone",
      "Unfortunately as the previous two big commits (error handling HDDS-1068, checkstyle HDDS-1103) are committed in the same time a few new errors are introduced during the rebase.\n\nThis patch will fix the remaining 5 issues (+ a type in the acceptance test executor) ",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13240605,
      "TestScmSafeNode is flaky",
      "org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode is failed at last night with the following error:\n{code:java}\njava.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode(TestScmSafeMode.java:285) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}\nLocally it can be tested but it's very easy to reproduce by adding an additional sleep DataNodeSafeModeRule:\n{code:java}\n+++ b/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/DataNodeSafeModeRule.java\n@@ -63,7 +63,11 @@ protected boolean validate() {\n\u00a0\n\u00a0\u00a0 @Override\n\u00a0\u00a0 protected void process(NodeRegistrationContainerReport reportsProto) {\n-\n+\u00a0\u00a0\u00a0 try {\n+\u00a0\u00a0\u00a0\u00a0\u00a0 Thread.sleep(3000);\n+\u00a0\u00a0\u00a0 } catch (InterruptedException e) {\n+\u00a0\u00a0\u00a0\u00a0\u00a0 e.printStackTrace();\n+\u00a0\u00a0\u00a0 }{code}\nThis is a clear race condition:\n\nDatanodeSafeModeRule and ContainerSafeModeRule are processing the same events but it can be possible (in case of an accidental sleep) that the container safe mode rule is done, but DatanodeSafeModeRule didn't process the new event (yet).\n\nAs a result the test execution will continue:\n{code:java}\nGenericTestUtils\n    .waitFor(() -> scm.getCurrentContainerThreshold() == 1.0, 100, 20000);\n{code}\n(This line is waiting ONLY for the ContainerSafeModeRule).\n\nThe fix is easy, let's wait for the processing of all the async events:\n{code:java}\nEventQueue eventQueue =\n    (EventQueue) cluster.getStorageContainerManager().getEventQueue();\neventQueue.processAll(5000L);{code}\nAs we are sure that the events are already sent to the EventQueue (because we have the previous waitFor), it should be enough.",
      "HDDS",
      "Resolved",
      2,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13269847,
      "Provide command to wait until SCM is out from the safe-mode",
      "The safe mode can be checked with \"ozone scmcli safemode status\". But for acceptance tests there is no easy way to check if the cluster is ready to execute the tests (See HDDS-2606 for example).\n\nOne easy solution is to create a polling version from \"safemode status\".\n\n\"safemode wait --timeout ...\" can be blocked until the scm is out from the safe mode.\n\nWit proper safe mode rules (min datanodes + min pipline numbers) it can help us to check if the acceptance tests are ready to test.\n\nSame command can be used in k8s as well to test if the cluster is ready to start the freon commands...",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13239198,
      "Auditparser robot test shold use a world writable working directory",
      "When I tried to reproduce a problem which is reported by [~eyang], I found that the auditparser robot test uses the /opt/hadoop directory as a working directory to generate the audit.db export.\n\n/opt/hadoop is may or may not be writable, it's better to use /tmp instead.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13311991,
      "Schedule daily 2 builds from master branch build",
      "Mukul suggested to schedule cron based build to have more frequent data points to identify flaky tests.\n\nWe can start with two additional daily build which can be independent from the commit frequency (today we build master only after the commits).",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "build"
    ],
    [
      13229902,
      "Fix content and format of Ozone documentation",
      "During the review of HDDS-1457 I realized that the current documentation contains many outdated information regarding the usage of docker, build commands or s3 usage.\n\nThe security information is also rendered in an incorrect way.\n\nThe png files for the prometheus page are missing (were included in the patch of HDDS-846 but missing from the commit).",
      "HDDS",
      "Resolved",
      1,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13375816,
      "Use ReplicationConfig in OmKeyArgs",
      "During the implementation of HDDS-5145 I realized that OmKeyArgs also uses factor/type, it seems to be easier to convert it to replicationConfig as it's an in-memory class not a protobuf which is required to be persisted.\n\nHaving a half-baked patch planning to upload it soon.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13287203,
      "Fix TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel",
      "TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel is disabled due to intermittent issues. It should be fixed / rewritten or deleted.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "TriagePending"
    ],
    [
      13239295,
      "TestEventWatcher.testMetrics is flaky",
      "TestEventWatcher is intermittent. (Failed twice out of 44 executions).\n\nError is:\n\n{code}\nTests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.764 s <<< FAILURE! - in org.apache.hadoop.hdds.server.events.TestEventWatcher\ntestMetrics(org.apache.hadoop.hdds.server.events.TestEventWatcher)  Time elapsed: 2.384 s  <<< FAILURE!\njava.lang.AssertionError: expected:<2> but was:<3>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.hadoop.hdds.server.events.TestEventWatcher.testMetrics(TestEventWatcher.java:197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n{code}\n\nIn the test we do the following:\n\n 1. fire start-event1\n 2. fire start-event2\n 3. fire start-event3\n 4. fire end-event1\n 5. wait\n\nUsually the event2 and event3 are timed out and event1 is completed but in case of an accidental time between 3 and 4 (in fact between 1 and 4) the event1 also can be timed out.\n\nI improved the unit test and fixed the metrics calculation (completed message should be incremented only if it's not yet timed out).",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13329886,
      "the icon of hadoop-ozone is bigger than ever",
      "It could be a by-product of the introduction of the issue\uff1a\u00a0https://issues.apache.org/jira/browse/HDDS-4166",
      "HDDS",
      "Resolved",
      5,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13366713,
      "Introduce EC ReplicationConfig and Java based ReplicationConfig implementation",
      "SCM proto file should be extended to use ECReplicationConfig which can be de-serialized to a specific ReplicationConfiguration.\n\nNote: this is the bare minimum version of HDDS-4882 which doesn't include the rafactor of the existing proto/persistent fields but de-/serialize them to the new java pojos.\n ",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13285828,
      "README is missing from the source release tar",
      "When we do a dist build with -Psrc the README.md of the root project is not packaged to the tar file which makes it impossible to do a build from the source package as the README.md is required by the dist script.",
      "HDDS",
      "Resolved",
      1,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13291071,
      "Disable index and filter block cache for RocksDB",
      "During preformance tests It was noticed that the OM performance is dropped after 10-20 million of keys. (see the screenshot).\n\nBy default cache_index_and_filter_blocks is enabled for all of our RocksDB instances (see DBProfile) which is not the best option. (For example see this thread: https://github.com/facebook/rocksdb/issues/3961#)\n\nWith turning on this cache the indexes and bloom filters are cached **inside the block cache** which makes slower the cache when we have significant data.\n\nWithout turning it on (based on my understanding) all the indexes will remain open without any cache. With our current settings we have only a few number of sst files (even with million of keys) therefore it seems to be safe to turn this option off.\n\nWith turning this option of I was able to write >100M keys with high throughput. ",
      "HDDS",
      "Resolved",
      4,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13263050,
      "Provide new Freon test to test Ratis pipeline with pure XceiverClientRatis",
      "[~xyao] suggested during an offline talk to implement one additional Freon test to test the ratis part only.\n\nIt can use XceiverClientManager which creates a pure XceiverClientRatis. The client can be used to generate chunks as the datanode accepts any container id / block id.\n\nWith this approach we can stress-test one selected ratis pipeline without having full end2end overhead of the key creation (OM, SCM, etc.)",
      "HDDS",
      "Resolved",
      3,
      2,
      4052,
      "pull-request-available"
    ],
    [
      13324671,
      "Increase default timeout in kubernetes tests",
      "Kubernetes tests are timing out sometimes. (eg. here: https://github.com/elek/ozone-build-results/tree/master/2020/08/26/2562/kubernetes)\n\nBased on the log, SCM couldn't move out from safe mode. It's either a real issue or github environment is slow sometimes.\n\nTo make it clear what is the problem I propose to increase the default timeout from 90 sec to 300 sec (5 min).",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13219584,
      "Remove TestContainerSQLCli unit test stub",
      "In HDDS-447 we removed the support the 'ozone noz' cli tool which was a rocksdb/leveldb to sql exporter.\n\nBut still we have the unit test for the tool (in fact only the skeleton of the unit test, as the main logic is removed). Even worse this unit test is failing as it calls System.exit:\n\n{code}\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-tools: There are test failures.\n[ERROR] \n[ERROR] Please refer to /testptch/hadoop/hadoop-ozone/tools/target/surefire-reports for the individual test results.\n[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.\n[ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\n{code}\n\nI think this test can be deleted.",
      "HDDS",
      "Resolved",
      3,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13258303,
      "Some RPC metrics are missing from SCM prometheus endpoint",
      "In Hadoop metrics it's possible to register multiple metrics with the same name but with different tags. For example each RpcServere has an own metrics instance in SCM.\n\n{code}\n    \"name\" : \"Hadoop:service=StorageContainerManager,name=RpcActivityForPort9860\",\n    \"name\" : \"Hadoop:service=StorageContainerManager,name=RpcActivityForPort9863\",\n{code}\n\nThey are converted by PrometheusSink to a prometheus metric line with proper name and tags. For example:\n\n{code}\nrpc_rpc_queue_time60s_num_ops{port=\"9860\",servername=\"StorageContainerLocationProtocolService\",context=\"rpc\",hostname=\"72736061cbc5\"} 0\n{code}\n\nThe PrometheusSink uses a Map to cache all the recent values but unfortunately the key contains only the name (rpc_rpc_queue_time60s_num_ops in our example) but not the tags (port=...)\n\nFor this reason if there are multiple metrics with the same name, only the first one will be displayed.\n\nAs a result in SCM only the metrics of the first RPC server can be exported to the prometheus endpoint. \n",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13226144,
      "Make the ozonesecure-mr environment definition version independent",
      "The MapReduce example project on branch ozone-0.4 contains 0.5.0-SNAPSHOT references in the dir:\n\nhadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozonesecure-mr\n\nAfter HDDS-1333 (which introduce filtering) it will be straightforward to always use the current version.",
      "HDDS",
      "Resolved",
      3,
      1,
      4052,
      "pull-request-available"
    ],
    [
      13312158,
      "Hadoop3 artifact should depend on the ozonefs-shaded",
      "ozonefs-hadoop3 is an all-in-one ozonefs client which can be used as a single jar file.\n\nUnfortunately it uses wrong dependency (ozonefs-common instead of ozonefs-common-shaded) which means that it downloads additional dependencies (netty-all, ...) if it's used from maven.",
      "HDDS",
      "Resolved",
      1,
      7,
      4052,
      "pull-request-available"
    ],
    [
      13181326,
      "Remove dependencies between hdds/ozone and hdfs proto files",
      "It would be great to make the hdds/ozone proto files independent from hdfs proto files. It would help as to start ozone with multiple version of hadoop version.\n\nAlso helps to make artifacts from the hdds protos:  HDDS-220\n\n Currently we have a few unused \"hdfs.proto\" import in the proto files and we use the StorageTypeProto from hdfs:\n\n{code}\ncd hadoop-hdds\ngrep -r \"hdfs\" --include=\"*.proto\"\ncommon/src/main/proto/ScmBlockLocationProtocol.proto:import \"hdfs.proto\";\ncommon/src/main/proto/StorageContainerLocationProtocol.proto:import \"hdfs.proto\";\n\n cd ../hadoop-ozone\ngrep -r \"hdfs\" --include=\"*.proto\"\ncommon/src/main/proto/OzoneManagerProtocol.proto:import \"hdfs.proto\";\ncommon/src/main/proto/OzoneManagerProtocol.proto:    required hadoop.hdfs.StorageTypeProto storageType = 5 [default = DISK];\ncommon/src/main/proto/OzoneManagerProtocol.proto:    optional hadoop.hdfs.StorageTypeProto storageType = 6;\n{code}\n\nI propose to \n\n1.) remove the hdfs import statements from the proto files\n2.) Copy the StorageTypeProto and create a Hdds version from it (without PROVIDED)",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "newbie"
    ],
    [
      13194160,
      "Removing REST protocol support from OzoneClient",
      "Since we have functional {{S3Gateway}} for Ozone which works on REST protocol, having REST protocol support in OzoneClient feels redundant and it will take a lot of effort to maintain it up to date.\nAs S3Gateway is in a functional state now, I propose to remove REST protocol support from OzoneClient.\n\nOnce we remove REST support from OzoneClient, the following will be the interface to access Ozone cluster\n * OzoneClient (RPC Protocol)\n * OzoneFS (RPC Protocol)\n * S3Gateway (REST Protocol)",
      "HDDS",
      "Resolved",
      3,
      4,
      4052,
      "pull-request-available"
    ],
    [
      13267254,
      "Implement MiniOzoneHAClusterImpl#getOMLeader",
      "Implement MiniOzoneHAClusterImpl#getOMLeader and use it.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13439248,
      "[Multi-Tenant] Clean up unused tenantDefaultPolicyName field in CreateTenantRequest protobuf message",
      "There is this one place that hasn't been covered in the previous refactoring patch HDDS-6396.\n\n{code:title=Current OmClientProtocol.proto on branh HDDS-4944}\nmessage CreateTenantRequest {\n    optional string tenantId = 1;  // Tenant name\n    optional string tenantDefaultPolicyName = 2;  // TODO: REMOVE\n    optional string volumeName = 3;\n}\n{code}\n\nThis {{tenantDefaultPolicyName}} field is no longer in-use. Remove it.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13424105,
      "[Multi-Tenant] Fix KMS Encryption/Decryption",
      "We need to pass the correct user principal from the client to KMS to get the correct DEK. Currently in multi-tenancy, accessId is passed rather than the actual user principal.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13564239,
      "[hsync] Adopt RATIS-1994 to reduce hsync latency",
      "RATIS-1994 proposes a new Ratis AsyncApi. It could potentially reduce hsync latency.",
      "HDDS",
      "Patch Available",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13589003,
      "[hsync] Revert config default ozone.fs.hsync.enabled to false",
      "ozone.fs.hsync.enabled was initially added in HDDS-8302 with default value of false.\n\nBut the default was later flipped to true in HDDS-10252. We need to:\n\n1. Switch the default back to false\n2. Ensure this doesn't break any existing tests, etc.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13359493,
      "Update NodeStatus OperationalState for Datanodes in Recon",
      "Possibly due to Recon ignoring {{setNodeOperationalStateCommand}} (HDDS-4766), {{NodeStatus}} isn't being updated for its {{operationalState}} and {{opStateExpiryEpochSeconds}} fields (but {{DatanodeInfo}}'s {{persistedOpState}} and {{persistedOpStateExpiryEpochSec}} are correct).\n\nSee the attached screenshot.\n\nFound this during development of HDDS-4832.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13273899,
      "Maven property skipShade should not skip ozonefs compilation",
      "Currently, if {{-DskipShade}} is specified when running {{mvn}}, it will skip {{ozonefs}} module ({{hadoop-ozone-filesystem}} / Apache Hadoop Ozone FileSystem) compilation:\n{code:xml|title=hadoop-ozone/pom.xml}\n    <profile>\n      <id>build-with-ozonefs</id>\n      <activation>\n        <property>\n          <name>!skipShade</name>\n        </property>\n      </activation>\n      <modules>\n        <module>ozonefs</module>\n        <module>ozonefs-lib-current</module>\n        <module>ozonefs-lib-legacy</module>\n      </modules>\n    </profile>\n{code}\n\nAs result of this, when I make code change under {{./hadoop-ozone/ozonefs/}} then run {{mvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true -DskipShade}}, the change won't be reflected in the dist. Property {{skipShade}} should not be expected to do this.\n\nWe should compile {{ozonefs}} regardless of {{-DskipShade}}.",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13277761,
      "Implement ofs://: mkdir",
      "A sub-task in HDDS-2665 to lay the foundation and make mkdir work in the new filesystem.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13286813,
      "OzoneFileStatus#getModificationTime should return actual directory modification time when its OmKeyInfo is available",
      "As of current implementation, [{{getModificationTime()}}|https://github.com/apache/hadoop-ozone/blob/c9f26ccf9f93a052c5c0c042c57b6f87709597ae/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java#L90-L107] always returns \"fake\" modification time (current time) for directory due to the reason that a directory in Ozone might be faked from a file key.\n\nBut, there are cases where real directory key exists in OzoneBucket. For example when user calls {{fs.mkdirs(directory)}}. In this case, a reasonable thing to do would be getting the modification time from the OmInfoKey, rather than faking it.\n\nCC [~xyao]\n\n\nMy POC for the fix:\n{code:java|title=Diff}\ndiff --git a/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java b/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java\nindex 8717946512..708e62d692 100644\n--- a/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java\n+++ b/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java\n@@ -93,7 +93,7 @@ public FileStatus makeQualified(URI defaultUri, Path parent,\n    */\n   @Override\n   public long getModificationTime(){\n-    if (isDirectory()) {\n+    if (isDirectory() && super.getModificationTime() == 0) {\n       return System.currentTimeMillis();\n     } else {\n       return super.getModificationTime();\ndiff --git a/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java\nindex 1be5fb3f3c..cb8f647a41 100644\n--- a/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java\n+++ b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java\n@@ -2004,8 +2004,14 @@ public OmKeyInfo lookupFile(OmKeyArgs args, String clientAddress)\n               } else {\n                 // if entry is a directory\n                 if (!deletedKeySet.contains(entryInDb)) {\n-                  cacheKeyMap.put(entryInDb,\n-                      new OzoneFileStatus(immediateChild));\n+                  if (!entryKeyName.equals(immediateChild)) {\n+                    cacheKeyMap.put(entryInDb,\n+                        new OzoneFileStatus(immediateChild));\n+                  } else {\n+                    // If entryKeyName matches dir name, we have the info\n+                    cacheKeyMap.put(entryInDb,\n+                        new OzoneFileStatus(value, 0, true));\n+                  }\n                   countEntries++;\n                 }\n                 // skip the other descendants of this child directory.\n{code}",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13248446,
      "Ozone fs shell command should work with default port when port number is not specified",
      "{code:bash|title=Without port number -> Error}\n$ ozone fs -ls o3fs://bucket.volume.localhost/\n-ls: Ozone file system url should be either one of the two forms: o3fs://bucket.volume/key  OR o3fs://bucket.volume.om-host.example.com:5678/key\n...\n{code}\n{code:bash|title=With port number -> Success}\n$ ozone fs -ls o3fs://bucket.volume.localhost:9862/\nFound 1 items\n-rw-rw-rw-   1 hadoop hadoop       1485 2019-08-01 21:14 o3fs://bucket.volume.localhost:9862/README.txt\n{code}\n\nWe expect the first command to attempt port 9862 by default.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13502748,
      "[Snapshot] Fix SnapshotInfo#dbTxSequenceNumber (de)serialization",
      "It turns out in HDDS-7281 I forgot to add the serialization and deserialization logic for dbTxSequenceNumber in SnapshotInfo. As a result, dbTxSequenceNumber is always zero when deserialized. This is a simple fix to address that issue.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13265038,
      "Speed up TestOzoneManagerHA#testOMRetryProxy and #testTwoOMNodesDown",
      "Marton's comment:\nhttps://github.com/apache/hadoop-ozone/pull/30#pullrequestreview-302465440\n\nOut of curiosity, I ran entire TestOzoneManagerHA locally. The entire test class finished in 10m 30s. I discovered {{testOMRetryProxy}} and {{testTwoOMNodesDown}} are taking the most time (2m and 2m 30s respectively) to finish. Most time are wasted on retry and wait. We could reasonably reduce the amount of time on the wait.\n\nAs I tested, with the patch, {{testOMRetryProxy}} and {{testTwoOMNodesDown}} finish in 20 sec each, saving almost 4 min runtime on those two tests alone. The whole TestOzoneManagerHA test finishes in 5m 44s with the patch.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13401441,
      "[Multi-Tenant] GetS3Secret should retrieve secret from new tables as well",
      "Update existing GetS3Secret to support new multi-tenant tables as secrets generated by AssignUserToTenantRequest are stored in these new tables (i.e. TenantAccessIdTable).",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13449127,
      "[Snapshot] Implement Snapshot Delete CLI and API",
      "Snapshot delete through a Ratis transaction.\n\nThe Ratis tx moves the snapshot from ACTIVE state to DELETED.\n\nThis does not remove the\u00a0snapshot's RocksDB checkpoint directory. That would be done by a new background service\u00a0{{SnapshotDeletingTask}}, which would also move the snapshot to RECLAIMED state and eventually remove the snapshotInfo from the table and remove the snapshot DB checkpoint.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13449111,
      "Bump kotlin-stdlib to 1.6.21 due to CVE-2022-24329",
      "[CVE-2022-24329|https://nvd.nist.gov/vuln/detail/CVE-2022-24329]\n\n1.6.21 seems to be the latest stable version as of now: https://mvnrepository.com/artifact/org.jetbrains.kotlin/kotlin-stdlib",
      "HDDS",
      "Resolved",
      3,
      3,
      9462,
      "pull-request-available"
    ],
    [
      13392201,
      "[OFS] URI parser throws URISyntaxException when path contains space",
      "In docker-compose ozone:\n\n{code:bash}\nbash-4.2$ ozone fs -put \"compose/common/grafana/dashboards/Ozone - Object Metrics.json\" ofs://om/vol1/bucket2/dir3/\n-put: Fatal internal error\njava.lang.RuntimeException: java.net.URISyntaxException: Illegal character in path at index 51: user/hadoop/compose/common/grafana/dashboards/Ozone - Object Metrics.json\n\tat org.apache.hadoop.ozone.OFSPath.<init>(OFSPath.java:79)\n\tat org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileStatus(BasicRootedOzoneClientAdapterImpl.java:546)\n\tat org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileStatus(BasicRootedOzoneFileSystem.java:785)\n\tat org.apache.hadoop.fs.shell.PathData.lookupStat(PathData.java:173)\n\tat org.apache.hadoop.fs.shell.PathData.<init>(PathData.java:105)\n\tat org.apache.hadoop.fs.shell.PathData.<init>(PathData.java:82)\n\tat org.apache.hadoop.fs.shell.CopyCommands$Put.expandArgument(CopyCommands.java:287)\n\tat org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233)\n\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105)\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:177)\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:327)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.fs.ozone.OzoneFsShell.main(OzoneFsShell.java:81)\nCaused by: java.net.URISyntaxException: Illegal character in path at index 51: user/hadoop/compose/common/grafana/dashboards/Ozone - Object Metrics.json\n\tat java.base/java.net.URI$Parser.fail(URI.java:2913)\n\tat java.base/java.net.URI$Parser.checkChars(URI.java:3084)\n\tat java.base/java.net.URI$Parser.parseHierarchical(URI.java:3166)\n\tat java.base/java.net.URI$Parser.parse(URI.java:3125)\n\tat java.base/java.net.URI.<init>(URI.java:600)\n\tat org.apache.hadoop.ozone.OFSPath.<init>(OFSPath.java:77)\n\t... 13 more\n{code}\n\nLooks like a parsing bug in OFSPath.\n\nEasier repro:\n\n{code}\n$ ozone fs -ls \"ofs://ozone1/vo1/bucket2/ \"\n-ls: Fatal internal error\njava.lang.RuntimeException: java.net.URISyntaxException: Illegal character in path at index 12: vo1/bucket2/\n\tat org.apache.hadoop.ozone.OFSPath.<init>(OFSPath.java:79)\n\tat org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileStatus(BasicRootedOzoneClientAdapterImpl.java:538)\n...\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13563756,
      "Add hsync metadata to hsync'ed keys in OpenKeyTable as well",
      "Currently, only those keys in KeyTable/FileTable would have metadata {{HSYNC_CLIENT_ID}} when those keys have been hsync'ed (and not closed yet). The problem with this is that it makes {{getExpiredOpenKeys()}} and {{listOpenKeys}} (HDDS-8830) very *inefficient* by forcing them to look up KeyTable/FileTable while they could have just used OpenKeyTable/OpenFileTable solely to determine whether an open key is hsync'ed or not.\n\nProposal:\n1. during an hsync(), persist metadata {{HSYNC_CLIENT_ID}} to {{OmKeyInfo}} in {{OpenKeyTable}} as well (in addition to {{KeyTable}}). Only write when the client ID changes so it doesn't cause write amplifications. Ideally only the first hsync() of a key would cause a write to {{OpenKeyTable}}.\n2. during key close/commit, remove {{HSYNC_CLIENT_ID}} from {{OpenKeyTable}} if necessary so that {{HSYNC_CLIENT_ID}} isn't written to the final key.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13527056,
      "Enforce new checkstyle: NewlineAtEndOfFile",
      "It is brought up by [~hemantkumar.dindi] a while back that we generally want a new line at the end of a file: https://github.com/apache/ozone/pull/4125#discussion_r1087167605\n\nand it also makes sense in terms of diff output, from [doc|https://checkstyle.sourceforge.io/apidocs/com/puppycrawl/tools/checkstyle/checks/NewlineAtEndOfFileCheck.html]:\n\nbq. Rationale: Any source files and text files in general should end with a line separator to let other easily add new content at the end of file and \"diff\" command does not show previous lines as changed.\n\n\nBack in HDDS-2119, [~nanda] actually [added|https://github.com/apache/hadoop/commit/e78848fc3cb113733ea640f0aa3abbb271b16005#diff-bbd8da0f280e38951da50da904cf93b9915743c40f3424ccc24a9745f4c733c7R60] the NewlineAtEndOfFile check but it is commented out to this day for some reason. Though I didn't find any objections in the [PR|https://github.com/apache/hadoop/pull/1435].\n\nIn order to add the check we just need to uncomment that line, and fix whatever existing files that don't have a new line at EOF, similar to what I did in HDDS-6208.",
      "HDDS",
      "Resolved",
      3,
      5,
      9462,
      "pull-request-available"
    ],
    [
      13254015,
      "Add tests for incorrect OM HA config when node ID or RPC address is not configured",
      "-OM will NPE and crash when `ozone.om.service.ids=id1,id2` is configured but `ozone.om.nodes.id1` doesn't exist; or `ozone.om.address.id1.omX` doesn't exist.-\n\n-Root cause:-\n-`OzoneManager#loadOMHAConfigs()` didn't check the case where `found == 0`. This happens when local OM doesn't match any `ozone.om.address.idX.omX` in the config.-\n\nDue to the refactoring done in HDDS-2162. This fix has been included in that commit. I will repurpose the jira to add some tests for the HA config.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13352346,
      "Add permission check in OMDBCheckpointServlet",
      "TBA",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13405087,
      "`ozone sh volume/bucket/key list` should print valid JSON array",
      "Right now the output is just a bunch of separate JSONs (not a valid one as a whole).\n\nThis jira simply fixes it by grouping them into one JSON array. Should make it easier to be parsed by {{jq}} and similar utilies.\n\nCC [~erose]",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13431374,
      "[Multi-Tenant] Merge and cleanup tenant group/role/policy tables, refactor protobuf messages and `isTenantAdmin`",
      "Goal: Clean up unnecessary tables before merging the feature branch to master.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13368407,
      "Use getShortUserName in getTrashRoot(s)",
      "Inspired by HDDS-5019, we should use getShortUserName() instead of getUserName() as Kerberos principal can differ on different nodes while we trust admin will map those Kerberos principal to the same short user names.\n\nhttps://github.com/apache/ozone/blob/4128813ab495dfd3941c0252e61e41ca7d1cf1ce/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OFSPath.java#L301\n\nhttps://github.com/apache/ozone/blob/8585fba44a6ffe22fa2c65cc651acec6b6872e5e/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java#L581\n\ncc [~xyao]",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13312024,
      "OzoneManager#listVolumeByUser ignores userName parameter when ACL is enabled",
      "When {{ozone.acl.enabled}} is set to {{true}}, the [ACL check logic in OzoneManager#listVolumeByUser|https://github.com/apache/hadoop-ozone/blob/aa04ac0a894e15c98b05b1acef110c6e26bb01dc/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L1845-L1857] ignored the provided {{userName}}.\n\nThis bug is introduced by my commit HDDS-3056, unfortunately.\n\nh3. Impact\ne.g. {{userA}} won't be able to use {{ozone sh volume list --user userB}} to list {{userB}}'s volumes when ACL is enabled.\n\nh3. Solution\nUse {{userName}} rather than {{ProtobufRpcEngine.Server.getRemoteUser()}} for ACL check.",
      "HDDS",
      "Resolved",
      2,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13359584,
      "Improve Ozone admin shell decommission/recommission/maintenance commands user experience",
      "1. Currently, entering `ozone admin datanode decommission` command alone doesn't give any feedback.\n\nw/ patch:\n{code}\nbash-4.2$ ozone admin datanode decommission\nIncomplete command\nUsage: ozone admin datanode decommission [-hV] [--scm=<scm>] [<hosts>...]\nDecommission a datanode\n      [<hosts>...]   List of fully qualified host names\n  -h, --help         Show this help message and exit.\n      --scm=<scm>    The destination scm (host:port)\n  -V, --version      Print version information and exit.\n{code}\n\n2. When decommission command is executed successfully, it lacks feedback on the client (it does log on the server side though).\n\nw/ patch:\n{code}\nbash-4.2$ ozone admin datanode decommission 172.18.0.7 172.18.0.2\nStarted decommissioning datanodes:\n172.18.0.7\n172.18.0.2\n{code}\n\n3. Improve decommission failure message due to host/port resolution.\n\nw/ patch:\n{code}\nbash-4.2$ ozone admin datanode decommission 172.18.0.71\nHost 172.18.0.71 (172.18.0.71) is not running any datanodes registered with SCM. Please check the host name.\n\nbash-4.2$ ozone admin datanode decommission 172.18.0.7:9999\nHost 172.18.0.7:9999 is running a datanode registered with SCM, but the port number doesn't match. Please check the port number.\n{code}\n\n\n\nSame for recommission and maintenance commands.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13540290,
      "Remove redundant checkAcls() when caller is volume owner during key or prefix access",
      "It is unnecessary to call checkAcls() twice when caller is volume owner in {{OzoneAclUtils#checkAllAcls}}.\n\nBecause the reason we had to split that into two calls in HDDS-5903 is because Ranger only has one OWNER tag, and that we want OWNER tag on bucket/key level policies to be filled in with the *bucket* owner during ACL check if the caller is NOT the volume owner.\n\nIn the case where the caller is *volume* owner, this hierarchical check is already enforced by the authorizer (OzoneNativeAuthorizer or RangerOzoneAuthorizer) internally. Thus it is unnecessary.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13430080,
      "Relax protolock rule to allow changing field names",
      "h2. Motivation\n\nThe motivation behind this is that currently in the master branch we use {{kerberosID}} as the protobuf message field name while it really means {{accessId}}. For example here: https://github.com/apache/ozone/blob/master/hadoop-ozone/interface-client/src/main/proto/OmClientProtocol.proto#L1323\n\nIt just so happens to be the case that before Multi-Tenancy (HDDS-4944) is implemented, kerberosID is equivalent to accessId in the context of Ozone S3 access. But with Multi-Tenancy feature this is no longer the case -- accessId does not equal to kerberosID any more.\n\nAnd it could be confusing for other developers to work on S3 / Multi-Tenancy related stuff in the future. e.g. those {{getKerberosID}} calls really means {{getAccessId}}, and {{setKerberosID}} really is {{setAccessId}}. Filed HDDS-6339.\n\n\nh2. Status Quo\n\nRight now {{proto-backwards-compatibility}} uses default [protolock|https://github.com/nilslice/protolock#usage] command line argument, effectively enforcing strict mode for the [rules|https://github.com/nilslice/protolock#rules-enforced], which doesn't allow changing existing protobuf field names:\n\n{code:xml|title=https://github.com/apache/ozone/blob/68c5ac5df4fbc0edd7114394112fa696a3dc9229/pom.xml#L1619-L1633}\n          <groupId>com.salesforce.servicelibs</groupId>\n          <artifactId>proto-backwards-compatibility</artifactId>\n          <version>${proto-backwards-compatibility.version}</version>\n          <configuration>\n            <protoSourceRoot>${basedir}/target/classes</protoSourceRoot>\n          </configuration>\n{code}\n\nChanging the field name itself does not break wire compatiblity (field type and field number are still the same), unless that protobuf message is decoded into JSON at some point (which could use the field name as key). Ref: https://stackoverflow.com/a/45431953\n\n\nh2. Proposal\n\nAdd {{<options>--strict false</options>}} in {{pom.xml}} so that strict mode is off, which as a side affect also turns off two other rules currently enforced according to [protolock readme|https://github.com/nilslice/protolock#rules-enforced]. I'm not sure if protolock provides a way for more granular control of which exact rule to turn off.\n\n{code}\nNo Removing Reserved Fields\nCompares the current vs. updated Protolock definitions and will return a list of warnings if any reserved field has been removed.\n\nNote: This rule is not enforced when strict mode is disabled.\n{code}\n{code}\nNo Changing Field Names\nCompares the current vs. updated Protolock definitions and will return a list of warnings if any message's previous fields have been renamed.\n\nNote: This rule is not enforced when strict mode is disabled.\n{code}\n{code}\nNo Removing RPCs\nCompares the current vs. updated Protolock definitions and will return a list of warnings if any RPCs provided by a Service have been removed.\n\nNote: This rule is not enforced when strict mode is disabled.\n{code}\n",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13293855,
      "Rebase OFS branch",
      "Merge commits on master branch to OFS dev branch: {{git merge master}}\n\nAlso need to manually apply a couple of changes in master branch to OFS classes:\n- HDDS-3049\n- HDDS-2914 (HDDS-2188)\n- HDDS-3065",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13561963,
      "Deleted file reappears after HSync",
      "Scenario:\nOpen multiple FSO files in Ozone, write data and do Hsync. In middle Remove some file with -skipTrash.\n\nBefore deleting File_98.txt and File_99.txt\n{code:java}\n[root@ccycloud-1 ~]# ozone fs -du -s -h ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/* | grep File_9\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_9.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_90.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_91.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_92.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_93.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_94.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_95.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_96.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_97.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt\n[root@ccycloud-1 ~]# {code}\nRemoving both 98 and 99th file\n{code:java}\n[root@ccycloud-1 ~]# ozone fs -rm -r -skipTrash ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt\nDeleted ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt\n[root@ccycloud-1 ~]# ozone fs -rm -r -skipTrash ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt\nDeleted ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt\n[root@ccycloud-1 ~]# {code}\nFile gets removed:\n{code:java}\n[root@ccycloud-1 ~]# ozone fs -du -s -h ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/* | grep File_9\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_9.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_90.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_91.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_92.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_93.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_94.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_95.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_96.txt\n766 K \u00a02.2 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_97.txt\n[root@ccycloud-1 ~]#\n[root@ccycloud-1 ~]# {code}\nAfter the next Hsync call in test, Both file reappears with the exact length as if they were not deleted previously.\n{code:java}\n[root@ccycloud-1 ~]# ozone fs -du -s -h ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/* | grep File_9\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_9.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_90.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_91.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_92.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_93.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_94.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_95.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_96.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_97.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt\n771 K \u00a02.3 M \u00a0ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt {code}",
      "HDDS",
      "Resolved",
      2,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13304455,
      "Implement ofs://: Override getTrashRoot",
      "[~pifta] found if we delete file with Hadoop shell, namely {{hadoop fs -rm}}, without {{-skipTrash}} option, the operation would fail in OFS due to the client is renaming the file to {{/user/<username>/.Trash/}} because renaming across different buckets is not allowed in Ozone. (Unless the file happens to be under {{/user/<username>/}}, apparently.)\n\nWe could override {{getTrashRoot()}} in {{BasicOzoneFileSystem}} to a dir under the same bucket to mitigate the problem. Thanks [~umamaheswararao] for the suggestion.\n\nThis raises one more problem though: We need to implement trash clean up on OM. Opened HDDS-3575 for this.\n\nCC [~arp] [~bharat]",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "Triaged, pull-request-available"
    ],
    [
      13428656,
      "Upgrade acceptance test doesn't collect logs when the test fails",
      "Upgrade acceptance test is not collecting logs when the test fails:\n\nhttps://github.com/smengcl/hadoop-ozone/runs/5205586631\nhttps://github.com/smengcl/hadoop-ozone/runs/5207373946\n\nThe log bundle is supposed to have these logs regardless of whether the upgrade test suite fails or not:\n\n{code}\ndocker-1.1.0-downgraded.log\ndocker-1.1.0.log\ndocker-1.2.0-finalized.log\ndocker-1.2.0-pre-finalized.log\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13517074,
      "[Snapshot] Delete keys in snapshot scope from deleteTable during createSnapshot to accommodate snapshot garbage collection",
      "During snapshot creation, use deleteRange to remove the keys in the snapshot scope (bucket) from deletedTable so that they aren't pointlessly scanned by KeyDeletingService since optimization around KeyDeletingService scan of the keys \"trapped\" inside snapshots aren't yet in place.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13272307,
      "Implement new Ozone Filesystem scheme ofs://",
      "Implement a new scheme for Ozone Filesystem where all volumes (and buckets) can be accessed from a single root.\n\nAlso known as Rooted Ozone Filesystem.",
      "HDDS",
      "Resolved",
      1,
      2,
      9462,
      "Triaged, pull-request-available"
    ],
    [
      13458622,
      "[Multi-Tenant] Move Ranger plugin version to a separate tag",
      "Goal: Move Ranger plugin version to a separate tag. Before this it was hard-coded.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13322214,
      "Tests in TestOzoneFileSystem should use the existing MiniOzoneCluster",
      "In HDDS-2833, [~adoroszlai] made a change that greatly reduces the run time of the test suite {{TestOzoneFileSystem}} by sharing one {{MiniOzoneCluster}} among the tests.\n\nBut 4 new tests have been added since and are not sharing that {{MiniOzoneCluster}}.\n\nI am able to cut down the run time of {{TestOzoneFileSystem}} from 3m18s to 1m2s on my Mac. It would only save more run time on GitHub Workflow.",
      "HDDS",
      "Resolved",
      5,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13358924,
      "Show Datanode OperationalState (IN_SERVICE/DECOMMISSION/MAINTENANCE) in Recon",
      "There are 5 NodeOperationalState s defined at the moment:\n\n{code}\n  /**\n   * Protobuf enum {@code hadoop.hdds.NodeOperationalState}\n   */\n  public enum NodeOperationalState\n      implements com.google.protobuf.ProtocolMessageEnum {\n    /**\n     * <code>IN_SERVICE = 1;</code>\n     */\n    IN_SERVICE(0, 1),\n    /**\n     * <code>DECOMMISSIONING = 2;</code>\n     */\n    DECOMMISSIONING(1, 2),\n    /**\n     * <code>DECOMMISSIONED = 3;</code>\n     */\n    DECOMMISSIONED(2, 3),\n    /**\n     * <code>ENTERING_MAINTENANCE = 4;</code>\n     */\n    ENTERING_MAINTENANCE(3, 4),\n    /**\n     * <code>IN_MAINTENANCE = 5;</code>\n     */\n    IN_MAINTENANCE(4, 5),\n    ;\n{code}\n\nWe should show the Datanode OperationalState in Recon Datanode page as well as it provides valuable information. See the attached screenshots for the result.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13450659,
      "[Multi-Tenant] Use RangerClient for Ranger operations",
      "HDDS-5836 is merged. But we have yet to switch the actual logic to RangerClient.\n\n1. Use {{RangerClientMultiTenantAccessController}} instead of {{RangerRestMultiTenantAccessController}}.\n2. Get rid of {{MultiTenantAccessAuthorizer}} and {{MultiTenantAccessAuthorizerRangerPlugin}} -- use {{MultiTenantAccessController}} instead.\n3. -work around RangerClient's missing getServiceVersion() API- Use {{rangerClient.getService(serviceName).getPolicyVersion()}} to implement {{RangerClientMultiTenantAccessController#getRangerServiceVersion()}}\n\n{{RangerClient}} allows the use of Kerberos principal and ticket as login credential (preferred than username and password).",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13337718,
      "Datanode can go OOM when a Recon or SCM Server is very slow in processing reports.",
      "From [~nanda619]'s analysis.\n\nContainerReportPublisher thread runs periodically (default interval 60s) in Datanode and adds ContainerReport to StateContext (Queue).\nHeartbeat thread runs periodically (default interval 30s), picks up the ContainerReport (if any) from StateContext.\nFor short time, the ContainerReport will be held in Datanode StateContext.\nFor Recon, a change was made in datanode such that the ContainerReport will be cached in Datanode StateContext separately for each endpoint (i.e. SCM and Recon). As I see, if Recon is configured in the Datanode and all the reports that are to be sent to Recon will be pending in the StateContextQueue (LinkedList)",
      "HDDS",
      "Resolved",
      1,
      3,
      9462,
      "pull-request-available"
    ],
    [
      13427506,
      "Upgrade acceptance test log flooded with parse error",
      "These two lines are repeatedly printed during upgrade acceptance test:\n\n{code:title=https://github.com/apache/ozone/runs/5122095480?check_suite_focus=true#step:5:1354}\nPython-dotenv could not parse statement starting at line 27\nPython-dotenv could not parse statement starting at line 28\n{code}",
      "HDDS",
      "Resolved",
      4,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13487040,
      "Remove Ozone 0.5.0-beta download link from the website",
      "Remove Ozone 0.5.0-beta download link from the site, per discussion in the dev mailing list: https://lists.apache.org/thread/r3cdjk924o62yd9n5781vg7lqdgyd6gr",
      "HDDS",
      "Resolved",
      3,
      3,
      9462,
      "pull-request-available"
    ],
    [
      13589385,
      "[hsync] Add a config as HBase-related features master switch",
      "Simliar to what JVM does with {{-XX:+UnlockExperimentalVMOptions}}, we are adding a config to Ozone (client AND server) to lock hbase-related Ozone features (hsync, incremental chunklist, putBlock piggybacking, lease recovery) and enhancements behind an experimental flag.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13324571,
      "Add OFS to FileSystem META-INF",
      "So that {{fs.ofs.impl}} won't have to be explicitly set in core-site.xml.\n\nDerived from HDDS-3805.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13359495,
      "Add line break when node has no pipelines for `ozone admin datanode list` command",
      "The line {{No related pipelines or the node is not in Healthy state.}} should have a new line after it.\n\nJust a trivial formatting issue.\n\nBefore:\n{code}\nbash-4.2$ ozone admin datanode list\nDatanode: e56040c8-5cfa-4558-966b-0851ccf5c6c5 (/default-rack/172.22.0.2/ozone_datanode_1.ozone_default/2 pipelines)\nOperational State: IN_SERVICE\nRelated pipelines:\nece97f05-fbf4-49db-9959-58c49c479f9b/ONE/RATIS/OPEN/Leader\n62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Follower\n\nDatanode: f702a7cc-d887-4095-8f1a-7826669a5ddd (/default-rack/172.22.0.7/ozone_datanode_4.ozone_default/0 pipelines)\nOperational State: DECOMMISSIONED\nRelated pipelines:\nNo related pipelines or the node is not in Healthy state.\nDatanode: 1baca70e-69f5-48e0-ae31-7f6d8ed2fafc (/default-rack/172.22.0.3/ozone_datanode_2.ozone_default/2 pipelines)\nOperational State: IN_SERVICE\nRelated pipelines:\n62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Follower\n8a387971-315d-4fd2-b2a3-c993cd36e8bb/ONE/RATIS/OPEN/Leader\n\nDatanode: 30815665-dcda-40e4-bce6-000a46ab6d3d (/default-rack/172.22.0.6/ozone_datanode_3.ozone_default/2 pipelines)\nOperational State: IN_SERVICE\nRelated pipelines:\n62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Leader\na7ff21f5-6d91-4452-9532-0c8789b1d435/ONE/RATIS/OPEN/Leader\n{code}\n\nAfter:\n\n{code}\nbash-4.2$ ozone admin datanode list\nDatanode: 8f572444-6134-4740-845f-12a8f454fad0 (/default-rack/172.22.0.2/ozone_datanode_4.ozone_default/0 pipelines)\nOperational State: DECOMMISSIONING\nRelated pipelines:\nNo related pipelines or the node is not in Healthy state.\n\nDatanode: ae9cab4e-d163-4983-b7bb-4d140fbd41b5 (/default-rack/172.22.0.7/ozone_datanode_1.ozone_default/2 pipelines)\nOperational State: IN_SERVICE\nRelated pipelines:\n7e7bd855-3cc4-4e95-b363-38740580915c/ONE/RATIS/OPEN/Leader\n9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower\n\nDatanode: d992655c-aa4f-44ec-8bca-631191a527ef (/default-rack/172.22.0.8/ozone_datanode_3.ozone_default/2 pipelines)\nOperational State: IN_SERVICE\nRelated pipelines:\n9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower\n6fa79344-0791-454f-8c4f-2edb0cb2ea34/ONE/RATIS/OPEN/Leader\n\nDatanode: 327bc5f1-874b-42da-9951-f4cc1571bab9 (/default-rack/172.22.0.9/ozone_datanode_2.ozone_default/2 pipelines)\nOperational State: IN_SERVICE\nRelated pipelines:\na28a6008-b3b8-45c8-a7f4-23ca680b3e46/ONE/RATIS/OPEN/Leader\n9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower\n{code}",
      "HDDS",
      "Resolved",
      5,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13408007,
      "OFS mkdir -p does not work as expected for bucket creation when volume exists due to volume create ACL check",
      "We discovered this problem during the implementation of HttpFS Gateway. I did an acceptance test for the HttpFS with Robot framework. In the ozonesecure docker environment when I tried to make a volume with the testuser principal it didn't work, because it doesn't have permission to do it. So we decided to make a volume with an admin, set the testuser as the owner of it and then create buckets with the testuser. Even after the owner change happened successfully it gave the same error:\nUser testuser/httpfs@EXAMPLE.COM doesn't have CREATE permission to access volume vol01 null null\nAfter debugging we found why this happened. As the bucket is not existing first it goes to the getBucket() method in the\u00a0[BasicRootedOzoneClientAdapterImpl|https://github.com/apache/ozone/blob/master/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java#L234] class.\n\nThe createIfNotExist is true and both in the VOLUME_NOT_FOUND and\u00a0BUCKET_NOT_FOUND cases tries to create volume first, which the testuser does not have permission. So we got the error from there, despite of the fact that testuser is the owner of that volume, so it should be able to create buckets inside.\n\nWe were able to recreate this in terminal in the scm container (in that because it has both testuser and testuser2 as principals).\n{code:java}\nbash-4.2$ klist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: testuser/scm@EXAMPLE.COM\n\nValid starting     Expires            Service principal\n10/18/21 11:23:39  10/19/21 11:23:39  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n        renew until 10/25/21 11:23:39\n{code}\nIn the scm testuser is an admin, testuser/scm@EXAMPLE.COM is added as an ozone administrator in the docker-config. I did the same with testuser/httpfs@EXAMPLE.COM but it is not an admin, as the username is mapped to\u00a0short user principal name (with an auth-to-local rule), which is testuser. Because of this the equality check between testuser and testuser/httpfs@EXAMPLE.COM is false, so it is not taken as an admin user.\n{code:java}\nbash-4.2$ ozone sh volume update --user=testuser2 vol02\n\\{\n  \"metadata\" : { },\n  \"name\" : \"vol02\",\n  \"admin\" : \"testuser\",\n  \"owner\" : \"testuser2\",\n  \"quotaInBytes\" : -1,\n  \"quotaInNamespace\" : -1,\n  \"usedNamespace\" : 0,\n  \"creationTime\" : \"2021-10-18T11:19:59.777Z\",\n  \"modificationTime\" : \"2021-10-18T11:24:04.183Z\",\n  \"acls\" : [ \\{\n    \"type\" : \"USER\",\n    \"name\" : \"testuser\",\n    \"aclScope\" : \"ACCESS\",\n    \"aclList\" : [ \"ALL\" ]\n  } ]\n}\nbash-4.2$ kinit -kt /opt/hadoop/compose/_keytabs/testuser2.keytab testuser2/scm@EXAMPLE.COM\nbash-4.2$ klist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: testuser2/scm@EXAMPLE.COM\n\nValid starting     Expires            Service principal\n10/18/21 11:24:17  10/19/21 11:24:17  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n        renew until 10/25/21 11:24:17\nbash-4.2$ ozone fs -mkdir -p ofs://om/vol01/buck01\n2021-10-18 11:24:47,729 [main] INFO rpc.RpcClient: Creating Volume: vol01, with testuser2 as owner and space quota set to -1 bytes, counts quota set to -1\nmkdir: User testuser2/scm@EXAMPLE.COM doesn't have CREATE permission to access volume vol01 null null\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13283316,
      "Implement ofs://: Fix getFileStatus for mkdir volume",
      "[~xyao] discovered that when running {{ozone fs -mkdir -p ofs://om/vol1/}} (only volume name is given, no bucket name or key path), the command would fail in {{getFileStatus}} (before reaching {{createDirectory()}}) in Hadoop common code:\n\n{code:bash}\nbash-4.2$ ozone fs -mkdir -p ofs://om/vol1/\n-mkdir: Bucket or Volume length is illegal, valid length is 3-63 characters\n\n# Same w/o -p\nbash-4.2$ ozone fs -mkdir ofs://om/vol1/\n-mkdir: Bucket or Volume length is illegal, valid length is 3-63 characters\n{code}\n\nAnd we discovered with debugger attached that the root cause is that {{getFileStatus()}} is not behaving as expected.\n\nSolution: Patch existing OFS code, throw proper exception in {{getBucket()}} code to make Hadoop common happy.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13281073,
      "Implement ofs://: temp directory mount",
      "Because of ofs:// filesystem hierarchy starts with volume then bucket, an application typically won't be able to write directly under a first-level folder, e.g. ofs://service-id1/tmp/. /tmp/ is a special case that we need to handle since that is the default location most legacy Hadoop applications write to for swap/temporary files. In order to address this, we would introduce /tmp/ as a client-side \"mount\" to another Ozone bucket.\n\nNote that the preliminary implementation would only allow for /tmp/ to be a mount but not any user-defined path.\n\nThis depends on HDDS-2840 and HDDS-2928.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13286925,
      "Allow users to list volumes they have access to, and optionally allow all users to list all volumes",
      "Current implementation of {{listVolumes}} only returns the volumes the user creates.\nAnd there's no existing OM public API to return a list of users or return all volumes. Which means we must add new APIs to OM to either return user list or all volumes in order for this feature to work.\n\n-We can open another jira on master branch to add those APIs, get back to this jira and add this function to OFS.-\n-After a discussion with [~arpaga], Sanjay suggested we should allow *all* users to list *all* volumes for now. (Users still won't be able to access volumes which they don't have permission to.)-\nNote: As HDDS-2610 is committed, it enabled clients to perform {{listAllVolumes}}, which allows admin users to list all volumes. This jira just need to make sure non-admin users can list all volumes as well.\n\nIn fact this jira tries to achieve two objectives:\n1. Allow users to list volumes they have access to\n2. Optionally allow all users to list all volumes",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13297603,
      "OMVolumeSetOwnerRequest doesn't check if user is already the owner",
      "OMVolumeSetOwnerRequest doesn't seem to check if the user is already the owner.\nIf the user is already the owner, it shouldn't proceed to the update logic, otherwise the resulting volume list for that user in {{UserVolumeInfo}} would have duplicate volume entry. As demonstrated in the test case.\n\n-It also doesn't seem to remove the volume from the UserVolumeInfo from the previous owner.- Checked [here|https://github.com/apache/hadoop-ozone/blob/80e9f0a7238953e41b06d22f0419f04ab31d4212/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetOwnerRequest.java#L152-L153].\n\n[~bharat]",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13426635,
      "Bump centos to 7.9.2009, dependencies and tools in ozone-runner",
      "This is a first step toward HDDS-6263.\n\narm64 docker base images are available in centos 8 official Docker Hub repo.\n\nThis should not affect any existing x86-64 / amd64 builds and tests.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13249870,
      "Owner/group information for a file should be returned from OzoneFileStatus",
      "BasicOzoneFilesystem returns the file's user/group information as the current user/group. This should default to the information read from the acl's for the file.\n\ncc [~xyao]",
      "HDDS",
      "Resolved",
      2,
      1,
      9462,
      "Triaged"
    ],
    [
      13342255,
      "[OFS] Listing volumes under root should return all volumes whenever possible",
      "Currently {{listStatusRoot()}} only lists volumes owned(ACL disabled) or accessible(ACL enabled) by current user.\n\nThis prevents HttpFS from listing all volumes under OFS root.\n\nSince we probably can't provide an argument to allow passing in parameter {{--all}} due to conforming to HCFS, we should default to list ALL volumes whenever this is possible.\n\nNote: {{ozone.om.volume.listall.allowed}} is an OM side argument that controls whether any user can list all volumes on an Ozone cluster, it defaults to {{true}}.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13442950,
      "[MultiTenancy] User get-secret throws USER_MISMATCH",
      "User get-secret API fails with user mismatch even though its kinited with correct user.\n\n\u00a0\n{code:java}\nbash-4.2$ kinit -kt /etc/security/keytabs/testuser.keytab testuser/om\nbash-4.2$ ozone tenant user get-secret 'tenantone$testuser'\nUSER_MISMATCH Requested accessId 'tenantone$testuser' doesn't match current user 'testuser/om@EXAMPLE.COM', nor does current user has administrator privilege.{code}",
      "HDDS",
      "Resolved",
      2,
      7,
      9462,
      "ozone-multitenancy"
    ],
    [
      13440847,
      "[Multi-Tenant] Follow-up: Set owner of buckets created via S3 Gateway to actual user",
      "This is a follow-up jira to HDDS-6574 in the multi-tenancy branch. See https://github.com/apache/ozone/pull/3298#discussion_r848659291",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13299773,
      "Refactor OFSPath to adapt to master branch",
      "See PR description: https://github.com/apache/hadoop-ozone/pull/847",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13326113,
      "[OFS] Better owner and group display for listing Ozone volumes and buckets",
      "Improve volumes' and buckets' owner and group display when listing in OFS.\n\n1. Display short name instead of full Kerberos principal.\n2. For volumes, get actual group of the owner (currently it is the volume admin name which is incorrect)\n3. For buckets, display the owner and group of its parent volume.",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13589513,
      "FS CLI gives incorrect recursive volume deletion prompt",
      "Symptom:\n\nFrom CLI (with {{compose/ozone}} Docker dev cluster):\n{code}\nbash-4.2$ ozone fs -rm -skipTrash -r ofs://om/vol1/\nrm: Recursive volume delete using ofs is not supported. Instead use 'ozone sh volume delete -r -skipTrash -id <OM_SERVICE_ID> <Volume_URI>' command\n{code}\n\nBut when I follow the prompt, I get \"Unknown options\":\n\n{code}\nbash-4.2$ ozone sh volume delete -r -skipTrash -id om /vol1\nUnknown options: '-skipTrash', '-id', '/vol1'\nUsage: ozone sh volume delete [-hrVy] [-t=<threadNo>] <value>\ndeletes a volume\n      <value>     URI of the volume.\n                  Ozone URI could either be a full URI or short URI.\n                  Full URI should start with o3://, in case of non-HA\n                  clusters it should be followed by the host name and\n                  optionally the port number. In case of HA clusters\n                  the service id should be used. Service id provides a\n                  logical name for multiple hosts and it is defined\n                  in the property ozone.om.service.ids.\n                  Example of a full URI with host name and port number\n                  for a key:\n                  o3://omhostname:9862/vol1/bucket1/key1\n                  With a service id for a volume:\n                  o3://omserviceid/vol1/\n                  Short URI should start from the volume.\n                  Example of a short URI for a bucket:\n                  vol1/bucket1\n                  Any unspecified information will be identified from\n                  the config files.\n\n  -h, --help      Show this help message and exit.\n  -r              Delete volume recursively\n  -t, --thread, --threads=<threadNo>\n                  Number of threads used to execute recursive delete\n  -V, --version   Print version information and exit.\n  -y, --yes       Continue without interactive user confirmation\nbash-4.2$\n{code}\n\nThe correct command line to achieve the intended purpose (recursively delete the volume) should be this:\n\n{code}\nbash-4.2$ ozone sh volume delete -r o3://om/vol1\nThis command will delete volume recursively.\nThere is no recovery option after using this command, and no trash for FSO buckets.\nDelay is expected running this command.\nEnter 'yes' to proceed': yes\nVolume vol1 is deleted\nbash-4.2$\n{code}",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13530490,
      "[Snapshot] Reduce flakiness in testSkipTrackingWithZeroSnapshot",
      "https://github.com/apache/ozone/actions/runs/4540058938/jobs/8001489891?pr=3980",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13298356,
      "Add response to SetVolumePropertyResponse proto",
      "https://github.com/apache/hadoop-ozone/pull/806#discussion_r408279098\n\n1. Add {{optional bool response = 1;}} in the [message|https://github.com/apache/hadoop-ozone/blob/15db251f16236228c6596253dab6946494fc8f5b/hadoop-ozone/common/src/main/proto/OzoneManagerProtocol.proto#L425-L427].\n2. Handle the response on the client. e.g. in setOwner, if the response is false, we can print message: The specified user is already the owner of the volume.\n\nWill start the work after PR #821 is merged.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13393546,
      "Restore adding ozone-site.xml to default resource in OzoneConfiguration#activate",
      "In rare cases, applications will not seemingly trigger OzoneConfiguration#loadDefaults, resulting in only default in ozone-default.xml being loaded (in activate()). We can *steadily repro* the bug with pysparkshell in the below mode:\n\n{code}\npyspark --master yarn --deploy-mode client --name \"PySparkShell\"\n{code}\n\nThe line was removed in HDDS-1469 ([PR|https://github.com/apache/hadoop/pull/773#pullrequestreview-230833296]) with the refactoring as it was seemingly unnecessary to keep it.",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13426526,
      "OM crashes when trying to overwrite a key during upgrade downgrade testing",
      "While working on HDDS-6084 (related to upgrade acceptance testing), [~erose] and I found that if:\n1) a key is created with a new OM version (1.3.0)\n2) downgrade to OM 1.1.0\n3) try to overwrite the key created in (1)\n\nStep (3) will result in all 3 OMs crashing.\n\nThe issue seems to be introduced in the unreleased branch of 1.3.0. The same issue cannot be reproduced in 1.1.0 to 1.2.0 upgrade/downgrade testing (which should exclude HDDS-5243 or HDDS-5393 as a potential root cause). This could indicate some unreleased changes has broken the key versioning after the downgrade. (Could well be an incompatible change. Need further investigation.)\n\n{code}\nom2_1    | 2022-02-03 21:36:15,228 [OM StateMachine ApplyTransaction Thread - 0] ERROR ratis.OzoneManagerStateMachine: Terminating with exit status 1: Request cmdType: CreateKey\nom2_1    | traceID: \"\"\nom2_1    | clientId: \"client-72B024AF247D\"\nom2_1    | userInfo {\nom2_1    |   userName: \"dlfknslnfslf\"\nom2_1    |   remoteAddress: \"10.9.0.19\"\nom2_1    |   hostName: \"ha_s3g_1.ha_net\"\nom2_1    | }\nom2_1    | version: 1\nom2_1    | createKeyRequest {\nom2_1    |   keyArgs {\nom2_1    |     volumeName: \"s3v\"\nom2_1    |     bucketName: \"old1-bucket\"\nom2_1    |     keyName: \"key2\"\nom2_1    |     dataSize: 17539\nom2_1    |     type: RATIS\nom2_1    |     factor: THREE\nom2_1    |     keyLocations {\nom2_1    |       blockID {\nom2_1    |         containerBlockID {\nom2_1    |           containerID: 1\nom2_1    |           localID: 107736214721200128\nom2_1    |         }\nom2_1    |         blockCommitSequenceId: 0\nom2_1    |       }\nom2_1    |       offset: 0\nom2_1    |       length: 268435456\nom2_1    |       createVersion: 0\nom2_1    |       pipeline {\nom2_1    |         members {\nom2_1    |           uuid: \"b92bf4c8-3b0c-40b0-bb2b-05b6d3594e13\"\nom2_1    |           ipAddress: \"10.9.0.16\"\nom2_1    |           hostName: \"ha_dn2_1.ha_net\"\nom2_1    |           ports {\nom2_1    |             name: \"REPLICATION\"\nom2_1    |             value: 9886\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS\"\nom2_1    |             value: 9858\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS_ADMIN\"\nom2_1    |             value: 9857\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS_SERVER\"\nom2_1    |             value: 9856\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"STANDALONE\"\nom2_1    |             value: 9859\nom2_1    |           }\nom2_1    |           networkName: \"b92bf4c8-3b0c-40b0-bb2b-05b6d3594e13\"\nom2_1    |           networkLocation: \"/default-rack\"\nom2_1    |           persistedOpState: IN_SERVICE\nom2_1    |           persistedOpStateExpiry: 0\nom2_1    |           uuid128 {\nom2_1    |             mostSigBits: -5103716611873029968\nom2_1    |             leastSigBits: -4959864281830437357\nom2_1    |           }\nom2_1    |         }\nom2_1    |         members {\nom2_1    |           uuid: \"f0b7e615-d4ee-4ec4-a6b5-ec68b82c07e9\"\nom2_1    |           ipAddress: \"10.9.0.15\"\nom2_1    |           hostName: \"ha_dn1_1.ha_net\"\nom2_1    |           ports {\nom2_1    |             name: \"REPLICATION\"\nom2_1    |             value: 9886\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS\"\nom2_1    |             value: 9858\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS_ADMIN\"\nom2_1    |             value: 9857\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS_SERVER\"\nom2_1    |             value: 9856\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"STANDALONE\"\nom2_1    |             value: 9859\nom2_1    |           }\nom2_1    |           networkName: \"f0b7e615-d4ee-4ec4-a6b5-ec68b82c07e9\"\nom2_1    |           networkLocation: \"/default-rack\"\nom2_1    |           persistedOpState: IN_SERVICE\nom2_1    |           persistedOpStateExpiry: 0\nom2_1    |           uuid128 {\nom2_1    |             mostSigBits: -1101158602427707708\nom2_1    |             leastSigBits: -6433976558118238231\nom2_1    |           }\nom2_1    |         }\nom2_1    |         members {\nom2_1    |           uuid: \"c7912312-811d-469d-8c40-c739cd2a1e62\"\nom2_1    |           ipAddress: \"10.9.0.17\"\nom2_1    |           hostName: \"ha_dn3_1.ha_net\"\nom2_1    |           ports {\nom2_1    |             name: \"REPLICATION\"\nom2_1    |             value: 9886\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS\"\nom2_1    |             value: 9858\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS_ADMIN\"\nom2_1    |             value: 9857\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"RATIS_SERVER\"\nom2_1    |             value: 9856\nom2_1    |           }\nom2_1    |           ports {\nom2_1    |             name: \"STANDALONE\"\nom2_1    |             value: 9859\nom2_1    |           }\nom2_1    |           networkName: \"c7912312-811d-469d-8c40-c739cd2a1e62\"\nom2_1    |           networkLocation: \"/default-rack\"\nom2_1    |           persistedOpState: IN_SERVICE\nom2_1    |           persistedOpStateExpiry: 0\nom2_1    |           uuid128 {\nom2_1    |             mostSigBits: -4066430426156284259\nom2_1    |             leastSigBits: -8340447458821005726\nom2_1    |           }\nom2_1    |         }\nom2_1    |         state: PIPELINE_OPEN\nom2_1    |         type: RATIS\nom2_1    |         factor: THREE\nom2_1    |         id {\nom2_1    |           id: \"c0b6f272-9a39-4dc3-ada8-c3b833cc6e17\"\nom2_1    |           uuid128 {\nom2_1    |             mostSigBits: -4560190998638408253\nom2_1    |             leastSigBits: -5933277313150194153\nom2_1    |           }\nom2_1    |         }\nom2_1    |         leaderID: \"f0b7e615-d4ee-4ec4-a6b5-ec68b82c07e9\"\nom2_1    |         creationTimeStamp: 1643924132125\nom2_1    |         suggestedLeaderID {\nom2_1    |           mostSigBits: -1101158602427707708\nom2_1    |           leastSigBits: -6433976558118238231\nom2_1    |         }\nom2_1    |         leaderID128 {\nom2_1    |           mostSigBits: -1101158602427707708\nom2_1    |           leastSigBits: -6433976558118238231\nom2_1    |         }\nom2_1    |       }\nom2_1    |       partNumber: 0\nom2_1    |     }\nom2_1    |     isMultipartKey: false\nom2_1    |     acls {\nom2_1    |       type: USER\nom2_1    |       name: \"dlfknslnfslf\"\nom2_1    |       rights: \"\\200\"\nom2_1    |       aclScope: ACCESS\nom2_1    |     }\nom2_1    |     modificationTime: 1643924174840\nom2_1    |   }\nom2_1    |   clientID: 107736214722445312\nom2_1    | }\nom2_1    | failed with exception\nom2_1    | java.lang.IllegalArgumentException\nom2_1    | \tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:128)\nom2_1    | \tat org.apache.hadoop.ozone.om.helpers.OmKeyInfo.<init>(OmKeyInfo.java:81)\nom2_1    | \tat org.apache.hadoop.ozone.om.helpers.OmKeyInfo$Builder.build(OmKeyInfo.java:378)\nom2_1    | \tat org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getFromProtobuf(OmKeyInfo.java:460)\nom2_1    | \tat org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.fromPersistedFormat(OmKeyInfoCodec.java:59)\nom2_1    | \tat org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.fromPersistedFormat(OmKeyInfoCodec.java:36)\nom2_1    | \tat org.apache.hadoop.hdds.utils.db.CodecRegistry.asObject(CodecRegistry.java:55)\nom2_1    | \tat org.apache.hadoop.hdds.utils.db.TypedTable.getFromTableIfExist(TypedTable.java:261)\nom2_1    | \tat org.apache.hadoop.hdds.utils.db.TypedTable.getIfExist(TypedTable.java:248)\nom2_1    | \tat org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:236)\nom2_1    | \tat org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)\nom2_1    | \tat org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:415)\nom2_1    | \tat org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:240)\nom2_1    | \tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)\nom2_1    | \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\nom2_1    | \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\nom2_1    | \tat java.base/java.lang.Thread.run(Thread.java:834)\nom2_1    | 2022-02-03 21:36:15,253 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG:\nom2_1    | /************************************************************\nom2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at a250845831a7/10.9.0.12\nom2_1    | ************************************************************/\n{code}\n\nThis is where OM is crashing in ozone 1.1.0 code:\n\nhttps://github.com/apache/ozone/blob/ozone-1.1.0/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyInfo.java#L81-L82",
      "HDDS",
      "Resolved",
      1,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13311491,
      "[OFS] Add User Guide",
      "Need to add a user guide markdown for OFS. Especially the usage for {{/tmp}}.\n\nThanks [~umamaheswararao] and [~xyao] for the reminder.\n\n{{hadoop-hdds/docs/content/design/ofs.md}}",
      "HDDS",
      "Resolved",
      1,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13442951,
      "[MultiTenancy] DBinfo message on console on missing accessId",
      "Assignadmin on an invalid accessId should throw better error message\n{code:java}\nbash-4.2$ ozone tenant user assignadmin none\nFailed to assign admin to 'none': OmDBAccessIdInfo is missing for accessId 'none' in DB.{code}",
      "HDDS",
      "Resolved",
      4,
      7,
      9462,
      "ozone-multitenancy"
    ],
    [
      13423595,
      "New checkstyle: WhitespaceAround",
      "To enforce whitespace check around symbols.\n\nRef: https://checkstyle.sourceforge.io/config_whitespace.html#WhitespaceAround\n\nCould optionally exclude some cases (allowEmptyMethods, allowEmptyConstructors, allowEmptyTypes, allowEmptyLoops, allowEmptyLambdas and allowEmptyCatches).",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13337674,
      "Update the container replica history to the Recon DB lazily instead of for every report.",
      "Recon tracks the history for every container replica on the Ozone cluster in its SQL DB (By default, this is Derby). To track this, it keeps track of the last timestamp of a replica on a DN through reports. This becomes a SQL DB scan + write operation for every container report received.  Even though there is async hand off from the report to EventQueue, the event queue handler itself by default uses 1 thread per event type (report type). Hence, there is implicit blocking behavior here which is pushed down to DNs.\n\nThis has to be changed into a lazy update of DB to support better scalability. Details on how to achieve this will be added to the JIRA later.",
      "HDDS",
      "Resolved",
      2,
      3,
      9462,
      "pull-request-available"
    ],
    [
      13319640,
      "[OFS] BasicRootedOzoneFileSystem to support batchDelete",
      "This Jira is to use deleteObjects in OFS delete now that HDDS-3286 is committed.\n\nCurrently when ozone.om.enable.filesystem.paths is enabled it normalizes the path, so using deleteKey for delete directory will fail.\n\nAccording to [~bharat] this should be a blocker for 0.6.0.\n",
      "HDDS",
      "Resolved",
      1,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13272337,
      "Suppress loader constraint violation message in TestOzoneFileSystemWithMocks",
      "{{TestOzoneFileSystemWithMocks}} throws LinkageError error when run (but test succeeds):\n\n{code}\nERROR StatusLogger Could not reconfigure JMX\n java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/MockClassLoader) previously initiated loading for a different type with name \"javax/management/MBeanServer\"\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n\tat org.powermock.core.classloader.MockClassLoader.loadUnmockedClass(MockClassLoader.java:250)\n\tat org.powermock.core.classloader.MockClassLoader.loadModifiedClass(MockClassLoader.java:194)\n\tat org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:71)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:335)\n\tat org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:259)\n\tat org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:164)\n\tat org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)\n\tat org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:558)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:619)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:636)\n\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:231)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)\n\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)\n\tat org.apache.commons.logging.LogAdapter$Log4jLog.<clinit>(LogAdapter.java:135)\n\tat org.apache.commons.logging.LogAdapter$Log4jAdapter.createLog(LogAdapter.java:102)\n\tat org.apache.commons.logging.LogAdapter.createLog(LogAdapter.java:79)\n\tat org.apache.commons.logging.LogFactoryService.getInstance(LogFactoryService.java:46)\n\tat org.apache.commons.logging.LogFactoryService.getInstance(LogFactoryService.java:41)\n\tat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)\n\tat org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:136)\n\tat org.apache.hadoop.fs.ozone.TestOzoneFileSystemWithMocks.testFSUriWithHostPortOverrides(TestOzoneFileSystemWithMocks.java:73)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:316)\n\tat org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:88)\n\tat org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:96)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:300)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:131)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.access$100(PowerMockJUnit47RunnerDelegateImpl.java:59)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner$TestExecutorStatement.evaluate(PowerMockJUnit47RunnerDelegateImpl.java:147)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.evaluateStatement(PowerMockJUnit47RunnerDelegateImpl.java:107)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:288)\n\tat org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:86)\n\tat org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:49)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:208)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:147)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:121)\n\tat org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:33)\n\tat org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:45)\n\tat org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:123)\n\tat org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:121)\n\tat org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:53)\n\tat org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:160)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)\n\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)\n\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)\n\nProcess finished with exit code 0\n{code}\n\nGoal: Suppress this error message.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13321348,
      "[OFS] Implement AbstractFileSystem for RootedOzoneFileSystem",
      "Extracted from HDDS-3805: introduce an implementation of {{AbstractFileSystem}}, similar to {{OzFs}}, for {{RootedOzoneFileSystem}}.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13588909,
      "[hsync] Block ECKeyOutputStream from calling hsync and hflush",
      "ECKeyOutputStream extends KeyOutputStream, but EC keys does not support hsync() as there are known issues with that: HDDS-8932\n\n(Even if we add EC hsync support in the future, there would be significant overhead with small writes due to striping and parity calc. But that is a different topic.)\n\nThis jira prevents hsync() and hflush() from being called on ECKeyOutputStream by throwing NotImplementedException.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13534881,
      "[Snapshot] Custom SnapshotCache implementation to replace LoadingCache",
      "This is the continuation of HDDS-7935.\n\nIn HDDS-7935 we replaced LoadingCache's {{maximumSize()}} with {{softValues()}}. But there are concerns regarding JVM GC's undeterministic behavior of invalidating the cache entries with the soft references.\n\nIn this jira, we are going to implement a custom SnapshotCache to replace LoadingCache gain full control over snapshot cache entry eviction/invalidation.\n\nPR: https://github.com/apache/ozone/pull/4567",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13567243,
      "Remove unnecessary sleep in TestMiniOzoneCluster",
      "See PR.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13440213,
      "[MultiTenancy] No user validation on assignUser API",
      "No validation of user while running assignUser API under tenant.\n\nNon-existent User\n{code:java}\nbash-4.2$ ozone tenant user assign user -t tenantone\nAssigned 'user' to 'tenantone' with accessId 'tenantone$user'.\nexport AWS_ACCESS_KEY_ID='tenantone$user'\nexport AWS_SECRET_ACCESS_KEY='b58a64f66e6091cd22cdd1666e226c82e8138ba7a86804a3086d108ef6036961'{code}\nInvalid user (tried regex)\n{code:java}\nbash-4.2$ ozone tenant user assign \"*\" -t tenantone\nAssigned '*' to 'tenantone' with accessId 'tenantone$*'.\nexport AWS_ACCESS_KEY_ID='tenantone$*'\nexport AWS_SECRET_ACCESS_KEY='27f9420833b1433774660654a8cc054e76d630e0d5d2ee3d0e3a1c327ecc5ac8'\nbash-4.2$ ozone tenant user assign \"user*\" -t tenantone\nAssigned 'user*' to 'tenantone' with accessId 'tenantone$user*'.\nexport AWS_ACCESS_KEY_ID='tenantone$user*'\nexport AWS_SECRET_ACCESS_KEY='99c4652cc90a4f5b46396432b00c3422f0ba481528cdc968b91ee6cedaa2f649'{code}\n\nUser of length greater than 100\n{code:java}\nbash-4.2$ ozone tenant user assign --tenant=tenantone 'testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas'\nAssigned 'testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas' to 'tenantone' with accessId 'tenantone$testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas'.\nexport AWS_ACCESS_KEY_ID='tenantone$testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas'\nexport AWS_SECRET_ACCESS_KEY='b9e5ad69c39561446b571419dba3e39b0b90936040c63b2a70ba5b94a7fb9f85'\n{code}\n",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "ozone-multitenancy"
    ],
    [
      13442382,
      "Make Hugo markdown image syntax add responsive image class",
      "This should solve the image size issue (some are too large, like in PrefixFSO.md) *when using the Markdown image syntax* (not the shortcode).\n\nSee comment: https://github.com/apache/ozone/pull/2582/files#r861196807",
      "HDDS",
      "Resolved",
      3,
      3,
      9462,
      "pull-request-available"
    ],
    [
      13298358,
      "Rebase OFS branch - 2. Adapt OFS classes to HDDS-3101",
      "HDDS-3353 broke OFSPath since the latter uses {{org.apache.commons.codec.digest.DigestUtils}} but the dependency is removed.\n\nHDDS-3359 broke OFSPath because the latter also uses {{org.apache.yetus.audience}}, but this is a simple fix - just replace it with {{org.apache.hadoop.hdds.annotation}}.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13310429,
      "[OFS] Address merge conflicts after HDDS-3627",
      "HDDS-3627 removed isolated class loader and moved classes around.\n\nI will address the merge conflicts after rebasing OFS feature branch to master branch which includes HDDS-3627 in this jira.",
      "HDDS",
      "Resolved",
      1,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13550952,
      "Instruct admins to use OZONE_MANAGER_CLASSPATH for RangerOzoneAuthorizer",
      "Use {{OZONE_MANAGER_CLASSPATH}} instead of {{OZONE_CLASSPATH}} to narrow down the effective scope of the classpath to OMs only.",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13428978,
      "[Multi-Tenant] Add tenant CLI option to print results in JSON",
      "Applicable to all tenant subcommands:\n{code}\nozone tenant create\nozone tenant user assign\nozone tenant user assignadmin\nozone tenant user info\nozone tenant list\n...\n{code}\n\nAnd this would be especially helpful for TenantListHandler (`ozone tenant list`) which could print a lot of extra info ({{BucketNS, UserRole, AdminRole, BucketNSPolicy, BucketPolicy}})",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13419769,
      "Update log4j version to 2.17.1",
      "Release notes: https://github.com/apache/logging-log4j2/blob/rel/2.17.1/RELEASE-NOTES.md\n\nLooks like another RCE ([CVE-2021-44832|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44832]) in 2.17.0.\n\n{code}\nApache Log4j2 versions 2.0-beta7 through 2.17.0 (excluding security fix releases 2.3.2 and 2.12.4) are vulnerable to a remote code execution (RCE) attack where an attacker with permission to modify the logging configuration file can construct a malicious configuration using a JDBC Appender with a data source referencing a JNDI URI which can execute remote code. This issue is fixed by limiting JNDI data source names to the java protocol in Log4j2 versions 2.17.1, 2.12.4, and 2.3.2.\n{code}\n\nhttps://www.bleepingcomputer.com/news/security/log4j-2171-out-now-fixes-new-remote-code-execution-bug/",
      "HDDS",
      "Resolved",
      3,
      1,
      9462,
      "pull-request-available"
    ],
    [
      13543190,
      "Pin maven-antrun-plugin version to 3.1.0",
      "Now sure what changed on my end but I suddenly hit this error during maven build today when I am building from the latest master branch:\n\n{code}\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:3.1.0:run (default) on project hdds-interface-client: You are using 'tasks' which has been removed from the maven-antrun-plugin. Please use 'target' and refer to the >>Major Version Upgrade to version 3.0.0<< on the plugin site. -> [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:3.1.0:run (default) on project hdds-interface-client: You are using 'tasks' which has been removed from the maven-antrun-plugin. Please use 'target' and refer to the >>Major Version Upgrade to version 3.0.0<< on the plugin site.\n{code}\n\nThis issue can be solved by pinning maven-antrun-plugin version to the latest, and replacing the removed {{<tasks>}} tag, according to \"Major Version Upgrade to version 3.0.0\" section here:\n\nhttps://maven.apache.org/plugins/maven-antrun-plugin/",
      "HDDS",
      "Resolved",
      3,
      3,
      9462,
      "pull-request-available"
    ],
    [
      13249214,
      "Place ozone.om.address config key default value in ozone-site.xml",
      "{code:xml}\n   <property>\n     <name>ozone.om.address</name>\n-    <value/>\n+    <value>0.0.0.0:9862</value>\n     <tag>OM, REQUIRED</tag>\n     <description>\n       The address of the Ozone OM service. This allows clients to discover\n{code}",
      "HDDS",
      "Resolved",
      3,
      4,
      9462,
      "pull-request-available"
    ],
    [
      13281066,
      "Implement ofs://: listStatus",
      "ofs:// should be able to handle list (recursive or not) under \"root\" and under each volume \"directory\", as if it is a flat filesystem (if the user has permissions to see the volumes).\n\nThis is dependent on HDDS-2840. Will post PR after HDDS-2840 is reviewed and committed.",
      "HDDS",
      "Resolved",
      3,
      7,
      9462,
      "pull-request-available"
    ],
    [
      13505361,
      "EC: ReplicationManager - refactor logic to send datanode commands into a central place",
      "The logic to send datanode commands, such as replicate, reconstruct, delete and close container happens in a few different places in the RM classes. It would make sense to centralise this in some common code, as it will make it more re-usable. There is some logic in the balancer which could re-use these new central methods to send replicate and delete commands too.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13529441,
      "Let ReplicationManager decide the timeout for commands in Datanodes",
      "Right now, \"hdds.scm.replication.command.deadline.factor\" is a fraction. For long durations such as 60 minutes, the difference between SCM deadline and Datanode deadline will be 60 - 60 * 0.9, which is 6 minutes. This is a significant difference, so perhaps this configuration should be a duration instead, like 30 seconds.\n\nCurrently the APIs provided by RM expose the DN deadline as a parameter. We could remove this and just let the RM decide a deadline for commands in the DN.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13259239,
      "Extend SCMCLI Topology command to print node Operational States",
      "The scmcli topology command only consider the node health (healthy, stale or dead). With decommission and maintenance stages, we need to also consider the operational states and display them with this command.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13516730,
      " EC: Verify unrecoverable EC containers which are empty transition to deleting",
      "In EC, a container is considered \"missing\" if there are not enough replicas to reconstruct it. If a key is deleted from one of these containers, the blocks should still be deleted from the replicas that are present. When all the blocks are deleted, the remaining replicas will be empty so the container should be deleted. It looks like this already happens in the code, so this Jira is to add a unit test for this case as well.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13527630,
      "ReplicationManager: Add RatisMisReplicationHandler into rm.processUnderReplicatedContainer",
      "We missed adding the RatisMisReplicationHandler into the ReplicationManager.processUnderReplicatedContainer() method. This means that RatisMisReplication is never handled.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13401335,
      "EC: ECBlockOutputstream commitKey should create one keyLocationInfo per logical block",
      "HDDS-5477 is pending due to a larger refactor effort. This change is therefore a temporary fix to the problem described in HDDS-5477, and will be superseded when HDDS-5477 is ready.\n\nCurrently at the time of commitKey, client creates the KeyLocationInfo for each individual instance of EC blocks. But when committing we should create one KeyLocationInfo per block as OM required to save one block info per block group in EC",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13558883,
      "Incorrect sorting order for all unhealthy replicas in RatisOverReplicationHandler",
      "{code}\n    if (allUnhealthy) {\n      // prefer deleting replicas with lower sequence IDs\n      return replicas.stream()\n          .sorted(Comparator.comparingLong(ContainerReplica::getSequenceId)\n              .reversed())\n          .collect(Collectors.toList());\n    }\n{code}\nThis should actually be the opposite, allowing lower sequence IDs to be deleted first. Also need to consider what happens when two replicas have the same sequence ID - how are ties broken? Consistent ordering matters in case of SCM failover.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13267512,
      "Refactor ReplicationManager to consider maintenance states",
      "In its current form the replication manager does not consider decommission or maintenance states when checking if replicas are sufficiently replicated. With the introduction of maintenance states, it needs to consider decommission and maintenance states when deciding if blocks are over or under replicated.\n\nIt also needs to provide an API to allow the decommission manager to check if blocks are over or under replicated, so the decommission manager can decide if a node has completed decommission and maintenance or not.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13579268,
      "Rename rewriteGeneration to expectedDataGeneration in the protobuf and builders",
      "Currently in the code, the generation passed in the atomic overwrite API is named as overwriteGeneration in the proto definition.\n\nComments on the original PR suggest we should change that to one of:\n\ngeneration\ndataGeneration\nexpectedGeneration\nexpectedDataGeneration\n\nThis Jira is to decide on the name and then make the code change.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13392297,
      "EC: Add missing break in switch statement when requesting EC blocks",
      "We missed a break statement in the EC branch of the switch statement, so calls for EC blocks are falling through to the exception case.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13292082,
      "Refactor SafeModeHandler to use a Notification Interface",
      "The SafeModeHandler currently accepts several objects which it notifies when the safe mode status changes.\n\nEach of these object are notified using a different method (there is no \"notification interface\") and some of the logic which really belongs in those objects (ie what to do when safemode goes on or off) is in the safemode classes rather than in the receiving class.\n\nAs we may need to extend safemode somewhat to delay pipeline creation until sufficient nodes have registered, I think it is worthwhile to refactor this area to do the following:\n\n1. Introduce a new Interface \"SafeModeTransition\" which must be implemented by any object which wants to listen for safemode starting or ending.\n\n{code}\npublic interface SafeModeTransition {\n  void handleSafeModeTransition(SCMSafeModeManager.SafeModeStatus status);\n}\n{code}\n\n2. Pass the SafeModeStatus object over this new interface. That way, we can extend SafeModeStatus to include more states in the future than just safemode = true / false.\n\n3. Change the constructor of SafeModeHandler to allow any number of objects to be registered to make it more flexible going forward.\n\n4. Ensure the logic of what action to take on safemode transition lives within the notified objects rather than in the Safemode clases.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13450257,
      "EC: ReplicationManager - Logic to process the over replicated queues and assign work to DNs",
      "We need some sort of thread which picks work from the under / over replicated queue and assigns the work to DNs with capacity. The DNs will pick the work up on each heartbeat.\n\nThis would use the class created in another Jira to create the command to fix the replication issue.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13521639,
      "EC: Add normal and low priority to replication supervisor and commands",
      "In order to allow replication commands to fix under and mis-replication to run with higher priority that replication commands related to the balancer, we will change the replication supervisor to be a priority queue, ordered by priority, enqueue time.\n\nCommands sent from the balancer will have a low priority, other commands will a normal priority. This means that balancer commands will not run while there are other replication commands present in the queue.\n\nThis means it is not important if the balancer adds a lot of replication commands to the queue and then some nodes go down in the cluster. The replication commands related to the down nodes will automatically get to the front of the queue.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13433178,
      "EC: Avoid allocating buffers in EC Reconstruction Streams until first read",
      "Due to the issue described in HDDS-6424, where KeyInputStream opens all its inputStreams for the entire key upfront, we should avoid allocating buffers in the EC Reconstruction InputStreams until they are actually needed (ie on first read).",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13426222,
      "EC: Fix todo items in TestECKeyOutputStream",
      "There are a couple of TODO comments in the test testWriteShouldSucceedWhenDNKilled - we should fix those. \n\nAdditionally, we should look into making that test faster, as right now it can take over 1 minute to run.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13402299,
      "Skip safemode check in TestOzoneManagerRocksDBLogging",
      "TestOzoneManagerRocksDBLogging can be made faster by skipping the SCM safemode check and setting the number of datanodes to 0 in the mini cluster.\n\nLast runtime was:\n\n{code}\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 81.735 s - in org.apache.hadoop.ozone.om.TestOzoneManagerRocksDBLogging\n{code}",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13427563,
      "Ensure immutable ContainerReplica set is returned from ContainerStateManagerImpl",
      "Inside ContainerStateMap, the replicas for a container are stored in a Set backed by a ConcurrentHashMap.\n\nWhen you ask for the current replicas of a container, this method is used:\n{code:java}\npublic Set<ContainerReplica> getContainerReplicas(\n\u00a0 \u00a0 \u00a0 final ContainerID containerID) {\n\u00a0 \u00a0 Preconditions.checkNotNull(containerID);\n\u00a0 \u00a0 final Set<ContainerReplica> replicas = replicaMap.get(containerID);\n\u00a0 \u00a0 return replicas == null ? null : Collections.unmodifiableSet(replicas);\n} {code}\nNote that it pulls out the Set, wraps it as unmodifiable and returns it.\n\nThere is a problem here, in that if the Set is updated by ICR / FCR at the same time as another part of the code has taken a reference to it, the other part of the code can make incorrect decisions. Eg:\n\n\u00a0\n{code:java}\nSet<> replicas = getContainerReplicas(...)\nreplicaCount = replicas.size()\n// continue to do something based on the size{code}\nReplicationManger has run into a race condition like this. We also use the Replicas to form pipelines for closed containers, so I worry there could be some strange issues if the set if mutated during the pipeline creation.\n\nI see two possible solutions here. `GetContainerReplicas` should create a copy of the Set and return that, so the copy the other part of the code gets is its own copy and nothing can change it.\n\nOr, we make the Set immutable, so that each new replica details are received, we create the new copy of the set and store that. Then any other parts of the code can get a reference to it, and know it will never change.\n\nMutations to the replicas for a closed container will only happen with FCR, which is relatively rare.\n\nHowever we may ask for read pipelines very frequently, so it would be cheaper overall to use option 2.\n\nIt we go with option 2, I think we can move from a concurrentHashMap to a plain hashMap too, which may make the memory footprint slightly smaller.\n\nNote access to the replicas is via ContainerStateManagerImpl, which already has a course RW lock protecting access to the container manager. Quite possibly FCR reporting could be improved by a finer grained or striped lock.\n\nThis problem was reported in HDDS-5643.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13428788,
      "Remove toString in debug log parameters within SCMCommonPlacementPolicy",
      "The debug log has \"toString()\" called on datanode details, which means it must be evaluated before it gets passed into the debug logger. That means this string will always get created even when the log messages is not emitted.\n{code:java}\nLOG.debug(\"Datanode {} is chosen. Required metadata size is {} and \" +\n        \"required data size is {}\",\n    datanodeDetails.toString(), metadataSizeRequired, dataSizeRequired); {code}\nWe can just drop the toString part to fix this.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13425225,
      "EC: Bucket does not display correct EC replication details",
      "After creating a bucket using the shell:\n\n\n{code:java}\nozone sh bucket create /vol1/testec -t EC -r rs-3-2-1024k {code}\nAnd then listing the bucket info, the EC Replication details are not reflected in the output:\n{code:java}\n{\n\u00a0 \"metadata\" : { },\n\u00a0 \"volumeName\" : \"vol1\",\n\u00a0 \"name\" : \"testec\",\n\u00a0 \"storageType\" : \"DISK\",\n\u00a0 \"versioning\" : false,\n\u00a0 \"usedBytes\" : 0,\n\u00a0 \"usedNamespace\" : 0,\n\u00a0 \"creationTime\" : \"2022-01-27T17:29:54.289Z\",\n\u00a0 \"modificationTime\" : \"2022-01-27T17:29:54.289Z\",\n\u00a0 \"quotaInBytes\" : -1,\n\u00a0 \"quotaInNamespace\" : -1,\n\u00a0 \"bucketLayout\" : \"OBJECT_STORE\",\n\u00a0 \"owner\" : \"hadoop\",\n\u00a0 \"replicationConfig\" : {\n\u00a0 \u00a0 \"replicationFactor\" : \"THREE\",\n\u00a0 \u00a0 \"requiredNodes\" : 3,\n\u00a0 \u00a0 \"replicationType\" : \"RATIS\"\n\u00a0 },\n\u00a0 \"link\" : false\n} {code}",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13410811,
      "EC: Create reusable buffer pool shared by all EC input and output streams",
      "When reading and writing EC, we often have to allocate chunk sized Bytebuffers. It would be good if we could reuse them across multiple input / output instances to save freeing and re-allocating memory.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13505506,
      "EC: ReplicationManager - remove calls to ECHealthCheck from under and over replication processing",
      "The under replication processing makes some calls which we no longer need into the ECContainerHealth check class. If the container ends up in the under replicated handler, it means it was under-replicated and we can just check that is still the case by checking the ECContainerReplicaCount object instead of going back to the ECContainerHealthCheck class.\n\n",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13530576,
      "ReplicationManager: Basic Throttling of EC Reconstruction commands",
      "EC Reconstruction commands should be throttled in a similar way to replicate container commands, so that a limited number of commands can be sent to any given node.\n\nAs reconstruction and replication share the same queue on the datanode, the datanode could be filled with a combination of replication and reconstruction commands, so the sendThrottledReplicationCommand method will also need adjusted to consider the number of reconstructions commands queue when checking the limit.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13473753,
      "EC: ReplicationManager - Over replication handler should set repIndex on delete cmds",
      "ContainerReplicaPendingOps needs to track the replica index of any pending replicas. In ECOverReplicationHandler we missed setting the repIndex of the replica in the delete command.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13541155,
      "Move protobuf conversion out of the lock in PipelineStateManagerImpl",
      "There are a few places where protobuf conversion is perform under a lock in PipelineStateManagerImpl. We should move these outside of the lock.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13530833,
      "ReplicationManager: Add nodes to exclude list if they are overloaded",
      "When sending throttled replication / reconstruction commands, we know if the target node is at the replication task limit. If it is, we should add it to an exclude list in replicationManager.\n\nWhen the DN heartbeats, it triggers a callback to RM, which lets it check if the node should be removed from the exclude list.\n\nThe exclude list can then be fed into the node selection for reconstruction tasks, so it can avoid picking nodes which are already overloaded as targets, but that will be another Jira.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13526983,
      "ECReconstructionCoordinatorTask.runTask should catch Exception",
      "ECReconstructionCoordinatorTask.runTask() catches IOException, but any RuntimeExceptions fall threw the handler and are not logged correctly.\n\nThe handler should catch Exception to ensure any Runtime Exception caused by precondition checks are caught and handled too.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13334276,
      "DatanodeAdminMonitor no longers needs maintenance end time to be passed",
      "An earlier change moved the maintenance endtime into the NodeStatus object. However when adding a node to the decommission monitor the end time must still be passed. This value is never used.\n\nThis Jira will remove the endInHours field from the interface:\n\n{code}\npublic interface DatanodeAdminMonitor extends Runnable {\n\n  void startMonitoring(DatanodeDetails dn, int endInHours);\n  void stopMonitoring(DatanodeDetails dn);\n\n}\n{code}",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13502205,
      "EC: ReplicationManager: Move Mis-Replicated into a separate unhealthy state",
      "While looking into mis-replicated handling for EC, we found the code is much simplified if we handle mis-replicated containers only when they are not also over or under replicated.\n\nWith that in mind, we should have a separate unhealthy state for mis-replication, rather than making it part of under-replication.\n\nThis change adds that new state, adds mis-replication logic to the ECReplicationCheckHandler and amends the RatisReplicationCheckHandler to match it.\n\nFor now, a mis-replicated queue has been added, but this may change later, as we need to look into the queues and see if they need more work to separate out EC and Ratis or not.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13386556,
      "Avoid object creation in ReplicationManger debug log statements",
      "There are a few debug log entries in replication manager like this:\n\n{code}\n    LOG.debug(\"Handling under-replicated container: {}\",\n        container.getContainerID());\n\n      LOG.debug(\"Deleting empty container {} replicas,\", container.containerID());\n{code}\n\nEspecially the latter one, will always allocate a new ContainerID object on each call, even if debug is not enabled.\n\nIf we just pass \"container\" then it will use the container.toString() method inside the logger, only if debug is enabled. The ContainerInfo toString looks like:\n\n{code}\n  @Override\n  public String toString() {\n    return \"ContainerInfo{\"\n        + \"id=\" + containerID\n        + \", state=\" + state\n        + \", pipelineID=\" + pipelineID\n        + \", stateEnterTime=\" + stateEnterTime\n        + \", owner=\" + owner\n        + '}';\n  }\n{code}\n\nIt contains some extra information, but some of that may be useful if debugging a problem.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13426220,
      "EC: Container list command should allow filtering of EC containers",
      "The container list command currently allows filtering by Factor, but with EC, it needs to be extended to filter by replication type and EC / Ratis replication schemes.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13287729,
      "Replication manager should detect and correct containers which don't meet the replication policy",
      "In the current implementation, replication manager does not consider the container placement when checking if a container is healthy. Only the number of replicas are checked.\n\nNow we have network topology available, we should consider whether replication manager should detect and correct mis-replicated containers.\n\nIn HDFS, the namenode will not automatically correct mis-replicated containers automatically, except at startup when all blocks are checked.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13256157,
      "Update JMX metrics for node count in SCMNodeMetrics for Decommission and Maintenance",
      "Currently the class SCMNodeMetrics exposes JMX metrics for the number of HEALTHY, STALE and DEAD nodes.\n\nIt also exposes the disk capacity of the cluster and the amount of space used and available.\n\nWe need to decide how we want to display things in JMX when nodes are in and entering maintenance, decommissioning and decommissioned.\n\nWe now have 15 states rather than the previous 3, as we can have nodes in:\n * IN_SERVICE\n * ENTERING_MAINTENANCE\n * IN_MAINTENANCE\n * DECOMMISSIONING\n * DECOMMISSIONED\n\nAnd in each of these states, nodes can be:\n * HEALTHY\n * STALE\n * DEAD\n\nThe simplest case would be to expose these 15 states directly in JMX, as it gives the complete picture, but I wonder if we need any summary JMX metrics too?\n\n\u00a0\n\nWe also need to consider how to count disk capacity and usage. For example:\n # Do we count capacity and usage on a DECOMMISSIONING node? This is not a clear cut answer, as a decommissioning node does not provide any capacity for writers in the cluster, but it does use capacity.\n # For a DECOMMISSIONED node, we probably should not count capacity or usage\n # For an ENTERING_MAINTENANCE node, do we count capacity and usage? I suspect we should include the capacity and usage in the totals, however a node in this state will not be available for writes.\n # For an IN_MAINTENANCE node that is healthy?\n # For an IN_MAINTENANCE node that is dead?\n\nI would welcome any thoughts on this before changing the code.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13260557,
      "SortDatanodes does not return correct orders when many DNs on a given host",
      "In HDDS-2199 ScmNodeManager.getNodeByAddress() was changed to return a list of nodes rather than a single entry, to handle the case where many datanodes are running on the same host.\n\nIn SCMBlocKProtocol.sortDatanodes(), it uses the results returned from getNodesByAddress to determine if the client submitting the request is running on a cluster node, and if it is, it attempts to sort the datanodes by distance from the client machine.\n\nTo do this, the code currently takes the first DatanodeDetails object returned by getHostsByAddress and then compares it with the other passed in nodes. If any of the passed nodes are equal to the client node (based on the Java object ID) it returns a zero distance, otherwise the distance is calculated.\n\nThe sort is performed in NetworkTopologyImpl.sortByDistanceCost() which later calls NetworkTopologyImpl.getDistanceCost() which is where the object comparison is performed:\n\n{code}\nif ((node1 != null && node2 != null && node1.equals(node2)) ||\n (node1 == null && node2 == null)) {\n return 0;\n}\n{code}\n\nThis does not always work when there are many datanodes on the same host, as the first node returned from getNodesByAddress() is guarantted to be on the same host as the client, but the list of passed datanodes may not include that datanode instance.\n\nTo fix this, we should probably have getDistanceCost() compare hostnames or IP as a second check or instead of the object equality, however this is not trivial to implement.\n\nThe reason, is that getDistanceCost() takes Node objects (not DatanodeDetails) and a Node does not have a IP or Hostname field. It does have a getNetworkName method, which should return the hostname, but it is overwritten by the hosts UUID when it registed to the node manager, by this line in NodeManager.register():\n\ndatanodeDetails.setNetworkName(datanodeDetails.getUuidString());\n\n\u00a0\n\nNote this only affects test clusters where many DNs are on a single host, and it does not cause any failures. The DNs may be returned a less than ideal order.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "TriagePending"
    ],
    [
      13442030,
      "Use injected clocks in PipelineManagerImp and BackgroundPipelineScrubber to ease testing",
      "\"There are a couple of places in the new scrubber code and the existing scrubber code in PipelineManagerImpl, where it uses\u00a0{{Time.monotonicNow()}}\u00a0to decide if the Safemode interval has passed, or if a pipeline has been Closed long enough etc. The unit tests do not correctly test these scenarios, as we just set the time to zero so there is no delay, otherwise the tests would need sleep calls, which will make them slow.\n\nIn ReplicationManager, we addressed this problem by injecting a Clock dependency. See\u00a0{{MonotonicClock}}\u00a0- if we inject this as a dependency to the scrubber code, then we can inject a\u00a0{{MonotonicClock}}\u00a0for runtime, but inject\u00a0{{TestClock}}\u00a0for tests. Then you can properly test the safemode delay by advancing the clock between check calls. Same for pipelines - we can check CLOSED ones are not removed before the delay, and then check they are scrubbed after the delay.\n\nIn general, we should try to avoid calls to\u00a0{{Time.monotonicNow()}}\u00a0across the codebase, and instead inject a clock as a dependency to make the code more testable without sleeps.\"\n\n-- Stephen",
      "HDDS",
      "Resolved",
      4,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13529466,
      "ReplicationManager: Throttle delete container commands from over replication handlers",
      "Similar to ReplicateContainerCommands, we should limit the number of delete commands queued on a given datanode at any time. This PR will enforce the limit with a static config variable with a view to making this more dynamic later.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13530837,
      "ReplicationManager: EC Mis and Under replication handler should handle overloaded exceptions",
      "In RatisOverReplicationHandler and ECOverReplicationHandler, a container can be over replicated by several replicas, and the deletes are done in two stages:\n\n1. First unhealthy replicas are removed.\n2. Then healthy are removed.\n\nWhile removing any replica, the handler could get a CommandTargetOverloadedException, but rather than throwing that exception immediately, it continues trying other replicas. At the end, if it has not deleted enough replicas, it re-throws the first CommandTargetOverloadedException so the over replication is re-queued on the over replication queue.\n\nOther handlers also have multiple stages, but in the event of an error like CommandTargetOverloadedException, they give up immediately.\n\nRatisOverReplicationHandler works as expected. So does ECOverReplicationHandler.\n\nFor RatisUnderReplicationHandler, as the command target is the source, and the RM.sentThrottleReplicationCommand() handles picking the lowest loaded source - it is possible to send one command, and then fail to send the second, but there is no point in retrying as it means all the sources are overloaded. As things stand, it will send what it can and then throw an exception, so that is fine.\n\nFor MisReplicationHandler, which is currently shared with EC and Ratis (HDDS-8109 may change this), I believe it could run into this problem with EC, where it may need to make a new copy of 2 EC indexes, and 1 of the nodes is overloaded and the other is not. It would be better to not fail completely if the first is overloaded.\n\nFor Ratis Mis Replication, as we can copy any replica after HDDS-8109 it should behave like the RatisUnderReplicationHandler after HDDS-8109.\n\nFor ECUnderReplicationHandler, there are multiple stages for processing and potential for partial success.\n\nWe should review both ECUnderReplicationHandler and EC MisReplication handling (after HDDS-8109) to handle overloaded exceptions and throw exceptions on partial success.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13399631,
      "Reduce number of mini-clusters needed for decommission tests",
      "There are a few scenarios in the decommission and maintenance integration tests that can be combined into the same test, reducing the number of mini-clusters needed. These changes reduce the number of tests by 3.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13360697,
      "Ozone admin datanode list should report dead and stale nodes",
      "In ListInfoSubcommand, the logic explicitly only displays HEALTHY nodes:\n\n{code}\n  private List<DatanodeWithAttributes> getAllNodes(ScmClient scmClient)\n      throws IOException {\n    List<HddsProtos.Node> nodes = scmClient.queryNode(null,\n        HddsProtos.NodeState.HEALTHY, HddsProtos.QueryScope.CLUSTER, \"\");\n  ...\n{code}\n\nI believe we should include stale and dead nodes in the the output too.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13486622,
      "EC: Close pipelines with unregistered nodes",
      "A datanode is stopped and before the stale node handler is triggered, SCM is restarted. When SCM restarts its loads all the only pipelines and nodes from RocksDB, and then all the nodes will register again.\n\nIn the case of EC pipelines, there is nothing to trigger the close of a pipeline (and the containers on it) except:\n\n1. The Container getting full and the DN triggering the close\n2. The stale / dead node handlers noticing a node on it has gone dead.\n\nIn the case above, the EC pipeline will sit forever in an Open state, but any attempt to write to it will likely result in errors on the client due to one of the nodes not being available. These errors still will not trigger it to close.\n\nA solution to this problem, is to add logic to the pipeline scrubber to close any pipelines that have unregistered nodes. Stale / Dead nodes should be handled by the existing stale / dead node handlers.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13536856,
      "ReplicationManager: Change default command timeout to 10 minutes",
      "In Replication Manager, a deadline is set on commands sent to a datanode. If the command has not completed within the timeout, RM assumes it is lost and will schedule a new command to another random node.\n\nRight now the default is set to 30 minutes as the legacy RM scheduled a lot of work onto the DNs and it could take a long time to complete. The new RM throttles the work sent, so a large queue on the DNs should not be possible.\n\nWe should change the default event timeout to 10 minutes instead of 30.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13320904,
      "Non rack aware pipelines should not be created if multiple racks are alive",
      "If we have a scenario where one rack has more nodes that others, it is possible for all hosts in the cluster to have reached their pipeline limit, while 3 nodes on the larger rack have not. \n\nThe current fallback logic will then allow a pipeline to be created which uses only the 3 nodes on the same rack, violating the rack placement policy.\n\nThere may be other ways this could happen with cluster load too, were the pipeline capacity has reached its limit on some nodes but not others.\n\nThe proposal here, is that if the cluster has multiple racks AND there are healthy nodes covering at least 2 racks, where healthy is defined as a node which is registered and not stale or dead, then we should not allow \"fallback\" (pipelines which span only 1 rack) pipelines to be created.\n\nThis means if you have a badly configured cluster - eg Rack 1 = 10 nodes; Rack 2 = 1 node, the pipeline limit will be constrained by the capacity of that 1 node on rack 2. Even a setup like Rack 1 = 10 nodes, Rack 2 = 5 would be constrained by this.\n\nThis constraint is better than creating non rack aware pipelines, and the rule above will handle the case when the cluster degrades to 1 rack, as the healthy node definition will notice only 1 rack is alive.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13534432,
      "ReplicationManager: Pass used and excluded node separately for Under and Mis-Replication",
      "With HDDS-7226 merged, the RackAwarePlacementPolicy now supports passing used nodes and excluded nodes separately. This allows it to select the racks correctly when there are one or two used nodes.\n\nWe need to change the Ratis under and mis-replication handlers to pass the used nodes and excluded nodes as two separate lists now.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13551254,
      "Remove duplicate containers when loading volumes on a datanode",
      "Prior to HDDS-5032, if the same container is found on multiple volumes, then the second volume to load it fails the entire volume.\n\nAfter HDDS-5032, the exception is caught so the volume doesn't fail, but depending on which container is loaded faster, either one of the replicas could win. Over several restarts, the container on either volume could be the one loaded, potentially resulting in inconsistencies.\n\nThis change catches the error, and then removes one of the duplicates based on the BCSID. The container with the largest BCSID is the one kept, while the other is removed.\n\nThis will free the disk space taken by the duplicate container, and also avoid the chance of a different copy being loaded on each restart.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13500747,
      "EC: Notify ReplicationManager when a heartbeat updates datanode command counts",
      "When a datanode heartbeat is processed, it updates the queued command counts on the datanodes. Replication is going to use this information when assigning work to datanodes, and if a datanode exceeds the load limit it will be excluded until it has more capacity.\n\nTo allow RM to remove nodes from any exclude lists, we should fire a SCM event when the counts are updated, as this will avoid RM polling frequently.\n\nFor now, the notification in RM does not do anything - we will add code which uses it in future changes as we build out the load aware under / over replication processing.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13330341,
      "Remove no longer needed class DatanodeAdminNodeDetails",
      "DatanodeAdminNodeDetails was added earlier in the decommission branch, to track metrics and, the decommission state and maintenance end time. \n\nAfter enhancing NodeStatus to old the Maintenance Expiry time, this class is no longer needed and it also duplicates information which is stored in other existing places.\n\nThis change removes it and then metrics etc can be added later in a different way.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13221390,
      "Fix error propagation for SCM protocol",
      "HDDS-1068 fixed the error propagation between the OM client and OM server.\n\nBy default the Server.java transforms all the IOExceptions to one string (message + stack trace) and this is returned to the client.\n\nBut for business exception (eg. volume not found, chill mode is active, etc.) this is not what we need.\n\nIn the OM side we fixed this behaviour. In the ServerSideTranslator classes we catch (server) the business (OMException) exceptions and serialize them to the response object.\n\nThe exception (and the status code) is stored in message/status field of the OMResponse (hadoop-ozone/common/src/main/proto/OzoneManagerProtocol.proto)\n\nHere I propose to do the same for the ScmBlockLocationProtocol.proto.\n\nUnfortunately there is no common parent object (like OMRequest) in this protocol, but we can easily add one as only the Serverside/Clientside translator should be changed for that. ",
      "HDDS",
      "Resolved",
      2,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13514763,
      "EC: ReplicationManager - UnderRep maintenance handler should not request nodes if none needed",
      "If an EC container is under-replicated somehow, and also has some maintenance indexes that do not need replicated, the logic calls the placement policy requesting zero nodes. The policy then throws an exception as it expects to have greater than zero nodes requested.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13426223,
      "EC: Add replica index to the output in the container info command",
      "HDDS-6177 added replica details to the container info command on the master branch. On the EC branch, we need to extend it to add the new replica index field.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13576050,
      "Do not fail read of EC block if the last chunk is empty",
      "Due to HDDS-10682 some EC blocks in a cluster could have an empty final chunk. These blocks will fail to read and could cause data to become unavailable, even though it is still present on disk.\n\nIf the last chunk is empty, this should not stop the block from being empty.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13382535,
      "ContainerInfo should use ReplicationConfig",
      "We introduced ReplicationConfig to most classes already, but we missed ContainerInfo.\n\nThis change will ensure that ContainerInfo uses ReplicationConfig rather than the legacy Type and Factor fields.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13519772,
      "EC: GetChecksum for EC files can fail intermittently with IndexOutOfBounds exception",
      "When calculating a checksum for an EC file with Rack Topology enabled, you can get the following error intermittently:\n\n{code}\nERROR : Failed with exception null\n\u00a0 java.lang.IndexOutOfBoundsException\n\u00a0 \u00a0 \u00a0 \u00a0 at java.nio.ByteBuffer.wrap(ByteBuffer.java:375)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.computeCompositeCrc(ECBlockChecksumComputer.java:163)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.compute(ECBlockChecksumComputer.java:65)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.getBlockChecksumFromChunkChecksums(ECFileChecksumHelper.java:148)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlock(ECFileChecksumHelper.java:106)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlocks(ECFileChecksumHelper.java:73)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.BaseFileChecksumHelper.compute(BaseFileChecksumHelper.java:220)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.OzoneClientUtils.getFileChecksumWithCombineMode(OzoneClientUtils.java:223)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileChecksum(BasicRootedOzoneClientAdapterImpl.java:1123)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileChecksum(BasicRootedOzoneFileSystem.java:955)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.getFileChecksum(FileSystem.java:2831)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addInsertNonDirectoryInformation(Hive.java:3659)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3632)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3578)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3563)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:3224)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:418)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:769)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:748)\nERROR : FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.MoveTask. java.lang.IndexOutOfBoundsException\n\u00a0 \u00a0 \u00a0 \u00a0 at java.nio.ByteBuffer.wrap(ByteBuffer.java:375)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.computeCompositeCrc(ECBlockChecksumComputer.java:163)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.compute(ECBlockChecksumComputer.java:65)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.getBlockChecksumFromChunkChecksums(ECFileChecksumHelper.java:148)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlock(ECFileChecksumHelper.java:106)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlocks(ECFileChecksumHelper.java:73)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.BaseFileChecksumHelper.compute(BaseFileChecksumHelper.java:220)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.OzoneClientUtils.getFileChecksumWithCombineMode(OzoneClientUtils.java:223)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileChecksum(BasicRootedOzoneClientAdapterImpl.java:1123)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileChecksum(BasicRootedOzoneFileSystem.java:955)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.getFileChecksum(FileSystem.java:2831)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addInsertNonDirectoryInformation(Hive.java:3659)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3632)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3578)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3563)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:3224)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:418)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:769)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:748)\nINFO \u00a0: Completed executing command(queryId=hive_20221214035652_bc45477d-98df-408e-b945-a63b4ac6896a); Time taken: 22.167 seconds\n\u00a0 INFO \u00a0: OK\n\u00a0 Error: Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.MoveTask. java.lang.IndexOutOfBoundsException\n\u00a0 \u00a0 \u00a0 \u00a0 at java.nio.ByteBuffer.wrap(ByteBuffer.java:375)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.computeCompositeCrc(ECBlockChecksumComputer.java:163)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.compute(ECBlockChecksumComputer.java:65)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.getBlockChecksumFromChunkChecksums(ECFileChecksumHelper.java:148)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlock(ECFileChecksumHelper.java:106)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlocks(ECFileChecksumHelper.java:73)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.client.checksum.BaseFileChecksumHelper.compute(BaseFileChecksumHelper.java:220)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.OzoneClientUtils.getFileChecksumWithCombineMode(OzoneClientUtils.java:223)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileChecksum(BasicRootedOzoneClientAdapterImpl.java:1123)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileChecksum(BasicRootedOzoneFileSystem.java:955)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.getFileChecksum(FileSystem.java:2831)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addInsertNonDirectoryInformation(Hive.java:3659)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3632)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3578)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3563)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:3224)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:418)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:769)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:748) (state=08S01,code=40000){noformat}\n{code}\n\nThis is because the wrong nodes are used to obtain the stripe checksum sometimes as the node does not correctly use the replicaIndex in the pipeline to order the nodes.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13441195,
      "Revert HDDS-6579",
      "HDDS-6579 added a flag to hide newly added information in the `ozone admin container info` command for compatibility reasons. However the flag is not necessary, as the changes are not incompatible. It is also not desirable to have a new command flag for every piece of new information that is to be added to every command. It will result in commands with many flags that have to be passed all the time to get the full set of information.\n\nThere is a community discussion about compatibility where the consensus is that we provide compatibility for these admin commands via the JSON output, and that the formatting and output of the \"human readable\" formats can change over time:\n\nhttps://lists.apache.org/thread/5gqnbstv1pznwmcvx7txspj1qrksy7gl\n\nFor those reasons I am reverting HDDS-6579.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13528478,
      "Replication Manager: Make all handlers send commands immediately instead of returning commands",
      "To allow better throttling control all the unhealthy handlers should send the command directly using the RM API, rather than gathering up a list of commands and returning them.\n\nThis change involves a change to the UnhealthyReplicationHandler interface, which previously returned the commands, but now returns just the count of commands sent instead.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13515116,
      "EC metrics related to replication commands don't add up",
      "{code}\n    \"EcReplicationCmdsSentTotal\" : 0,\n    \"EcDeletionCmdsSentTotal\" : 259,\n    \"EcReplicationCmdsCompletedTotal\" : 51,\n    \"EcDeletionCmdsCompletedTotal\" : 51,\n    \"EcReconstructionCmdsSentTotal\" : 571,\n    \"EcReplicationCmdsTimeoutTotal\" : 765,\n    \"EcDeletionCmdsTimeoutTotal\" : 204\n{code}\n\nTotal replication commands sent are 0, while timed out are 765.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13365631,
      "Decommission CLI should return details of nodes which fail",
      "With the current decommission / recommission / maintenance mode commands, you can pass a list of hosts to perform the operation on. If any of these hosts fail to enter the decommission / maintenance workflow, the command gives no feedback about the error. Some of the hosts can silently fail and the only way to know is to inspect the SCM log.\n\nThe most common way a host can fail, is if a node which is undergoing maintenance is instructed to go to decommission and vice versa as this is a transition which is not allowed.\n\nThis change will allow any failed nodes to feed back to the client. If the client detects that any of the nodes have failed, details will be written to stderr and the command exit code will be non-zero.\n\nNote that even though the exit code is non-zero, the command may have partially worked.\n\nAlso note that the errors which are fed back are only around transitioning the node into the admin workflow - it is still possible for it to fail later for other reasons which will not be fed back to the client. This is because the client does not wait for the process to complete, but exits after confirmation the command has been processed by scm.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13396122,
      "Speed up decommission tests using a background Mini Cluster provider",
      "The integration (ozone) test suit is the slowest part of the github actions build, taking over 2 hours usually. In a random PR I checked, 2hr16.\n\nOften in integration tests, a large part of the test time is spent creating a new mini-Ozone cluster for each test, which can take 10 - 20 seconds to startup.\n\nI also timed stopping a mini-cluster and found that can take up to 10 seconds.\n\nChanging the tests to reuse the same cluster can be difficult and make the tests less standalone and more brittle, which is not a good thing. Changing the tests is also time consuming work.\n\nAssuming a test runs for longer than the time taken to setup a mini-cluster and stop it, it would make the tests faster if we pre-created a mini-cluster in the background. Then when one test completes, the next cluster is already there, saving the startup time. Obviously this costs more concurrent cpu to reduce the wall clock time.\n\nWe could also queue the shutdown of the clusters in another background thread.\n\nThe slowest part of the Integration (Ozone) test suit are the decommission tests, taking 843 seconds on the last run I checked.\n\nThis PR adds a Mini-Cluster provider to the Decommission tests as an experiment to see if it makes the runtime significantly faster in practice. If it does, this may be something we can roll out across other integration tests.\n\nAs a baseline, I ran the decommission tests on my laptop, and it took 8min 37s.\n\nAfter the changes in this PR, the test suit ran in 3min 53s.\n",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13428338,
      "Remove replicas from ContainerStateMap when a container is deleted",
      "In ContainerStateMap, there are several maps to hold various details, eg:\n  private final Map<ContainerID, ContainerInfo> containerMap;\n  private final Map<ContainerID, Set<ContainerReplica>> replicaMap;\nWhen we add a new container, we add an entry to both of these sets. When a container is removed, we don\u2019t see to remove from replicaMap (see below). There doesn\u2019t seem to be any way to remove the replicas later once the containerMap entry is gone, so removing the container is leaking the replicas.\n\u00a0\n{code:java}\n  public void removeContainer(final ContainerID id) {\n    Preconditions.checkNotNull(id, \"ContainerID cannot be null\");\n    if (contains(id)) {\n      // Should we revert back to the original state if any of the below\n      // remove operation fails?\n      final ContainerInfo info = containerMap.remove(id);\n      lifeCycleStateMap.remove(info.getState(), id);\n      ownerMap.remove(info.getOwner(), id);\n      repConfigMap.remove(info.getReplicationConfig(), id);\n      typeMap.remove(info.getReplicationType(), id);\n      // Flush the cache of this container type.\n      flushCache(info);\n      LOG.trace(\"Container {} removed from ContainerStateMap.\", id);\n    }\n  } {code}\nYou cannot remove the replicas anyway later, as the methods check if the container exists first, which it no longer will, eg:\n\n\n{code:java}\npublic void removeContainerReplica(final ContainerID containerID,\n    final ContainerReplica replica) {\n  Preconditions.checkNotNull(containerID);\n  Preconditions.checkNotNull(replica);\n  if (contains(containerID)) {\n    replicaMap.get(containerID).remove(replica);\n  }\n} {code}\nNote that deleting a container seems to be a rare operation (eg delete it manually from the CLI). Empty containers are currently marked as deleted, but as far as I can tell, they are not actually removed from SCM.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13335011,
      "ContainerInfo does not persist BCSID leading to failed replicas reports",
      "If you create a container, and then close it, the BCSID is synced on the datanodes and then the value is updated in SCM via setting the \"sequenceID\" field on the containerInfo object for the container.\n\nIf you later restart just SCM, the sequenceID becomes zero, and then container reports for the replica fail with a stack trace like:\n\n{code}\nException in thread \"EventQueue-ContainerReportForContainerReportHandler\" java.lang.AssertionError\n\tat org.apache.hadoop.hdds.scm.container.ContainerInfo.updateSequenceId(ContainerInfo.java:176)\n\tat org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerStats(AbstractContainerReportHandler.java:108)\n\tat org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:83)\n\tat org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:162)\n\tat org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:130)\n\tat org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)\n\tat org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n{code}\n\nThe assertion here is failing, as it does not allow for the sequenceID to be changed on a CLOSED container:\n\n{code}\n  public void updateSequenceId(long sequenceID) {\n    assert (isOpen() || state == HddsProtos.LifeCycleState.QUASI_CLOSED);\n    sequenceId = max(sequenceID, sequenceId);\n  }\n{code}\n\nThe issue seems to be caused by the serialisation and deserialisation of the containerInfo object to protobuf, as sequenceId never persisted or restored.\n\nHowever, I am also confused about how this ever worked, as this is a pretty significant problem.\n\n",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13437926,
      "Have the datanode heartbeat include queued command counts",
      "To allow SCM to make better decisions about scheduling commands on datanodes, the datanodes should report their current command queue size in the heartbeat. This change adds a section to the heartbeat protobuf message to allow the queued command count for each command type to be reported in each heartbeat.\n\nThere will be a count reported for each registered command, with a zero count if the queue is empty.\n\nA followup change will be needed to use this information on SCM. With this Jira in place, the data will be sent but never used on SCM.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13521116,
      "EC: Refactor ReplicationSupervisor to allow Replication and Reconstruction tasks",
      "A refactor or the existing ReplicationSupervisor so we can have the same supervisor (and threadpool) manage both Replication and EC Reconstruction tasks.\n\nThis change does not include moving the EC tasks into the ReplicationSupervisor - that will be done in another PR as this one is already large enough with this change.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13323585,
      "Container report should update container key count and bytes used if they differ in SCM",
      "In HDDS-4037 it was noted that when blocks are deleted from closed containers, the bytesUsed and Key Count metrics on the SCM container are not updated correctly.\n\nThese stats should be updated via the container reports issued by the DNs to SCM periodically. However, in `AbstractContainerReportHandler#updateContainerStats`, the code assumes the values are always increasing and it will not update them if they are decreasing:\n\n{code}\n  private void updateContainerStats(final ContainerID containerId,\n                                    final ContainerReplicaProto replicaProto)\n      throws ContainerNotFoundException {\n    if (isHealthy(replicaProto::getState)) {\n      final ContainerInfo containerInfo = containerManager\n          .getContainer(containerId);\n\n      if (containerInfo.getSequenceId() <\n          replicaProto.getBlockCommitSequenceId()) {\n        containerInfo.updateSequenceId(\n            replicaProto.getBlockCommitSequenceId());\n      }\n      if (containerInfo.getUsedBytes() < replicaProto.getUsed()) {\n        containerInfo.setUsedBytes(replicaProto.getUsed());\n      }\n      if (containerInfo.getNumberOfKeys() < replicaProto.getKeyCount()) {\n        containerInfo.setNumberOfKeys(replicaProto.getKeyCount());\n      }\n    }\n  }\n{code}\n\nIn HDDS-4037 a change was made to the Replication Manager, so it updates the stats. However I don't believe that is the correct place to perform this check, and the issue is caused by the logic shared above.\n\nIn this Jira, I have removed the changes to Replication Manager in HDDS-4037 (but retained the other changes), ensuring the problem statistics are only updated via the containers reports if they are different in SCM from what is reported.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13400807,
      "EC: Resolve findbugs warnings after branch merge",
      "After merging master into the EC branch, there are findbugs warnings on the branch.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13267080,
      "ContainerReplica should contain DatanodeInfo rather than DatanodeDetails",
      "The ContainerReplica object is used by the SCM to track containers reported by the datanodes. The current fields stored in ContainerReplica are:\n\n{code}\nfinal private ContainerID containerID;\nfinal private ContainerReplicaProto.State state;\nfinal private DatanodeDetails datanodeDetails;\nfinal private UUID placeOfBirth;\n{code}\n\nNow we have introduced decommission and maintenance mode, the replication manager (and potentially other parts of the code) need to know the status of the replica in terms of IN_SERVICE, DECOMMISSIONING, DECOMMISSIONED etc to make replication decisions.\n\nThe DatanodeDetails object does not carry this information, however the DatanodeInfo object extends DatanodeDetails and does carry the required information.\n\nAs DatanodeInfo extends DatanodeDetails, any place which needs a DatanodeDetails can accept a DatanodeInfo instead.\n\nIn this Jira I propose we change the DatanodeDetails stored in ContainerReplica to DatanodeInfo.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13541424,
      "Allow EC PipelineChoosingPolicy to be defined separately from Ratis",
      "Cluster may have set the PipelineChoosingPolicy to the HealthyPipelineChoosePolicy for Ratis, but it adds overhead and is not necessary for EC, as the EC pipeline is always healthy:\n\n{code}\n  public boolean isHealthy() {\n    // EC pipelines are not reported by the DN and do not have a leader. If a\n    // node goes stale or dead, EC pipelines will by closed like RATIS pipelines\n    // but at the current time there are not other health metrics for EC.\n    if (replicationConfig.getReplicationType() == ReplicationType.EC) {\n      return true;\n    }\n    ...\n{code}\n\nTo allow for flexibility, add a new config for the ECPipelineChoosingPolicy so it can be different from the Ratis policy. ",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13330591,
      "Close Container event can fail if pipeline is removed",
      "If you call `pipelineManager.finalizeAndDestroyPipeline()` with onTimeout=false, then the finalizePipeline call will result in a closeContainer event to be fired for every container on the pipeline. These are handled asynchronously.\n\nHowever, immediately after that, the `destroyPipeline(...)` call is made. This will remove the pipeline details from the various maps / stores.\n\nThen the closeContainer events get processed, and they attempt to remove the container from the pipeline. However as the pipeline has already been destroyed, this throws an exception and the close container events never get sent to the DNs:\n\n{code}\n2020-10-01 15:44:18,838 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2\n2020-10-01 15:44:18,842 [EventQueue-CloseContainerForCloseContainerEventHandler] ERROR container.CloseContainerEventHandler: Failed to close the container #2.\norg.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=59e5ae16-f1fe-45ff-9044-dd237b0e91c6 not found\n\tat org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:372)\n\tat org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:111)\n\tat org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:413)\n\tat org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:352)\n\tat org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:331)\n\tat org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:66)\n\tat org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.Onmessage(CloseContainerEventHandler.java:45)\n\tat org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor\n{code}\n\nThe simple solution is to catch the exception and ignore it.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13480285,
      "testContainerIsReplicatedWhenAllNodesGotoMaintenance is failing frequently",
      "Since changes in HDDS-6975 the test testContainerIsReplicatedWhenAllNodesGotoMaintenance is failing frequently. This Jira is to fix the issues with that test.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13441448,
      "datanode usageinfo CLI should provide JSON output option",
      "It is good to have json output option so that it is easy parse the output when additonal information added to this command cli",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13472317,
      "EC: ReplicationManager - skip processing open containers",
      "When processing containers, we should skip the open containers until they are closed.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13443409,
      "EC: ReplicationManager - create version of ContainerReplicaCounts applicable to EC",
      "Currently, for Ratis containers a ContainerReplicaCounts object is used to calculate whether a container is over or under replicated, taking into account decommissioning, maintenance mode and inflight replicas.\n\nWe need a new version of this class to do something similar for EC, but it will need to return specific replicas with issues, rather than a simple delta which is returned by Ratis Containers.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13378098,
      "EC: ReplicaIndex in Pipeline should be serialized and deserialized in the protobuf message",
      "ReplicaIndex and ReplicationConfig were added to the Pipeline class, but we missed added ReplicaIndex to the serialized message. This Jira will add it in.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13444811,
      "EC: ReplicationManager - create ContainerReplicaPendingOps class and integrate with ContainerManager",
      "The legacy replication manager internally keeps a list of all pending replications and deletes. Each time a container is checked, it check this list and removes any replications that have been completed or expired. Then it gets the list of remaining pending operations to help decide if container is healthy or not.\n\nRather than the ReplicationManager removing the completed and expired replications, we could have a standalone PendingContainerOps monitor, that works as follows:\n\n1. Replication Manager adds pending replications and deletes to it.\n2. Replication Manager queries it for anything pending for the current container and gets a list of PendingActions back.\n3. The PendingReplicationMonitor has its own internal thread that checks for expired replications and removes them.\n4. Completed replications and deletes are removed in ComtainerManagerImpl, which has add and removeContainer triggered via the container reports (ICR and FCR) from the datanodes as they are replicated.\n\nThis way, the ReplicationManager does not need to worry about expiring replications or removing completed entries. We also get the ability to have a more up-to-date view of the system, as the ICR / FCRs will keep the pending table up-to-date in real time, rather than having to wait for the container to be re-check inside replication manager.\n\nWe can have a fairly simple \"ContainerReplicaPendingOps\" class that is basically standalone and inject it into ReplicationManager and ContainerManagerImpl. This would allow for removing some complexity from RM and let the expiry and completion be tested in an isolated way.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13363315,
      "EC: Implement ECBlockInputStream to read a single EC Block Group.",
      "Implement the happy-path read scenario, which does not support \"degraded read\" (on the fly EC recovery).",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13341627,
      "Open RocksDB read only when loading containers",
      "When a datanode is started, it must read some metadata from all the Containers. Part of that metadata is stored in RocksDB, so the startup process involves opening each rocksDB and closing it again.\n\nTesting on a dense node, with 45 high performance spinning disks and 200K containers, I saw about 75ms on average to open each RockDB. Further testing demonstrated that if we open RockDB read only, the average open time is about 35ms.\n\nAt startup time, the DBs are only read and never written, so opening read only is fine. HDDS-4427 already ensures these opened DBs are not cached.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13575517,
      "EC Reconstruction does not issue put block to data index if it is unused",
      "Given a small EC block:\n\n* <= 2MB for EC-3-2\n* <= 5MB for EC-6-3\n* <= 9MB for EC-10-4\n\nSo that it is less than a full stripe and does not use all the datanodes.\n\nWhen reconstruction happens to replace a replica which is not used in the stripe, the unused containers are not issued with the put block to store the details of the empty block within the container. Note that the container replica will likely have other blocks, so it will still get reconstructed, but it will not be given a reference to this empty block.\n\nAll containers are checked for the presence of all blocks during reconstruction. If any of the containers do not have a reference to the block, it is considered an orphan block / abandoned stripe and will not be reconstructed.\n\nTherefore if one replica has no entry for the block, then it is used it another reconstruction for another replica later, that block will not get reconstructed into a second replica. Over time this can result in the reference getting removed from all copies.",
      "HDDS",
      "Resolved",
      3,
      1,
      9499,
      "pull-request-available"
    ],
    [
      13539414,
      "ReplicationManager: Add metric to count how often replication is throttled",
      "Adding a metric for the number of containers where we fail to send a delete, replication or reconstruction due to the throttling. This will give some visibility into how often throttling is occurring.\n\nNew metrics are:\n\necReconstructionCmdsDeferredTotal\ndeleteContainerCmdsDeferredTotal\nreplicateContainerCmdsDeferredTotal",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13390559,
      "Replication Manager should process containers synchronously for tests",
      "The method ReplicationManager.processContainersNow() only wakes up the thread, and returns before the containers have been processed.\n\nThis results in all RM tests having a sleep(100) after all calls to this method.\n\nWith a small refactor to RM, we can avoid this sleep. After this change all tests run about 100ms faster, and the code in the tests is slightly better.",
      "HDDS",
      "Resolved",
      3,
      4,
      9499,
      "pull-request-available"
    ],
    [
      13376016,
      "EC: Allow EC blocks to be requests from OM",
      "We need to change the allocateBlock calls from OM to SCM so the EC Replication Config is passed through the stack.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13515130,
      "MisReplicationHandler does not consider QUASI_CLOSED replicas as sources",
      "MisReplicationHandler#filterSources gets a Set of replicas that can be used to fix mis replication. It selects CLOSED replicas:\n{code}\n  private Set<ContainerReplica> filterSources(Set<ContainerReplica> replicas) {\n    return replicas.stream().filter(r -> r\n                    .getState() == StorageContainerDatanodeProtocolProtos\n                    .ContainerReplicaProto.State.CLOSED)\n            .filter(r -> ReplicationManager\n                    .getNodeStatus(r.getDatanodeDetails(), nodeManager)\n                    .isHealthy())\n            .filter(r -> r.getDatanodeDetails().getPersistedOpState()\n                    == HddsProtos.NodeOperationalState.IN_SERVICE)\n            .collect(Collectors.toSet());\n  }\n{code}\n\nWhen thinking about Ratis Containers, QUASI_CLOSED replicas can also be mis replicated. They're also allowed to be replicated to datanodes. Should they also be considered as sources here?",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13580698,
      "Enable Atomic Rewrite in FSO buckets",
      "Extend the atomic commit support to FSO buckets. This will follow the same pattern for OBS buckets, ensuring that a key exists at the given path, and still existings upon commit with the same expected generation.",
      "HDDS",
      "Resolved",
      3,
      7,
      9499,
      "pull-request-available"
    ],
    [
      13357517,
      "Add Genesis benchmark for various CRC implementations",
      "As highlighted in HDDS-4138 Ozone appears to have a greater CRC overhead than Hadoop. In order to figure out where the problem is, we should add a benchmark to Genesis to test if one implementation is much better than the others.",
      "HDDS",
      "Resolved",
      3,
      2,
      9499,
      "pull-request-available"
    ]
  ]
}