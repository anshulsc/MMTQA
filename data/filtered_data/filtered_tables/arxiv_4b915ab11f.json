{
  "columns": [
    0,
    1,
    2,
    3
  ],
  "data": [
    [
      "Pretrained LLM",
      "Corpus size",
      "Training budget (A100\u00b7hours)",
      "Model architecture"
    ],
    [
      "BloomBergGPT",
      "363B Finance tokens + 345B public tokens",
      "1300000",
      "50B-BLOOM"
    ],
    [
      "XuanYuan2.0",
      "366B for pre-training + 13B for finetuning",
      "Not released",
      "176B-BLOOM"
    ],
    [
      "Fin-T5",
      "80B Finance tokens",
      "Days/weeks",
      "770M-T5"
    ]
  ]
}